This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
e11ocutionist/
  __init__.py
  __main__.py
  __version__.py
  chunker.py
  cli.py
  e11ocutionist.py
  elevenlabs_converter.py
  elevenlabs_synthesizer.py
  entitizer.py
  neifix.py
  orator.py
  tonedown.py
  utils.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="e11ocutionist/__init__.py">
"""e11ocutionist: A text processing pipeline for enhancing speech synthesis markup.

Created by Adam Twardoch
"""

from .__version__ import __version__

# Import main classes and types for convenience
try:
    from .e11ocutionist import E11ocutionistPipeline, PipelineConfig, ProcessingStep
    from .chunker import process_document as chunker_process
    from .entitizer import process_document as entitizer_process
    from .orator import process_document as orator_process
    from .tonedown import process_document as tonedown_process
    from .elevenlabs_converter import process_document as elevenlabs_converter_process
    from .elevenlabs_synthesizer import synthesize_with_all_voices
    from .neifix import transform_nei_content

    __all__ = [
        "E11ocutionistPipeline",
        "PipelineConfig",
        "ProcessingStep",
        "__version__",
        "chunker_process",
        "elevenlabs_converter_process",
        "entitizer_process",
        "orator_process",
        "synthesize_with_all_voices",
        "tonedown_process",
        "transform_nei_content",
    ]
except ImportError:
    # Handle the case where the modules are not yet available
    __all__ = ["__version__"]
</file>

<file path="e11ocutionist/__main__.py">
#!/usr/bin/env python3
# this_file: src/e11ocutionist/__main__.py
"""Main entry point for e11ocutionist package."""

from .cli import main

if __name__ == "__main__":
    main()
</file>

<file path="e11ocutionist/__version__.py">
# file generated by setuptools-scm
# don't change, don't track in version control

__all__ = ["__version__", "__version_tuple__", "version", "version_tuple"]

TYPE_CHECKING = False
if TYPE_CHECKING:
    from typing import Tuple
    from typing import Union

    VERSION_TUPLE = Tuple[Union[int, str], ...]
else:
    VERSION_TUPLE = object

version: str
__version__: str
__version_tuple__: VERSION_TUPLE
version_tuple: VERSION_TUPLE

__version__ = version = '1.0.0.post3+g4f8e413.d20250516'
__version_tuple__ = version_tuple = (1, 0, 0, 'post3', 'g4f8e413.d20250516')
</file>

<file path="e11ocutionist/chunker.py">
#!/usr/bin/env python3
# this_file: src/e11ocutionist/chunker.py
"""
Chunker module for e11ocutionist.

This module handles the semantic chunking of documents for processing.
It breaks documents into manageable semantic chunks based on content boundaries.
"""

import hashlib
import re
from pathlib import Path
from typing import Any
import datetime
from copy import deepcopy

import tiktoken
from lxml import etree
from loguru import logger
from functools import cache
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# Constants
DEFAULT_MODEL = "gpt-4o"  # Updated from "openrouter/openai/gpt-4.1"
FALLBACK_MODEL = (
    "gpt-4o-mini"  # Updated from "openrouter/google/gemini-2.5-pro-preview-03-25"
)
DEFAULT_CHUNK_SIZE = 12288
DEFAULT_TEMPERATURE = 0.2


@cache
def get_token_encoder():
    """Get the token encoder."""
    return tiktoken.encoding_for_model("gpt-4o")


def count_tokens(text: str) -> int:
    """
    Count the number of tokens in a text string accurately using tiktoken.

    Args:
        text: Text string to count tokens for

    Returns:
        Integer token count
    """
    encoder = get_token_encoder()
    return len(encoder.encode(text))


def escape_xml_chars(text: str) -> str:
    """
    Escape special XML characters to prevent parsing errors.

    Only escapes the five special XML characters while preserving UTF-8 characters.

    Args:
        text: Input text that may contain XML special characters

    Returns:
        Text with XML special characters escaped
    """
    # Replace ampersands first (otherwise you'd double-escape the other entities)
    text = text.replace("&", "&amp;")
    # Replace other special characters
    text = text.replace("<", "&lt;")
    text = text.replace(">", "&gt;")
    text = text.replace('"', "&quot;")
    text = text.replace("'", "&apos;")
    return text


def generate_hash(text: str) -> str:
    """Generate a 6-character base36 hash from text."""
    sha1 = hashlib.sha1(text.encode("utf-8")).hexdigest()
    # Convert to base36 (alphanumeric lowercase)
    base36 = int(sha1, 16)
    chars = "0123456789abcdefghijklmnopqrstuvwxyz"
    result = ""
    while base36 > 0:
        base36, remainder = divmod(base36, 36)
        result = chars[remainder] + result

    # Ensure we have at least 6 characters
    result = result.zfill(6)
    return result[:6]


def generate_id(prev_id: str, content: str) -> str:
    """Generate a unique ID for an item based on previous ID and content."""
    if not prev_id:
        # First item
        return f"000000-{generate_hash(content)}"
    else:
        # Subsequent items
        prev_suffix = prev_id.split("-")[-1]
        return f"{prev_suffix}-{generate_hash(content)}"


def split_into_paragraphs(text: str) -> list[str]:
    """Split text into paragraphs based on blank lines while preserving all whitespace."""
    # Remove Windows line endings
    text = text.replace("\r\n", "\n")
    # Split on blank lines (one or more newlines) and capture the delimiters
    parts = re.split(r"(\n\s*\n)", text)
    # Join each paragraph with its delimiter and keep everything, no stripping
    return ["".join(parts[i : i + 2]) for i in range(0, len(parts), 2)]


def is_heading(paragraph: str) -> bool:
    """Check if paragraph is a markdown heading or chapter heading in Polish/English."""
    # Check for markdown heading format
    if bool(re.match(r"^#{1,6}\s+", paragraph)):
        return True

    # Check for chapter headings in Polish and English
    if bool(re.search(r'(?i)^[„"]?(?:rozdział|chapter)\s+\w+', paragraph.strip())):
        return True

    return False


def is_blockquote(paragraph: str) -> bool:
    """Check if paragraph is a markdown blockquote."""
    return paragraph.strip().startswith("> ")


def is_list(paragraph: str) -> bool:
    """Check if paragraph is a markdown list of at least two items."""
    # Strip trailing blank line delimiters
    text = paragraph.rstrip("\n")
    lines = text.split("\n")
    bullet_pattern = re.compile(r"^[\*\-\+]\s+")
    number_pattern = re.compile(r"^\d+\.\s+")
    # Count lines that look like list items
    bullet_count = sum(
        1
        for line in lines
        if (bullet_pattern.match(line.strip()) or number_pattern.match(line.strip()))
    )
    # Treat as list only if two or more items
    return bullet_count >= 2


def is_code_block(paragraph: str) -> bool:
    """Check if paragraph is a markdown code block."""
    return paragraph.strip().startswith("```") and paragraph.strip().endswith("```")


def is_horizontal_rule(paragraph: str) -> bool:
    """Check if paragraph is a markdown horizontal rule."""
    return bool(re.match(r"^[\*\-_]{3,}\s*$", paragraph.strip()))


def is_table(paragraph: str) -> bool:
    """Check if paragraph is a markdown table."""
    lines = paragraph.split("\n")
    if len(lines) < 2:
        return False
    # Check for pipe character and separator line
    return "|" in lines[0] and bool(re.match(r"^[\|\s\-:]+$", lines[1].strip()))


def is_html_block(paragraph: str) -> bool:
    """Check if paragraph is an HTML block."""
    p = paragraph.strip()
    return p.startswith("<") and p.endswith(">")


def is_image_or_figure(paragraph: str) -> bool:
    """Check if paragraph is a markdown image or figure."""
    return bool(re.match(r"!\[.*\]\(.*\)", paragraph.strip()))


def itemize_document(document_text: str) -> list[tuple[str, str]]:
    """
    Split a document into semantic paragraph-level units.

    This function identifies paragraph boundaries and classifies each paragraph
    based on its content type (heading, blockquote, list, etc.). It also
    determines the attachment type of each paragraph (whether it should be
    attached to the preceding paragraph, following paragraph, or treated as normal).

    Args:
        document_text: The raw text document to process

    Returns:
        A list of tuples where each tuple contains (item_text, attachment_type)
        attachment_type is one of: "normal", "following", "preceding"
    """
    # Split the document into paragraphs
    paragraphs = split_into_paragraphs(document_text)

    # Process each paragraph to determine its attachment type
    itemized_paragraphs = []

    for i, paragraph in enumerate(paragraphs):
        if not paragraph.strip():
            continue  # Skip empty paragraphs

        attachment_type = "normal"  # Default attachment type

        # Determine attachment type based on paragraph characteristics
        if is_heading(paragraph):
            # Headings get their own item and following content attaches to them
            attachment_type = "normal"
        elif is_blockquote(paragraph) or is_list(paragraph) or is_code_block(paragraph):
            # These get their own items
            attachment_type = "normal"
        elif (
            is_horizontal_rule(paragraph)
            or is_table(paragraph)
            or is_html_block(paragraph)
        ):
            # These are normal items
            attachment_type = "normal"
        elif is_image_or_figure(paragraph):
            # Images/figures typically attach to preceding content
            attachment_type = "preceding"
        elif i > 0 and len(paragraph.strip()) < 100:
            # Short paragraphs after another paragraph might be continuations
            prev_para = paragraphs[i - 1].strip()
            if prev_para.endswith((":", "—")) or (
                prev_para
                and not any(
                    prev_para.endswith(x) for x in [".", "!", "?", '"', "'", ")", "]"]
                )
            ):
                attachment_type = "preceding"

        # Add to the list with its attachment type
        itemized_paragraphs.append((paragraph, attachment_type))

    return itemized_paragraphs


def replace_tok_placeholder(text: str, placeholder: str = 'tok="____"') -> str:
    """
    Replace token count placeholder with actual count.

    This function accurately counts tokens in XML elements and ensures
    the tok attribute reflects the actual token count.

    Args:
        text: XML text with token placeholders
        placeholder: The placeholder pattern to replace

    Returns:
        XML text with actual token counts
    """
    if placeholder not in text:
        return text

    # Temporarily replace placeholder with a unique string unlikely to appear in text
    temp_placeholder = "-987"
    text_with_temp = text.replace(placeholder, f'tok="{temp_placeholder}"')

    # Count actual tokens
    token_count = count_tokens(text_with_temp.replace(temp_placeholder, "0"))

    # Replace temp placeholder with actual count
    return text_with_temp.replace(temp_placeholder, str(token_count))


def create_item_elements(items: list[tuple[str, str]]) -> list[tuple[str, str]]:
    """
    Create XML item elements from itemized paragraphs.

    This function takes the output of itemize_document and generates XML items
    with appropriate attributes. Each item includes a unique ID, token count,
    and preserves whitespace.

    Args:
        items: List of tuples containing (item_text, attachment_type)

    Returns:
        List of tuples containing (item_xml, item_id)
    """
    xml_items = []
    prev_id = ""

    for _i, (text, attachment_type) in enumerate(items):
        # Escape XML special characters in the text
        escaped_text = escape_xml_chars(text)

        # Generate a unique ID for this item
        item_id = generate_id(prev_id, text)
        prev_id = item_id

        # Construct the XML item
        tok_placeholder = "____"  # Will be replaced with actual count later

        # Set additional attributes based on attachment type
        attachment_attr = ""
        if attachment_type == "following":
            attachment_attr = ' attach="following"'
        elif attachment_type == "preceding":
            attachment_attr = ' attach="preceding"'

        # Create the item element with attributes
        item_xml = f'<item xml:space="preserve" tok="{tok_placeholder}" id="{item_id}"{attachment_attr}>{escaped_text}</item>'

        # Add to the list
        xml_items.append((item_xml, item_id))

    return xml_items


def create_chunks(doc_with_units: str, max_chunk_size: int) -> str:
    """
    Organize units into chunks of manageable size.

    This function groups units into chunks, ensuring each chunk doesn't
    exceed the maximum token size. It handles oversized units by splitting
    them across chunks.

    Args:
        doc_with_units: XML document with unit tags
        max_chunk_size: Maximum token size for chunks

    Returns:
        XML document with chunk tags
    """
    logger.info(f"Creating chunks with max size of {max_chunk_size} tokens")

    try:
        # Parse the XML document
        parser = etree.XMLParser(remove_blank_text=False)
        root = etree.XML(doc_with_units.encode("utf-8"), parser)

        # Find all units
        units = root.xpath("//unit")
        if not units:
            logger.warning("No units found in document")
            return doc_with_units

        # Remove units from their current position
        for unit in units:
            unit.getparent().remove(unit)

        # Create initial chunk
        chunk_id = 1
        chunk = etree.SubElement(root, "chunk")
        chunk.set("id", f"{chunk_id:04d}")
        chunk.set("tok", "____")
        current_chunk_tokens = 0

        # Process each unit
        for unit in units:
            # Count tokens in the unit
            unit_xml = etree.tostring(unit, encoding="utf-8").decode("utf-8")
            unit_tokens = count_tokens(unit_xml)

            # Check if this unit is too large for a single chunk
            if unit_tokens > max_chunk_size:
                logger.info(f"Found oversized unit with {unit_tokens} tokens")
                handle_oversized_unit(root, unit, max_chunk_size, chunk_id)
                # Update chunk_id to account for new chunks
                chunk_id = len(root.xpath("//chunk"))
                # Create a new chunk for the next unit
                chunk = etree.SubElement(root, "chunk")
                chunk_id += 1
                chunk.set("id", f"{chunk_id:04d}")
                chunk.set("tok", "____")
                current_chunk_tokens = 0
            # Check if adding this unit would exceed the chunk size
            elif current_chunk_tokens + unit_tokens > max_chunk_size:
                # Start a new chunk
                chunk = etree.SubElement(root, "chunk")
                chunk_id += 1
                chunk.set("id", f"{chunk_id:04d}")
                chunk.set("tok", "____")
                chunk.append(unit)
                current_chunk_tokens = unit_tokens
            else:
                # Add to current chunk
                chunk.append(unit)
                current_chunk_tokens += unit_tokens

        # Update token counts
        for e in root.xpath("//*[@tok]"):
            if e.get("tok") == "____":
                # Count tokens in this element
                e_xml = etree.tostring(e, encoding="utf-8").decode("utf-8")
                e_tokens = count_tokens(e_xml)
                e.set("tok", str(e_tokens))

        # Serialize back to XML
        doc_with_chunks = etree.tostring(
            root, encoding="utf-8", method="xml", xml_declaration=True
        ).decode("utf-8")

        logger.info(f"Created {chunk_id} chunks")
        return doc_with_chunks

    except Exception as e:
        logger.error(f"Failed to create chunks: {e}")
        logger.info("Falling back to regex-based chunk creation")
        return create_chunks_regex(doc_with_units, max_chunk_size)


def handle_oversized_unit(
    doc_root, unit, max_chunk_size: int, chunk_id_start: int
) -> None:
    """
    Split an oversized unit across multiple chunks.

    This function splits a unit that's too large for a single chunk into
    multiple chunks, preserving the unit attributes.

    Args:
        doc_root: XML document root
        unit: Oversized unit element
        max_chunk_size: Maximum token size for chunks
        chunk_id_start: Starting chunk ID

    Returns:
        None (modifies doc_root in place)
    """
    logger.info("Handling oversized unit by splitting it across chunks")

    unit_type = unit.get("type", "unit")

    # Get all items in the unit
    items = unit.xpath(".//item")
    if not items:
        logger.warning("No items found in oversized unit")
        return

    # Group items into chunks
    current_items = []
    current_tokens = 0
    chunk_items = []  # List of item groups

    for item in items:
        item_xml = etree.tostring(item, encoding="utf-8").decode("utf-8")
        item_tokens = count_tokens(item_xml)

        # If adding this item exceeds the chunk size and we already have items
        if current_items and current_tokens + item_tokens > max_chunk_size:
            # Save current group and start a new one
            chunk_items.append(current_items)
            current_items = [item]
            current_tokens = item_tokens
        else:
            # Add to current group
            current_items.append(item)
            current_tokens += item_tokens

    # Add the last group if not empty
    if current_items:
        chunk_items.append(current_items)

    # Create chunks with the grouped items
    for i, item_group in enumerate(chunk_items):
        # Create a new chunk
        chunk = etree.SubElement(doc_root, "chunk")
        chunk_id = chunk_id_start + i
        chunk.set("id", f"{chunk_id:04d}")
        chunk.set("tok", "____")

        # Create a new unit inside the chunk
        new_unit = etree.SubElement(chunk, "unit")
        new_unit.set("type", unit_type)
        new_unit.set("tok", "____")

        # If this is a continuation, add an attribute
        if i > 0:
            new_unit.set("cont", "true")

        # Add items to the unit
        for item in item_group:
            # Copy the item
            item_copy = deepcopy(item)
            new_unit.append(item_copy)


def create_chunks_regex(doc_with_units: str, max_chunk_size: int) -> str:
    """
    Create chunks using regex when XML parsing fails.

    This is a fallback implementation that uses regex instead of XML parsing.

    Args:
        doc_with_units: XML document with unit tags
        max_chunk_size: Maximum token size for chunks

    Returns:
        XML document with chunk tags
    """
    logger.info("Using regex to create chunks")

    # Find the root tag
    root_match = re.search(r"<([^>]+)>", doc_with_units)
    if not root_match:
        logger.error("Could not find root tag")
        return doc_with_units

    root_tag = root_match.group(1)

    # Extract all units with their content
    unit_pattern = re.compile(r"<unit[^>]*>(.*?)</unit>", re.DOTALL)
    units = unit_pattern.findall(doc_with_units)

    if not units:
        logger.warning("No units found using regex")
        return doc_with_units

    # Create chunks
    chunks = []
    current_chunk = []
    current_tokens = 0

    for unit_content in units:
        unit_tokens = count_tokens(unit_content)

        # Check if this unit is too large for a single chunk
        if unit_tokens > max_chunk_size:
            # Split the oversized unit
            unit_type_match = re.search(r'<unit[^>]*type="([^"]+)"', unit_content)
            unit_type = unit_type_match.group(1) if unit_type_match else "unit"

            split_units = split_oversized_unit_regex(
                unit_type, unit_content, max_chunk_size, len(chunks) + 1
            )

            # Add each split unit as its own chunk
            for split_unit in split_units:
                chunks.append([split_unit])
        # Check if adding this unit would exceed the chunk size
        elif current_tokens + unit_tokens > max_chunk_size:
            # Save current chunk and start a new one
            if current_chunk:
                chunks.append(current_chunk)
                current_chunk = [unit_content]
                current_tokens = unit_tokens
        else:
            # Add to current chunk
            current_chunk.append(unit_content)
            current_tokens += unit_tokens

    # Add the last chunk if not empty
    if current_chunk:
        chunks.append(current_chunk)

    # Rebuild the document with chunks
    result = f'<?xml version="1.0" encoding="UTF-8"?>\n<{root_tag}>'

    for i, chunk_units in enumerate(chunks):
        chunk_id = i + 1
        chunk_content = "".join(
            f'<unit type="unit" tok="____">{unit}</unit>' for unit in chunk_units
        )
        result += f'\n  <chunk id="{chunk_id:04d}" tok="____">{chunk_content}</chunk>'

    result += f"\n</{root_tag}>"

    # Replace token placeholders with actual counts
    result = replace_tok_placeholder(result)

    logger.info(f"Created {len(chunks)} chunks using regex")
    return result


def split_oversized_unit_regex(
    unit_type: str, unit_content: str, max_chunk_size: int, chunk_id_start: int
) -> list[str]:
    """
    Split an oversized unit into multiple units using regex.

    Args:
        unit_type: Type of the unit ("chapter", "scene", "unit")
        unit_content: Content of the unit
        max_chunk_size: Maximum token size
        chunk_id_start: Starting chunk ID

    Returns:
        List of split unit contents
    """
    logger.info(f"Splitting oversized unit of type {unit_type}")

    # Extract items from the unit
    item_pattern = re.compile(r"<item[^>]*>.*?</item>", re.DOTALL)
    items = item_pattern.findall(unit_content)

    if not items:
        logger.warning("No items found in oversized unit")
        return [unit_content]

    # Group items into chunks
    result = []
    current_items = []
    current_tokens = 0

    for item in items:
        item_tokens = count_tokens(item)

        # If adding this item exceeds the chunk size and we already have items
        if current_items and current_tokens + item_tokens > max_chunk_size:
            # Save current group and start a new one
            result.append("".join(current_items))
            current_items = [item]
            current_tokens = item_tokens
        else:
            # Add to current group
            current_items.append(item)
            current_tokens += item_tokens

    # Add the last group if not empty
    if current_items:
        result.append("".join(current_items))

    logger.info(f"Split unit into {len(result)} parts")
    return result


def clean_consecutive_newlines(content: str) -> str:
    """Replace more than two consecutive newlines with exactly two."""
    return re.sub(r"\n{3,}", "\n\n", content)


def pretty_print_xml(xml_str: str) -> str:
    """Format XML string with proper indentation for readability."""
    try:
        parser = etree.XMLParser(remove_blank_text=True)
        root = etree.XML(xml_str.encode("utf-8"), parser)
        return etree.tostring(
            root, encoding="utf-8", pretty_print=True, xml_declaration=True
        ).decode("utf-8")
    except Exception:
        # If parsing fails, return the original string
        return xml_str


def create_backup(file_path: str | Path, stage: str = "") -> Path | None:
    """
    Create a timestamped backup of a file.

    Args:
        file_path: Path to the file to back up
        stage: Optional stage name to include in the backup filename

    Returns:
        Path to the backup file, or None if backup failed
    """
    file_path = Path(file_path)
    if not file_path.exists():
        logger.warning(f"Cannot back up non-existent file: {file_path}")
        return None

    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    if stage:
        backup_path = file_path.with_name(
            f"{file_path.stem}_{stage}_{timestamp}{file_path.suffix}"
        )
    else:
        backup_path = file_path.with_name(
            f"{file_path.stem}_{timestamp}{file_path.suffix}"
        )

    try:
        import shutil

        shutil.copy2(file_path, backup_path)
        logger.debug(f"Created backup: {backup_path}")
        return backup_path
    except Exception as e:
        logger.error(f"Failed to create backup: {e}")
        return None


def process_document(
    input_file: str,
    output_file: str,
    chunk_size: int = DEFAULT_CHUNK_SIZE,
    model: str = DEFAULT_MODEL,
    temperature: float = DEFAULT_TEMPERATURE,
    encoding: str = "utf-8",
    verbose: bool = False,
    backup: bool = False,
) -> dict[str, Any]:
    """
    Process a document through the semantic chunking pipeline.

    This function:
    1. Itemizes the document (splits it into paragraph-level items)
    2. Optionally performs semantic analysis using an LLM to identify chapter/scene boundaries
    3. Groups items into semantic units
    4. Organizes units into chunks of manageable size for further processing

    Args:
        input_file: Path to the input document
        output_file: Path to save the processed XML output
        chunk_size: Maximum token size for chunks
        model: LLM model to use for semantic analysis
        temperature: Temperature setting for the LLM
        encoding: Character encoding of the input file
        verbose: Enable verbose logging
        backup: Create backups at intermediate stages

    Returns:
        Dictionary with processing statistics
    """
    logger.info(f"Processing document: {input_file} -> {output_file}")
    logger.info(
        f"Using model: {model}, temperature: {temperature}, chunk size: {chunk_size}"
    )

    try:
        # Read the input file
        with open(input_file, encoding=encoding) as f:
            document_text = f.read()

        # Normalize line endings
        document_text = document_text.replace("\r\n", "\n")

        # Step 1: Itemize the document
        logger.info("Step 1: Itemizing document")
        itemized_paragraphs = itemize_document(document_text)
        logger.info(f"Identified {len(itemized_paragraphs)} paragraphs")

        # Step 2: Create XML items
        logger.info("Step 2: Creating XML items")
        xml_items = create_item_elements(itemized_paragraphs)

        # Create initial XML document with items
        items_xml = "\n".join(item_xml for item_xml, _ in xml_items)
        doc_xml = f"""<?xml version="1.0" encoding="UTF-8"?>
<doc>
{items_xml}
</doc>"""

        # Replace token placeholders
        doc_xml = replace_tok_placeholder(doc_xml)

        # Save backup after itemization if requested
        if backup:
            backup_path = create_backup(output_file, "items")
            if backup_path:
                Path(backup_path).write_text(doc_xml, encoding="utf-8")

        # Step 3: Semantic analysis to identify boundaries
        logger.info("Step 3: Performing semantic analysis")
        semantic_boundaries = semantic_analysis(doc_xml, model, temperature)

        # Step 4: Add unit tags based on semantic boundaries
        logger.info("Step 4: Adding unit tags")
        doc_with_units = add_unit_tags(doc_xml, semantic_boundaries)

        # Replace token placeholders
        doc_with_units = replace_tok_placeholder(doc_with_units)

        # Save backup after adding units if requested
        if backup:
            backup_path = create_backup(output_file, "units")
            if backup_path:
                Path(backup_path).write_text(doc_with_units, encoding="utf-8")

        # Step 5: Create chunks
        logger.info("Step 5: Creating chunks")
        doc_with_chunks = create_chunks(doc_with_units, chunk_size)

        # Replace token placeholders one last time
        doc_with_chunks = replace_tok_placeholder(doc_with_chunks)

        # Format for readability
        if verbose:
            doc_with_chunks = pretty_print_xml(doc_with_chunks)

        # Write the final result
        Path(output_file).write_text(doc_with_chunks, encoding="utf-8")

        # Count items, units, and chunks for statistics
        items_count = len(xml_items)
        units_count = len(re.findall(r"<unit[^>]*>", doc_with_chunks))
        chunks_count = len(re.findall(r"<chunk[^>]*>", doc_with_chunks))
        total_tokens = count_tokens(doc_with_chunks)

        logger.info(
            f"Processing complete: {items_count} items, {units_count} units, {chunks_count} chunks"
        )
        logger.info(f"Total tokens: {total_tokens}")

        return {
            "input_file": input_file,
            "output_file": output_file,
            "items_count": items_count,
            "units_count": units_count,
            "chunks_count": chunks_count,
            "total_tokens": total_tokens,
        }

    except Exception as e:
        logger.error(f"Processing failed: {e}")
        # Create a minimal valid XML in case of failure
        fallback_xml = f"""<?xml version="1.0" encoding="UTF-8"?>
<doc>
  <chunk id="0001" tok="1">
    <unit type="chapter" tok="1">
      <item xml:space="preserve" tok="1" id="000000-123456">
        Error processing document: {e!s}
      </item>
    </unit>
  </chunk>
</doc>
"""
        Path(output_file).write_text(fallback_xml, encoding="utf-8")

        return {
            "input_file": input_file,
            "output_file": output_file,
            "items_count": 1,
            "units_count": 1,
            "chunks_count": 1,
            "total_tokens": 1,
            "error": str(e),
        }


def semantic_analysis(
    doc_text: str, model: str, temperature: float
) -> list[tuple[str, str]]:
    """
    Use LLM to identify semantic boundaries in the document.

    This function sends the document to an LLM with instructions to identify
    semantic boundaries such as chapters, scenes, and thematic units. The
    LLM's response is processed to extract identified boundaries.

    Args:
        doc_text: Document text with item IDs
        model: LLM model to use
        temperature: Temperature setting for the LLM

    Returns:
        List of tuples (item_id, boundary_type) where boundary_type is
        "chapter", "scene", or "unit"
    """
    logger.info("Performing semantic analysis of document")

    # Extract item IDs from the document
    item_pattern = re.compile(r'<item[^>]*id="([^"]+)"[^>]*>.*?</item>', re.DOTALL)
    items = item_pattern.findall(doc_text)

    if not items:
        logger.warning("No items found in document, skipping semantic analysis")
        return []

    # Create a simplified version of the document for the LLM
    # This extracts just enough content for the LLM to understand the structure
    simplified_doc = []
    for match in item_pattern.finditer(doc_text):
        item_id = match.group(1)
        # Extract text content (remove tags and limit length)
        content = re.sub(r"<.*?>", "", match.group(0))
        # Truncate to first 100 chars for LLM efficiency
        content = content[:100] + ("..." if len(content) > 100 else "")
        simplified_doc.append(f"ID: {item_id}\nContent: {content}\n")

    "\n".join(simplified_doc)

    # Construct the prompt for the LLM

    try:
        # Call the LLM API (implementation will depend on your LLM library)
        # This is a placeholder for the actual API call
        # In a real implementation, you would use a library like litellm, openai, etc.
        logger.info(f"Calling LLM API with model: {model}")

        # Placeholder for LLM call
        # In the legacy code, this uses litellm.completion()
        # For now, we'll just create a dummy response
        response = {
            "choices": [{"message": {"content": "BOUNDARY: 000000-123456, chapter"}}]
        }

        logger.info("Successfully received response from LLM")

        # Process the LLM response
        boundaries = extract_and_parse_llm_response(response)

        # Apply fallback logic if too few boundaries
        if len(boundaries) < max(3, len(items) // 100):
            logger.warning("Too few boundaries identified, applying fallback logic")
            # Add fallback boundaries every N items as a backup
            interval = min(50, max(10, len(items) // 10))
            for i in range(0, len(items), interval):
                if i > 0:  # Skip the first item
                    item_id = items[i]
                    if not any(b[0] == item_id for b in boundaries):
                        boundaries.append((item_id, "unit"))

        logger.info(f"Identified {len(boundaries)} semantic boundaries")
        return boundaries

    except Exception as e:
        logger.error(f"Semantic analysis failed: {e}")
        # Fallback: create basic boundaries every N items
        logger.info("Using fallback boundary detection")
        boundaries = []
        interval = min(50, max(10, len(items) // 10))
        for i in range(0, len(items), interval):
            if i > 0:  # Skip the first item
                boundaries.append((items[i], "unit"))
        logger.info(f"Created {len(boundaries)} fallback boundaries")
        return boundaries


def extract_and_parse_llm_response(response) -> list[tuple[str, str]]:
    """
    Extract and parse boundaries from LLM response.

    Args:
        response: LLM API response object

    Returns:
        List of tuples (item_id, boundary_type)
    """
    boundaries = []

    # Extract the content from the response
    if hasattr(response, "choices") and response.choices:
        # For OpenAI-like responses
        content = response.choices[0].message.content
    elif isinstance(response, dict) and "choices" in response:
        # For dict-like responses
        content = response["choices"][0]["message"]["content"]
    else:
        # Fallback
        logger.warning("Unexpected response format from LLM")
        return boundaries

    # Parse the response to extract boundaries
    pattern = re.compile(r"BOUNDARY:\s*([^,]+),\s*([a-z]+)", re.IGNORECASE)
    matches = pattern.findall(content)

    for item_id, boundary_type in matches:
        item_id = item_id.strip()
        boundary_type = boundary_type.strip().lower()

        # Validate boundary type
        if boundary_type not in ["chapter", "scene", "unit"]:
            logger.warning(
                f"Invalid boundary type: {boundary_type}, defaulting to 'unit'"
            )
            boundary_type = "unit"

        boundaries.append((item_id, boundary_type))

    return boundaries


def add_unit_tags(itemized_doc: str, semantic_boundaries: list[tuple[str, str]]) -> str:
    """
    Add unit tags around sequences of items based on semantic analysis.

    This function groups items by their boundary types and adds appropriate
    XML unit tags with attributes.

    Args:
        itemized_doc: XML document with items
        semantic_boundaries: List of tuples (item_id, boundary_type)

    Returns:
        XML document with added unit tags
    """
    logger.info("Adding unit tags based on semantic boundaries")

    try:
        # Parse the XML document
        parser = etree.XMLParser(remove_blank_text=False)
        root = etree.XML(itemized_doc.encode("utf-8"), parser)

        # Create a map of item IDs to boundary types
        boundary_map = dict(semantic_boundaries)

        # Find all items
        items = root.xpath("//item")
        if not items:
            logger.warning("No items found in document")
            return itemized_doc

        # Keep track of units we're creating
        current_unit = None
        current_type = None
        current_items = []

        # Process each item
        for item in items:
            item_id = item.get("id")

            # Check if this item is a boundary
            if item_id in boundary_map:
                # If we have a current unit, finalize it
                if current_unit is not None and current_items:
                    # Create a new unit element
                    unit = etree.Element("unit")
                    unit.set("type", current_type)
                    unit.set("tok", "____")  # Will be replaced later

                    # Move items into the unit
                    parent = current_items[0].getparent()
                    unit_index = parent.index(current_items[0])
                    parent.insert(unit_index, unit)

                    for item_elem in current_items:
                        unit.append(item_elem)

                # Start a new unit
                current_type = boundary_map[item_id]
                current_unit = item_id
                current_items = [item]
            elif current_unit is not None:
                current_items.append(item)
            else:
                # No current unit, create a default one
                current_type = "unit"
                current_unit = item_id
                current_items = [item]

        # Handle the last unit
        if current_unit is not None and current_items:
            unit = etree.Element("unit")
            unit.set("type", current_type)
            unit.set("tok", "____")

            parent = current_items[0].getparent()
            unit_index = parent.index(current_items[0])
            parent.insert(unit_index, unit)

            for item_elem in current_items:
                unit.append(item_elem)

        # Serialize back to XML
        doc_with_units = etree.tostring(
            root, encoding="utf-8", method="xml", xml_declaration=True
        ).decode("utf-8")

        logger.info("Successfully added unit tags")
        return doc_with_units

    except Exception as e:
        logger.error(f"Failed to add unit tags: {e}")
        logger.info("Falling back to regex-based unit tagging")
        return add_unit_tags_regex(itemized_doc, semantic_boundaries)


def add_unit_tags_regex(
    itemized_doc: str, semantic_boundaries: list[tuple[str, str]]
) -> str:
    """
    Add unit tags using regex when XML parsing fails.

    This is a fallback implementation that uses regex instead of XML parsing.

    Args:
        itemized_doc: XML document with items
        semantic_boundaries: List of tuples (item_id, boundary_type)

    Returns:
        XML document with added unit tags
    """
    logger.info("Using regex to add unit tags")

    # Create a map of item IDs to boundary types
    boundary_map = dict(semantic_boundaries)

    # Add a dummy boundary at the beginning if none exists
    # This ensures the first items are also grouped
    first_item_match = re.search(r'<item[^>]*id="([^"]+)"', itemized_doc)
    if first_item_match and first_item_match.group(1) not in boundary_map:
        boundary_map[first_item_match.group(1)] = "unit"

    # Find all items with their IDs
    item_pattern = re.compile(r'<item[^>]*id="([^"]+)"[^>]*>.*?</item>', re.DOTALL)

    # Get all items and their positions
    items = []
    for match in item_pattern.finditer(itemized_doc):
        item_id = match.group(1)
        items.append((item_id, match.start(), match.end(), match.group(0)))

    # Check if we have items
    if not items:
        logger.warning("No items found using regex")
        return itemized_doc

    # Find boundaries
    boundaries = []
    for i, (item_id, _start, _end, _) in enumerate(items):
        if item_id in boundary_map:
            boundary_type = boundary_map[item_id]
            boundaries.append((i, boundary_type))

    # If no boundaries found, create default
    if not boundaries:
        logger.warning("No boundaries found, creating default")
        boundaries = [(0, "unit")]

    # Create units by inserting tags at appropriate positions
    result = itemized_doc
    offset = 0  # Track string position offset as we add tags

    # Add each unit
    for i, (boundary_index, boundary_type) in enumerate(boundaries):
        # Determine unit start and end
        start_index = boundary_index
        end_index = boundaries[i + 1][0] if i + 1 < len(boundaries) else len(items)

        if start_index >= end_index:
            continue  # Skip empty units

        # Get item positions
        unit_start = items[start_index][1]
        unit_end = items[end_index - 1][2]

        # Insert unit start tag
        unit_start_tag = f'<unit type="{boundary_type}" tok="____">'
        result = (
            result[: unit_start + offset]
            + unit_start_tag
            + result[unit_start + offset :]
        )
        offset += len(unit_start_tag)

        # Insert unit end tag
        unit_end_tag = "</unit>"
        result = (
            result[: unit_end + offset] + unit_end_tag + result[unit_end + offset :]
        )
        offset += len(unit_end_tag)

    logger.info("Successfully added unit tags using regex")
    return result


# The remaining functions from malmo_chunker.py will be implemented
# in subsequent commits. These include:
# - semantic_analysis: Use LLM to identify semantic boundaries
# - add_unit_tags: Group items into semantic units
# - create_chunks: Organize units into chunks of manageable size
# - handle_oversized_unit: Split large units into multiple chunks
</file>

<file path="e11ocutionist/cli.py">
#!/usr/bin/env python3
# this_file: src/e11ocutionist/cli.py
"""CLI module for e11ocutionist."""

import os
from pathlib import Path

from loguru import logger

from e11ocutionist import (
    chunker,
    elevenlabs_converter as converter,
    elevenlabs_synthesizer as synthesizer,
    entitizer,
    neifix,
    orator,
    tonedown,
)

VALID_MODELS = ["gpt-4", "gpt-3.5-turbo"]


def _configure_logging(debug: bool = False, verbose: bool = False) -> None:
    """Configure logging based on debug and verbose flags."""
    logger.remove()  # Remove default handler
    level = "DEBUG" if debug else "INFO" if verbose else "WARNING"
    logger.add(sink=lambda msg: print(msg), level=level)


def _validate_temperature(temperature: float) -> None:
    """Validate temperature value."""
    if not 0 <= temperature <= 1:
        msg = "Temperature must be between 0 and 1"
        raise ValueError(msg)


def _validate_model(model: str) -> None:
    """Validate model name."""
    if model not in VALID_MODELS:
        valid_models = ", ".join(VALID_MODELS)
        msg = f"Invalid model name. Must be one of: {valid_models}"
        raise ValueError(msg)


def _validate_min_distance(min_distance: int) -> None:
    """Validate minimum emphasis distance."""
    if min_distance <= 0:
        msg = "Minimum emphasis distance must be positive"
        raise ValueError(msg)


def _validate_output_dir(output_dir: str) -> None:
    """Validate output directory."""
    path = Path(output_dir)
    if path.exists() and not path.is_dir():
        msg = f"{output_dir} exists and is not a directory"
        raise NotADirectoryError(msg)


def chunk(
    input_file: str,
    output_file: str,
    model: str = "gpt-4",
    temperature: float = 0.2,
    chunk_size: int = 12288,
    verbose: bool = False,
    debug: bool = False,
) -> str:
    """Chunk text into semantic units."""
    _configure_logging(debug, verbose)
    _validate_model(model)
    _validate_temperature(temperature)

    logger.info("Running chunking step:")
    logger.info(f"Input: {input_file}")
    logger.info(f"Output: {output_file}")

    try:
        chunker.process_document(
            input_file=input_file,
            output_file=output_file,
            model=model,
            temperature=temperature,
            chunk_size=chunk_size,
            verbose=verbose,
        )
        logger.info(f"Chunking completed: {output_file}")
        return output_file
    except Exception as e:
        logger.error(f"Error: {e!s}")
        raise


def entitize(
    input_file: str,
    output_file: str,
    model: str = "gpt-4",
    temperature: float = 0.2,
    verbose: bool = False,
    debug: bool = False,
) -> str:
    """Process named entities in text."""
    _configure_logging(debug, verbose)
    _validate_model(model)
    _validate_temperature(temperature)

    logger.info("Running entitizing step:")
    logger.info(f"Input: {input_file}")
    logger.info(f"Output: {output_file}")

    try:
        entitizer.process_document(
            input_file=input_file,
            output_file=output_file,
            model=model,
            temperature=temperature,
            verbose=verbose,
        )
        logger.info(f"Entitizing completed: {output_file}")
        return output_file
    except Exception as e:
        logger.error(f"Error: {e!s}")
        raise


def orate(
    input_file: str,
    output_file: str,
    model: str = "gpt-4",
    temperature: float = 0.7,
    all_steps: bool = False,
    sentences: bool = False,
    words: bool = False,
    punctuation: bool = False,
    emotions: bool = False,
    verbose: bool = False,
    debug: bool = False,
) -> str:
    """Process text for speech synthesis."""
    _configure_logging(debug, verbose)
    _validate_model(model)
    _validate_temperature(temperature)

    if not any([all_steps, sentences, words, punctuation, emotions]):
        msg = "At least one processing step must be selected"
        raise ValueError(msg)

    logger.info("Running orating step:")
    logger.info(f"Input: {input_file}")
    logger.info(f"Output: {output_file}")

    steps = []
    if all_steps:
        steps.append("--all_steps")
    if sentences:
        steps.append("--sentences")
    if words:
        steps.append("--words")
    if punctuation:
        steps.append("--punctuation")
    if emotions:
        steps.append("--emotions")

    try:
        orator.process_document(
            input_file=input_file,
            output_file=output_file,
            model=model,
            temperature=temperature,
            steps=steps,
            verbose=verbose,
        )
        logger.info(f"Orating completed: {output_file}")
        return output_file
    except Exception as e:
        logger.error(f"Error: {e!s}")
        raise


def tone_down(
    input_file: str,
    output_file: str,
    model: str = "gpt-4",
    temperature: float = 0.1,
    min_em_distance: int = 5,
    verbose: bool = False,
    debug: bool = False,
) -> str:
    """Tone down emphasis in text."""
    _configure_logging(debug, verbose)
    _validate_model(model)
    _validate_temperature(temperature)
    _validate_min_distance(min_em_distance)

    logger.info("Running toning down step:")
    logger.info(f"Input: {input_file}")
    logger.info(f"Output: {output_file}")

    try:
        tonedown.process_document(
            input_file=input_file,
            output_file=output_file,
            model=model,
            temperature=temperature,
            min_emphasis_distance=min_em_distance,
            verbose=verbose,
        )
        logger.info(f"Toning down completed: {output_file}")
        return output_file
    except Exception as e:
        logger.error(f"Error: {e!s}")
        raise


def convert_11labs(
    input_file: str,
    output_file: str,
    dialog: bool = False,
    plaintext: bool = False,
    verbose: bool = False,
    debug: bool = False,
) -> str:
    """Convert text to ElevenLabs format."""
    _configure_logging(debug, verbose)

    if dialog and plaintext:
        msg = "Cannot enable both dialog and plaintext modes"
        raise ValueError(msg)

    logger.info("Running ElevenLabs conversion step:")
    logger.info(f"Input: {input_file}")
    logger.info(f"Output: {output_file}")

    try:
        converter.process_document(
            input_file=input_file,
            output_file=output_file,
            dialog=dialog,
            plaintext=plaintext,
            verbose=verbose,
        )
        logger.info(f"ElevenLabs conversion completed: {output_file}")
        return output_file
    except Exception as e:
        logger.error(f"Error: {e!s}")
        raise


def say(
    text: str | None = None,
    input_file: str | None = None,
    output_dir: str = "output",
    api_key: str | None = None,
    model_id: str = "eleven_multilingual_v2",
    output_format: str = "mp3_44100_128",
    verbose: bool = False,
    debug: bool = False,
) -> str:
    """Synthesize text using ElevenLabs voices."""
    _configure_logging(debug, verbose)
    _validate_output_dir(output_dir)

    if text is not None and input_file is not None:
        msg = "Cannot provide both text and input file"
        raise ValueError(msg)

    if text is None and input_file is not None:
        with open(input_file, encoding="utf-8") as f:
            text = f.read()
    elif text is None:
        msg = "Either text or input_file must be provided"
        raise ValueError(msg)

    if api_key is None:
        api_key = os.environ.get("ELEVENLABS_API_KEY")
        if not api_key:
            msg = "ElevenLabs API key not provided"
            raise ValueError(msg)

    logger.info("Running speech synthesis:")
    logger.info(f"Output directory: {output_dir}")

    try:
        synthesizer.synthesize_with_all_voices(
            text=text,  # type: ignore
            output_dir=output_dir,
            api_key=api_key,
            model_id=model_id,
            output_format=output_format,
        )
        logger.info(f"Synthesis completed: {output_dir}")
        return output_dir
    except Exception as e:
        logger.error(f"Error: {e!s}")
        raise


def fix_nei(
    input_file: str,
    output_file: str,
    verbose: bool = False,
    debug: bool = False,
) -> str:
    """Fix named entity issues in text."""
    _configure_logging(debug, verbose)

    if input_file == output_file:
        msg = "Input and output files must be different"
        raise ValueError(msg)

    logger.info("Running NEI fixing step:")
    logger.info(f"Input: {input_file}")
    logger.info(f"Output: {output_file}")

    try:
        neifix.transform_nei_content(
            input_file=input_file,
            output_file=output_file,
        )
        logger.info(f"NEI fixing completed: {output_file}")
        return output_file
    except Exception as e:
        logger.error(f"Error: {e!s}")
        raise


def process(  # noqa: PLR0913
    input_file: str,
    output_dir: str | None = None,
    start_step: str = "chunking",
    force_restart: bool = False,
    backup: bool = False,
    chunker_model: str = "gpt-4",
    chunker_temperature: float = 0.2,
    entitizer_model: str = "gpt-4",
    entitizer_temperature: float = 0.1,
    orator_model: str = "gpt-4",
    orator_temperature: float = 0.7,
    orator_all_steps: bool = True,
    orator_sentences: bool = False,
    orator_words: bool = False,
    orator_punctuation: bool = False,
    orator_emotions: bool = False,
    tonedown_model: str = "gpt-4",
    tonedown_temperature: float = 0.1,
    min_em_distance: int | None = 5,
    dialog_mode: bool = True,
    plaintext_mode: bool = False,
    verbose: bool = False,
    debug: bool = False,
) -> str:
    """
    Run the full e11ocutionist pipeline.

    Orchestrates the various processing steps from chunking to ElevenLabs conversion.
    """
    from e11ocutionist.e11ocutionist import (
        E11ocutionistPipeline,
        PipelineConfig,
        ProcessingStep,
    )

    _configure_logging(debug, verbose)

    if not Path(input_file).exists():
        logger.error(f"Input file not found: {input_file}")
        raise FileNotFoundError(f"Input file not found: {input_file}")

    if output_dir is None:
        output_dir_path = Path(input_file).parent / "output"
    else:
        output_dir_path = Path(output_dir)

    _validate_output_dir(str(output_dir_path))
    output_dir_path.mkdir(parents=True, exist_ok=True)

    try:
        start_step_enum = ProcessingStep[start_step.upper()]
    except KeyError:
        valid_steps = ", ".join([step.name for step in ProcessingStep])
        error_msg = f"Invalid start_step: {start_step}. Must be one of {valid_steps}"
        logger.error(error_msg)
        raise ValueError(error_msg) from None

    config = PipelineConfig(
        input_file=Path(input_file),
        output_dir=output_dir_path,
        start_step=start_step_enum,
        force_restart=force_restart,
        backup=backup,
        chunker_model=chunker_model,
        chunker_temperature=chunker_temperature,
        entitizer_model=entitizer_model,
        entitizer_temperature=entitizer_temperature,
        orator_model=orator_model,
        orator_temperature=orator_temperature,
        orator_all_steps=orator_all_steps,
        orator_sentences=orator_sentences,
        orator_words=orator_words,
        orator_punctuation=orator_punctuation,
        orator_emotions=orator_emotions,
        tonedown_model=tonedown_model,
        tonedown_temperature=tonedown_temperature,
        min_em_distance=min_em_distance,
        dialog_mode=dialog_mode,
        plaintext_mode=plaintext_mode,
        verbose=verbose,
        debug=debug,
    )

    logger.info(f"Starting e11ocutionist pipeline from step: {start_step}")
    logger.info(f"Input file: {config.input_file}")
    logger.info(f"Output directory: {config.output_dir}")

    pipeline = E11ocutionistPipeline(config)
    try:
        summary = pipeline.run()

        # Determine the final output path from the summary
        final_output_path = str(config.output_dir)  # Default to output_dir
        if summary and "progress" in summary and summary["progress"]:
            # Get the last executed step that completed successfully
            last_successful_step_output = None
            # Iterate in reverse order of typical execution to find the last one
            step_order = [
                ProcessingStep.ELEVENLABS_CONVERSION.name.lower(),
                ProcessingStep.TONING_DOWN.name.lower(),
                ProcessingStep.ORATING.name.lower(),
                ProcessingStep.ENTITIZING.name.lower(),
                ProcessingStep.CHUNKING.name.lower(),
            ]
            for step_name in step_order:
                if (
                    step_name in summary["progress"]
                    and summary["progress"][step_name].get("completed")
                    and "output_file" in summary["progress"][step_name]
                ):
                    output_file = summary["progress"][step_name]["output_file"]
                    last_successful_step_output = output_file
                    break

            if last_successful_step_output:
                final_output_path = str(last_successful_step_output)

        logger.info(
            "Pipeline completed successfully. "
            f"Final output considered: {final_output_path}"
        )
        return final_output_path
    except Exception as e:
        logger.error(f"Pipeline error: {e!s}")
        raise
</file>

<file path="e11ocutionist/e11ocutionist.py">
#!/usr/bin/env python3
# this_file: src/e11ocutionist/e11ocutionist.py
"""
E11ocutionist: A multi-stage document processing pipeline for transforming literary content
into enhanced speech synthesis markup.

This module serves as the main orchestrator for the processing pipeline.

Created by Adam Twardoch
"""

import json
import shutil
import datetime
from dataclasses import dataclass
from enum import Enum, auto
from pathlib import Path
from typing import Any

from loguru import logger


# Define processing steps as an enum
class ProcessingStep(Enum):
    """Processing steps in the e11ocutionist pipeline."""

    CHUNKING = auto()
    ENTITIZING = auto()
    ORATING = auto()
    TONING_DOWN = auto()
    ELEVENLABS_CONVERSION = auto()


@dataclass
class PipelineConfig:
    """Configuration for the e11ocutionist pipeline."""

    # Input and output
    input_file: Path
    output_dir: Path | None = None

    # Step configuration
    start_step: ProcessingStep = ProcessingStep.CHUNKING
    force_restart: bool = False
    backup: bool = False

    # Model configuration
    chunker_model: str = "gpt-4"
    chunker_temperature: float = 0.2
    entitizer_model: str = "gpt-4"
    entitizer_temperature: float = 0.1
    orator_model: str = "gpt-4"
    orator_temperature: float = 0.7
    tonedown_model: str = "gpt-4"
    tonedown_temperature: float = 0.1

    # Processing options
    orator_all_steps: bool = True
    orator_sentences: bool = False
    orator_words: bool = False
    orator_punctuation: bool = False
    orator_emotions: bool = False
    min_em_distance: int | None = None
    dialog_mode: bool = True
    plaintext_mode: bool = False

    # Other options
    verbose: bool = False
    debug: bool = False

    # Other fields will be added as needed


class E11ocutionistPipeline:
    """Main pipeline orchestrator for e11ocutionist."""

    def __init__(self, config: PipelineConfig):
        """Initialize the pipeline with the given configuration.

        Args:
            config: Pipeline configuration
        """
        self.config = config
        self.progress: dict[str, Any] = {}
        self.progress_file: Path | None = None

        # Set up logger
        if config.debug:
            logger.remove()
            logger.add(
                lambda msg: print(msg),
                level="DEBUG",
                format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>",
            )
        elif config.verbose:
            logger.remove()
            logger.add(
                lambda msg: print(msg),
                level="INFO",
                format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <level>{message}</level>",
            )

        # Set up output directory
        self._setup_output_directory()

    def _setup_output_directory(self) -> None:
        """Set up the output directory and progress file."""
        input_stem = self.config.input_file.stem

        if self.config.output_dir is None:
            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            self.config.output_dir = Path(f"{input_stem}_{timestamp}")

        self.config.output_dir.mkdir(parents=True, exist_ok=True)
        self.progress_file = self.config.output_dir / "progress.json"

        logger.info(f"Output directory: {self.config.output_dir}")

        # Initialize or load progress
        self._load_progress()

    def _load_progress(self) -> None:
        """Load progress from the progress file if it exists."""
        if (
            self.progress_file
            and self.progress_file.exists()
            and not self.config.force_restart
        ):
            try:
                with open(self.progress_file) as f:
                    self.progress = json.load(f)
                logger.info(f"Loaded progress from {self.progress_file}")
            except Exception as e:
                logger.error(f"Error loading progress: {e}")
                self.progress = {}
        else:
            self.progress = {}

    def _save_progress(self) -> None:
        """Save progress to the progress file."""
        if self.progress_file:
            try:
                with open(self.progress_file, "w") as f:
                    json.dump(self.progress, f, indent=2)
                logger.debug(f"Saved progress to {self.progress_file}")
            except Exception as e:
                logger.error(f"Error saving progress: {e}")

    def _create_backup(self, file_path: Path) -> None:
        """Create a backup of the specified file.

        Args:
            file_path: Path to the file to back up
        """
        if self.config.backup and file_path.exists():
            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_path = file_path.with_name(
                f"{file_path.stem}_{timestamp}{file_path.suffix}"
            )
            shutil.copy2(file_path, backup_path)
            logger.debug(f"Created backup: {backup_path}")

    def run(self) -> dict[str, Any]:
        """Run the e11ocutionist pipeline.

        Returns:
            Dictionary with processing results and summary
        """
        logger.info("Starting e11ocutionist pipeline")

        # Track what steps were executed in this run
        executed_steps = []

        # Process each step sequentially
        if self._should_run_step(ProcessingStep.CHUNKING):
            self._run_chunking()
            executed_steps.append("chunking")

        if self._should_run_step(ProcessingStep.ENTITIZING):
            self._run_entitizing()
            executed_steps.append("entitizing")

        if self._should_run_step(ProcessingStep.ORATING):
            self._run_orating()
            executed_steps.append("orating")

        if self._should_run_step(ProcessingStep.TONING_DOWN):
            self._run_toning_down()
            executed_steps.append("toning_down")

        if self._should_run_step(ProcessingStep.ELEVENLABS_CONVERSION):
            self._run_elevenlabs_conversion()
            executed_steps.append("elevenlabs_conversion")

        # Create summary
        summary = {
            "input_file": str(self.config.input_file),
            "output_dir": str(self.config.output_dir),
            "executed_steps": executed_steps,
            "progress": self.progress,
        }

        # Save summary to file
        summary_file = self.config.output_dir / "_summary.txt"
        with open(summary_file, "w") as f:
            f.write("E11ocutionist Pipeline Summary\n")
            f.write("============================\n\n")
            f.write(f"Input file: {self.config.input_file}\n")
            f.write(f"Output directory: {self.config.output_dir}\n\n")
            f.write(f"Executed steps: {', '.join(executed_steps)}\n\n")
            f.write("Step details:\n")
            for step, details in self.progress.items():
                if isinstance(details, dict):
                    f.write(f"  {step}:\n")
                    for key, value in details.items():
                        f.write(f"    {key}: {value}\n")
                else:
                    f.write(f"  {step}: {details}\n")

        logger.info(f"Pipeline completed. Summary saved to {summary_file}")
        return summary

    def _should_run_step(self, step: ProcessingStep) -> bool:
        """Determine if a step should be run based on start_step and progress.

        Args:
            step: The step to check

        Returns:
            True if the step should be run, False otherwise
        """
        # If force_restart, always run if step is at or after start_step
        if self.config.force_restart:
            return step.value >= self.config.start_step.value

        # Otherwise, check if step is completed in progress
        step_name = step.name.lower()
        if step_name in self.progress and self.progress[step_name].get(
            "completed", False
        ):
            return False

        # If step is at or after start_step, run it
        return step.value >= self.config.start_step.value

    def _run_chunking(self) -> None:
        """Run the chunking step."""
        from .chunker import process_document

        logger.info("Running chunking step")

        step_name = ProcessingStep.CHUNKING.name.lower()
        input_file = self.config.input_file
        output_file = self.config.output_dir / f"{input_file.stem}_step1_chunked.xml"

        # Create backup if needed
        self._create_backup(output_file)

        # Run the chunking process
        try:
            result = process_document(
                input_file=str(input_file),
                output_file=str(output_file),
                model=self.config.chunker_model,
                temperature=self.config.chunker_temperature,
                verbose=self.config.verbose,
                backup=self.config.backup,
            )

            # Update progress
            self.progress[step_name] = {
                "input_file": str(input_file),
                "output_file": str(output_file),
                "timestamp": datetime.datetime.now().isoformat(),
                "completed": True,
                "result": result,
            }
            self._save_progress()

            logger.info(f"Chunking completed: {output_file}")

        except Exception as e:
            logger.error(f"Error in chunking step: {e}")
            self.progress[step_name] = {
                "input_file": str(input_file),
                "output_file": str(output_file),
                "timestamp": datetime.datetime.now().isoformat(),
                "completed": False,
                "error": str(e),
            }
            self._save_progress()
            raise

    def _run_entitizing(self) -> None:
        """Run the entitizing step."""
        from .entitizer import process_document

        logger.info("Running entitizing step")

        step_name = ProcessingStep.ENTITIZING.name.lower()
        prev_step = ProcessingStep.CHUNKING.name.lower()

        if prev_step in self.progress and self.progress[prev_step].get(
            "completed", False
        ):
            input_file = Path(self.progress[prev_step]["output_file"])
        else:
            msg = "Cannot run entitizing: previous step (chunking) not completed"
            raise ValueError(msg)

        output_file = (
            self.config.output_dir
            / f"{self.config.input_file.stem}_step2_entitized.xml"
        )

        # Create backup if needed
        self._create_backup(output_file)

        # Run the entitizing process
        try:
            result = process_document(
                input_file=str(input_file),
                output_file=str(output_file),
                model=self.config.entitizer_model,
                temperature=self.config.entitizer_temperature,
                verbose=self.config.verbose,
                backup=self.config.backup,
            )

            # Update progress
            self.progress[step_name] = {
                "input_file": str(input_file),
                "output_file": str(output_file),
                "timestamp": datetime.datetime.now().isoformat(),
                "completed": True,
                "result": result,
            }
            self._save_progress()

            logger.info(f"Entitizing completed: {output_file}")

        except Exception as e:
            logger.error(f"Error in entitizing step: {e}")
            self.progress[step_name] = {
                "input_file": str(input_file),
                "output_file": str(output_file),
                "timestamp": datetime.datetime.now().isoformat(),
                "completed": False,
                "error": str(e),
            }
            self._save_progress()
            raise

    def _run_orating(self) -> None:
        """Run the orating step."""
        from .orator import process_document

        logger.info("Running orating step")

        step_name = ProcessingStep.ORATING.name.lower()
        prev_step = ProcessingStep.ENTITIZING.name.lower()

        if prev_step in self.progress and self.progress[prev_step].get(
            "completed", False
        ):
            input_file = Path(self.progress[prev_step]["output_file"])
        else:
            msg = "Cannot run orating: previous step (entitizing) not completed"
            raise ValueError(msg)

        output_file = (
            self.config.output_dir / f"{self.config.input_file.stem}_step3_orated.xml"
        )

        # Create backup if needed
        self._create_backup(output_file)

        # Determine which orating steps to run
        if self.config.orator_all_steps:
            steps = ["--all_steps"]
        else:
            steps = []
            if self.config.orator_sentences:
                steps.append("--sentences")
            if self.config.orator_words:
                steps.append("--words")
            if self.config.orator_punctuation:
                steps.append("--punctuation")
            if self.config.orator_emotions:
                steps.append("--emotions")

        # Run the orating process
        try:
            result = process_document(
                input_file=str(input_file),
                output_file=str(output_file),
                model=self.config.orator_model,
                temperature=self.config.orator_temperature,
                verbose=self.config.verbose,
                backup=self.config.backup,
                steps=steps,
            )

            # Update progress
            self.progress[step_name] = {
                "input_file": str(input_file),
                "output_file": str(output_file),
                "timestamp": datetime.datetime.now().isoformat(),
                "completed": True,
                "result": result,
                "steps": steps,
            }
            self._save_progress()

            logger.info(f"Orating completed: {output_file}")

        except Exception as e:
            logger.error(f"Error in orating step: {e}")
            self.progress[step_name] = {
                "input_file": str(input_file),
                "output_file": str(output_file),
                "timestamp": datetime.datetime.now().isoformat(),
                "completed": False,
                "error": str(e),
                "steps": steps,
            }
            self._save_progress()
            raise

    def _run_toning_down(self) -> None:
        """Run the toning down step."""
        from .tonedown import process_document

        logger.info("Running toning down step")

        step_name = ProcessingStep.TONING_DOWN.name.lower()
        prev_step = ProcessingStep.ORATING.name.lower()

        if prev_step in self.progress and self.progress[prev_step].get(
            "completed", False
        ):
            input_file = Path(self.progress[prev_step]["output_file"])
        else:
            msg = "Cannot run toning down: previous step (orating) not completed"
            raise ValueError(msg)

        output_file = (
            self.config.output_dir
            / f"{self.config.input_file.stem}_step4_toneddown.xml"
        )

        # Create backup if needed
        self._create_backup(output_file)

        # Run the toning down process
        try:
            em_args = (
                {"min_distance": self.config.min_em_distance}
                if self.config.min_em_distance
                else {}
            )

            result = process_document(
                input_file=str(input_file),
                output_file=str(output_file),
                model=self.config.tonedown_model,
                temperature=self.config.tonedown_temperature,
                verbose=self.config.verbose,
                **em_args,
            )

            # Update progress
            self.progress[step_name] = {
                "input_file": str(input_file),
                "output_file": str(output_file),
                "timestamp": datetime.datetime.now().isoformat(),
                "completed": True,
                "result": result,
                "em_min_distance": self.config.min_em_distance,
            }
            self._save_progress()

            logger.info(f"Toning down completed: {output_file}")

        except Exception as e:
            logger.error(f"Error in toning down step: {e}")
            self.progress[step_name] = {
                "input_file": str(input_file),
                "output_file": str(output_file),
                "timestamp": datetime.datetime.now().isoformat(),
                "completed": False,
                "error": str(e),
                "em_min_distance": self.config.min_em_distance,
            }
            self._save_progress()
            raise

    def _run_elevenlabs_conversion(self) -> None:
        """Run the ElevenLabs conversion step."""
        from .elevenlabs_converter import process_document

        logger.info("Running ElevenLabs conversion step")

        step_name = ProcessingStep.ELEVENLABS_CONVERSION.name.lower()
        prev_step = ProcessingStep.TONING_DOWN.name.lower()

        if prev_step in self.progress and self.progress[prev_step].get(
            "completed", False
        ):
            input_file = Path(self.progress[prev_step]["output_file"])
        else:
            msg = "Cannot run ElevenLabs conversion: previous step (toning down) not completed"
            raise ValueError(msg)

        output_file = (
            self.config.output_dir / f"{self.config.input_file.stem}_step5_11labs.txt"
        )

        # Create backup if needed
        self._create_backup(output_file)

        # Run the ElevenLabs conversion process
        try:
            result = process_document(
                input_file=str(input_file),
                output_file=str(output_file),
                dialog=self.config.dialog_mode,
                plaintext=self.config.plaintext_mode,
                verbose=self.config.verbose,
            )

            # Update progress
            self.progress[step_name] = {
                "input_file": str(input_file),
                "output_file": str(output_file),
                "timestamp": datetime.datetime.now().isoformat(),
                "completed": True,
                "result": result,
                "dialog": self.config.dialog_mode,
                "plaintext": self.config.plaintext_mode,
            }
            self._save_progress()

            logger.info(f"ElevenLabs conversion completed: {output_file}")

        except Exception as e:
            logger.error(f"Error in ElevenLabs conversion step: {e}")
            self.progress[step_name] = {
                "input_file": str(input_file),
                "output_file": str(output_file),
                "timestamp": datetime.datetime.now().isoformat(),
                "completed": False,
                "error": str(e),
                "dialog": self.config.dialog_mode,
                "plaintext": self.config.plaintext_mode,
            }
            self._save_progress()
            raise


def main() -> None:
    """Main entry point for e11ocutionist."""
    try:
        # Example usage
        config = PipelineConfig(
            input_file=Path("path/to/input.xml"),
            output_dir=Path("path/to/output"),
            start_step=ProcessingStep.CHUNKING,
            force_restart=False,
            backup=False,
            chunker_model="gpt-4",
            chunker_temperature=0.2,
            entitizer_model="gpt-4",
            entitizer_temperature=0.1,
            orator_model="gpt-4",
            orator_temperature=0.7,
            tonedown_model="gpt-4",
            tonedown_temperature=0.1,
            orator_all_steps=True,
            orator_sentences=False,
            orator_words=False,
            orator_punctuation=False,
            orator_emotions=False,
            min_em_distance=None,
            dialog_mode=True,
            plaintext_mode=False,
            verbose=False,
            debug=False,
        )
        pipeline = E11ocutionistPipeline(config)
        result = pipeline.run()
        logger.info("Processing completed: %s", result)

    except Exception as e:
        logger.error("An error occurred: %s", str(e))
        raise


if __name__ == "__main__":
    main()
</file>

<file path="e11ocutionist/elevenlabs_converter.py">
#!/usr/bin/env python3
# this_file: src/e11ocutionist/elevenlabs_converter.py
"""
ElevenLabs converter for e11ocutionist.

This module converts processed XML files to a text format compatible with ElevenLabs text-to-speech,
with options for processing dialog and handling plaintext input.
"""

import re
from pathlib import Path
from typing import Any
from lxml import etree
from loguru import logger

from .utils import clean_consecutive_newlines, parse_xml, unescape_xml_chars


def extract_text_from_xml(xml_root: etree._Element) -> str:
    """
    Extract text from XML document, processing each item element.

    Args:
        xml_root: XML root element

    Returns:
        Extracted and processed text
    """
    result = []

    for chunk in xml_root.xpath("//chunk"):
        for item in chunk.xpath(".//item"):
            # Get the inner XML of the item
            inner_xml = etree.tostring(item, encoding="utf-8", method="xml").decode(
                "utf-8"
            )
            # Extract just the content between the opening and closing tags
            item_text = re.sub(
                r"^<item[^>]*>(.*)</item>$", r"\1", inner_xml, flags=re.DOTALL
            )

            # Process item text
            # Convert <em>...</em> to "..."
            item_text = re.sub(r"<em>(.*?)</em>", r'"\1"', item_text)
            # Convert <nei new="true">...</nei> to "..."
            item_text = re.sub(
                r'<nei\s+new="true"[^>]*>(.*?)</nei>', r'"\1"', item_text
            )
            # Convert remaining <nei>...</nei> to its content
            item_text = re.sub(r"<nei[^>]*>(.*?)</nei>", r"\1", item_text)
            # Replace <hr/> with <break time="0.5s" />
            item_text = re.sub(r"<hr\s*/?>", r'<break time="0.5s" />', item_text)
            # Strip any remaining HTML/XML tags
            item_text = re.sub(r"<[^>]+>", "", item_text)

            # Unescape HTML entities
            item_text = unescape_xml_chars(item_text)

            result.append(item_text)

    # Join all items with double newlines
    return "\n\n".join(result)


def process_dialog(text: str) -> str:
    """
    Process dialog in text, converting em dashes to appropriate quotation marks.

    Args:
        text: Text with dialog lines

    Returns:
        Text with processed dialog
    """
    # Flag indicating whether we're inside dialog
    in_dialog = False

    # Process the text line by line
    lines = text.split("\n")
    processed_lines = []

    for line in lines:
        if line.strip().startswith("— "):
            # Line starts with em dash and space - it's a dialog line
            in_dialog = True
            line = "<q>" + line[2:]  # Replace em dash with <q> tag

        if in_dialog:
            # Process dialog toggles (— ) within the line
            parts = line.split(" — ")
            if len(parts) > 1:
                # There are dialog toggles in this line
                for i in range(len(parts)):
                    if i > 0:
                        # For toggles, close previous and open new
                        parts[i] = "</q><q>" + parts[i]
                line = " ".join(parts)

            # Check if line should end dialog
            if in_dialog and not any(
                next_line.strip().startswith("— ")
                for next_line in lines[lines.index(line) + 1 : lines.index(line) + 2]
                if next_line.strip()
            ):
                # No dialog in next line, close this one
                line += "</q>"
                in_dialog = False

        processed_lines.append(line)

    result = "\n".join(processed_lines)

    # Final post-processing for dialog
    # Replace <q> at start of line with opening smart quote
    result = re.sub(r"^\s*<q>", '"', result, flags=re.MULTILINE)
    # Replace </q> at end of line with closing smart quote
    result = re.sub(r"</q>\s*$", '"', result, flags=re.MULTILINE)
    # Replace other <q> and </q> tags with special markers for ElevenLabs
    result = re.sub(r"<q>", "; OPEN_Q", result)
    result = re.sub(r"</q>", "CLOSE_Q;", result)

    # Replace break tags with ellipsis
    result = re.sub(r'<break time="[^"]*"\s*/>', "...", result)

    # Normalize consecutive newlines
    result = clean_consecutive_newlines(result)

    # Add some spacing for readability
    result = re.sub(r"\n", "\n\n", result)

    return result


def process_document(
    input_file: str,
    output_file: str,
    dialog: bool = True,
    plaintext: bool = False,
    verbose: bool = False,
) -> dict[str, Any]:
    """
    Convert an XML document to a text format compatible with ElevenLabs.

    Args:
        input_file: Path to the input XML file
        output_file: Path to save the converted text output
        dialog: Whether to process dialog (default: True)
        plaintext: Whether to treat the input as plaintext (default: False)
        verbose: Enable verbose logging

    Returns:
        Dictionary with processing statistics
    """
    if verbose:
        logger.info(f"Processing {input_file} to {output_file}")
        logger.info(f"Dialog processing: {dialog}, Plaintext mode: {plaintext}")

    # Read the input file
    input_content = Path(input_file).read_text(encoding="utf-8")

    # Process the content
    output_text = ""

    if plaintext:
        # Plaintext mode - just process the content directly
        output_text = input_content
        if dialog:
            output_text = process_dialog(output_text)
    else:
        # XML mode - parse the XML and extract the text
        xml_root = parse_xml(input_content)
        if xml_root is None:
            logger.error(f"Failed to parse XML from {input_file}")
            return {"success": False, "error": "XML parsing failed"}

        output_text = extract_text_from_xml(xml_root)
        if dialog:
            output_text = process_dialog(output_text)

    # Write the output file
    Path(output_file).write_text(output_text, encoding="utf-8")

    if verbose:
        logger.info(f"Conversion completed: {output_file}")

    return {
        "input_file": input_file,
        "output_file": output_file,
        "dialog_processed": dialog,
        "plaintext_mode": plaintext,
        "success": True,
    }
</file>

<file path="e11ocutionist/elevenlabs_synthesizer.py">
#!/usr/bin/env python3
# this_file: src/e11ocutionist/elevenlabs_synthesizer.py
"""
ElevenLabs synthesizer for e11ocutionist.

This module provides functionality to synthesize text using
personal ElevenLabs voices and save the result as audio files.
"""

import os
import re
from typing import Any
import backoff
from tenacity import retry, stop_after_attempt, wait_exponential
from loguru import logger
from rich.progress import Progress, SpinnerColumn, TextColumn, TimeElapsedColumn

try:
    from elevenlabs import Voice, VoiceSettings, generate, set_api_key, voices
    from elevenlabs.api import Voices
except ImportError:
    logger.error(
        "ElevenLabs API not available. Please install it with: pip install elevenlabs"
    )

    # Define placeholders to prevent errors
    class Voice:
        pass

    class VoiceSettings:
        pass

    class Voices:
        pass

    def generate(*args, **kwargs):
        msg = "ElevenLabs API not available"
        raise ImportError(msg)

    def set_api_key(*args, **kwargs):
        pass

    def voices(*args, **kwargs):
        return []


def sanitize_filename(name: str) -> str:
    """
    Sanitize a string to be used as a filename.

    Args:
        name: Name to sanitize

    Returns:
        Sanitized filename
    """
    # Remove characters that are invalid in filenames
    s = re.sub(r'[\\/*?:"<>|]', "", name)
    # Replace spaces with underscores
    s = re.sub(r"\s+", "_", s)
    # Remove any other potentially problematic characters
    s = re.sub(r"[^\w\-.]", "", s)
    # Truncate if too long
    return s[:100]


@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))
def get_personal_voices(api_key: str) -> list[Voice]:
    """
    Get all personal (cloned, generated, or professional) voices from ElevenLabs.

    Args:
        api_key: ElevenLabs API key

    Returns:
        List of Voice objects
    """
    set_api_key(api_key)

    # Get all voices
    all_voices = voices()

    # Filter for personal voices (cloned, generated, professional)
    personal_voices = [
        v
        for v in all_voices
        if any(
            category in v.category
            for category in ["cloned", "generated", "professional"]
        )
    ]

    logger.info(f"Found {len(personal_voices)} personal voices")
    return personal_voices


@backoff.on_exception(
    backoff.expo, (Exception), max_tries=3, jitter=backoff.full_jitter
)
def synthesize_with_voice(
    text: str,
    voice: Voice,
    output_dir: str,
    model_id: str = "eleven_multilingual_v2",
    output_format: str = "mp3_44100_128",
) -> str:
    """
    Synthesize text with a specific voice and save to a file.

    Args:
        text: Text to synthesize
        voice: Voice object to use
        output_dir: Directory to save the audio file
        model_id: ElevenLabs model ID to use
        output_format: Output format to use

    Returns:
        Path to the saved audio file
    """
    # Get API key from environment
    api_key = os.environ.get("ELEVENLABS_API_KEY")
    if not api_key:
        msg = "ElevenLabs API key not found in environment"
        raise ValueError(msg)

    # Set API key
    set_api_key(api_key)

    # Create output directory if it doesn't exist
    os.makedirs(output_dir, exist_ok=True)

    # Sanitize the voice name for filename
    sanitized_name = sanitize_filename(voice.name)
    output_path = os.path.join(output_dir, f"{voice.voice_id}--{sanitized_name}.mp3")

    # Check if file already exists
    if os.path.exists(output_path):
        logger.info(f"File already exists, skipping: {output_path}")
        return output_path

    # Generate audio
    audio = generate(
        text=text, voice=voice, model=model_id, output_format=output_format
    )

    # Save to file
    with open(output_path, "wb") as f:
        f.write(audio)

    logger.info(f"Saved audio to: {output_path}")
    return output_path


def synthesize_with_all_voices(
    text: str,
    output_dir: str = "output_audio",
    api_key: str | None = None,
    model_id: str = "eleven_multilingual_v2",
    output_format: str = "mp3_44100_128",
) -> dict[str, Any]:
    """
    Synthesize text using all personal ElevenLabs voices.

    Args:
        text: Text to synthesize
        output_dir: Directory to save audio files
        api_key: ElevenLabs API key (falls back to environment variable)
        model_id: ElevenLabs model ID to use
        output_format: Output format to use

    Returns:
        Dictionary with processing statistics
    """
    # Get API key from environment if not provided
    if api_key is None:
        api_key = os.environ.get("ELEVENLABS_API_KEY")
        if not api_key:
            msg = "ElevenLabs API key not provided and not found in environment"
            raise ValueError(msg)

    set_api_key(api_key)

    # Get all personal voices
    personal_voices = get_personal_voices(api_key)

    # Create output directory
    os.makedirs(output_dir, exist_ok=True)

    # Track results
    results = {
        "success": True,
        "voices_processed": 0,
        "voices_succeeded": 0,
        "voices_failed": 0,
        "output_files": [],
    }

    # Process each voice with a progress bar
    with Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        TimeElapsedColumn(),
    ) as progress:
        task = progress.add_task(
            f"Synthesizing with {len(personal_voices)} voices...",
            total=len(personal_voices),
        )

        for voice in personal_voices:
            try:
                logger.info(f"Synthesizing with voice: {voice.name} ({voice.voice_id})")
                output_path = synthesize_with_voice(
                    text=text,
                    voice=voice,
                    output_dir=output_dir,
                    model_id=model_id,
                    output_format=output_format,
                )
                results["voices_succeeded"] += 1
                results["output_files"].append(output_path)
            except Exception as e:
                logger.error(f"Failed to synthesize with voice {voice.name}: {e}")
                results["voices_failed"] += 1

            results["voices_processed"] += 1
            progress.update(task, advance=1)

    logger.info(
        f"Synthesized with {results['voices_succeeded']} voices (failed: {results['voices_failed']})"
    )

    return results
</file>

<file path="e11ocutionist/entitizer.py">
#!/usr/bin/env python3
# this_file: src/e11ocutionist/entitizer.py
"""
Entitizer module for e11ocutionist.

This module identifies and tags Named Entities of Interest (NEIs) in documents
for consistent pronunciation in speech synthesis.
"""

import json
import os
import re
from typing import Any
from copy import deepcopy

import backoff
from lxml import etree
from loguru import logger
from dotenv import load_dotenv

from e11ocutionist.utils import parse_xml, serialize_xml, create_backup

# Load environment variables from .env file
load_dotenv()

# Constants
DEFAULT_MODEL = "gpt-4o"
FALLBACK_MODEL = "gpt-4o-mini"
DEFAULT_TEMPERATURE = 0.1


def get_chunks(root: etree._Element) -> list[etree._Element]:
    """
    Extract chunks from the XML document.

    Args:
        root: Root XML element

    Returns:
        List of chunk elements
    """
    return root.xpath("//chunk")


def get_chunk_text(chunk: etree._Element) -> str:
    """
    Extract the raw text content of a chunk while preserving whitespace.

    Args:
        chunk: XML chunk element

    Returns:
        Raw text content of the chunk with all whitespace preserved
    """
    # Using etree.tostring to preserve all whitespace in the XML structure
    chunk_xml = etree.tostring(chunk, encoding="utf-8", method="xml").decode("utf-8")
    return chunk_xml


def save_current_state(
    root: etree._Element,
    nei_dict: dict[str, dict[str, Any]],
    output_file: str,
    nei_dict_file: str | None = None,
    chunk_id: str | None = None,
    backup: bool = False,
) -> None:
    """
    Save current state to the output file after processing a chunk.

    Args:
        root: Current XML root element
        nei_dict: Current NEI dictionary
        output_file: File to save output
        nei_dict_file: File to save NEI dictionary (defaults to output_file
                       with _nei_dict.json)
        chunk_id: ID of the chunk that was processed
        backup: Whether to create backup copies with timestamps
    """
    try:
        # Save current XML state
        xml_string = serialize_xml(root)
        with open(output_file, "w", encoding="utf-8") as f:
            f.write(xml_string)

        # Save current NEI dictionary to a JSON file
        if nei_dict_file is None:
            nei_dict_file = f"{os.path.splitext(output_file)[0]}_nei_dict.json"

        with open(nei_dict_file, "w", encoding="utf-8") as f:
            json.dump(nei_dict, f, ensure_ascii=False, indent=2)

        # Create backup if requested
        if backup:
            chunk_suffix = f"_{chunk_id}" if chunk_id else ""
            backup_file = create_backup(output_file, f"entitizer{chunk_suffix}")
            nei_backup_file = create_backup(nei_dict_file, f"nei{chunk_suffix}")

            if backup_file and nei_backup_file:
                logger.debug(f"Created backups at: {backup_file} and {nei_backup_file}")

        suffix = f" after processing chunk {chunk_id}" if chunk_id else ""
        logger.info(f"Saved current state{suffix}")
        logger.debug(f"Files saved to {output_file} and {nei_dict_file}")

    except Exception as e:
        logger.error(f"Error saving current state: {e}")


def extract_item_elements(xml_text: str) -> dict[str, str]:
    """
    Extract item elements from XML text and map them by ID.

    Args:
        xml_text: XML text containing item elements

    Returns:
        Dictionary mapping item IDs to their full XML content
    """
    item_map = {}
    # Pattern to match complete <item> elements including their attributes and content
    pattern = r"<item\s+([^>]*)>(.*?)</item>"

    # Find all item elements
    for match in re.finditer(pattern, xml_text, re.DOTALL):
        # Extract attributes and content
        attributes = match.group(1)
        content = match.group(2)

        # Extract ID attribute
        id_match = re.search(r'id="([^"]*)"', attributes)
        if id_match:
            item_id = id_match.group(1)
            # Store the complete item element
            item_map[item_id] = f"<item {attributes}>{content}</item>"

    return item_map


def merge_tagged_items(
    original_chunk: etree._Element, tagged_items: dict[str, str]
) -> etree._Element:
    """
    Merge tagged items back into the original chunk.

    Args:
        original_chunk: Original XML chunk
        tagged_items: Dictionary of tagged items mapped by ID

    Returns:
        Updated chunk with merged tagged items
    """
    # Create a deep copy of the original chunk to avoid modifying it
    updated_chunk = deepcopy(original_chunk)

    # Find all items in the updated chunk
    for item in updated_chunk.xpath(".//item"):
        item_id = item.get("id")
        if item_id in tagged_items:
            # Parse the tagged item
            try:
                tagged_item = etree.fromstring(
                    tagged_items[item_id].encode("utf-8"),
                    etree.XMLParser(remove_blank_text=False, recover=True),
                )

                # Replace the content of the original item with the tagged content
                # This preserves the attributes of the original item
                item.clear()

                # Copy attributes from tagged item
                for name, value in tagged_item.attrib.items():
                    item.set(name, value)

                # Copy children and text from tagged item
                if tagged_item.text:
                    item.text = tagged_item.text

                for child in tagged_item:
                    item.append(deepcopy(child))

                if tagged_item.tail:
                    item.tail = tagged_item.tail

            except Exception as e:
                logger.error(f"Error merging tagged item {item_id}: {e}")

    return updated_chunk


@backoff.on_exception(backoff.expo, Exception, max_tries=3)
def identify_entities(
    chunk_text: str, nei_dict: dict[str, dict[str, Any]], model: str, temperature: float
) -> str:
    """
    Identify Named Entities of Interest (NEIs) in a chunk of text using an LLM.

    Args:
        chunk_text: XML text of the chunk
        nei_dict: Current dictionary of NEIs
        model: LLM model to use
        temperature: Temperature setting for the LLM

    Returns:
        Chunk text with NEIs tagged
    """
    try:
        # Import here to avoid circular imports
        from litellm import completion

        # Extract existing entities for the prompt
        existing_entities = []
        for _key, value in nei_dict.items():
            if "text" in value:
                entity_info = f"{value['text']}"
                if value.get("pronunciation"):
                    entity_info += f" (pronounced: {value['pronunciation']})"
                existing_entities.append(entity_info)

        # Construct the prompt
        prompt = f"""
You are an expert in Named Entity Recognition for text-to-speech applications.

Your task is to identify Named Entities of Interest (NEIs) in a document that might need special pronunciation guidance.
These entities should be tagged using the <nei> XML tag in the exact original text.

Types of entities to tag:
1. PROPER NAMES: People, places, organizations, brands, titles
2. TECHNICAL TERMS: Scientific, technical, or specialized terminology
3. FOREIGN WORDS: Non-English words or phrases
4. ABBREVIATIONS & ACRONYMS: Especially those that aren't obviously pronounced
5. UNUSUAL SPELLINGS: Words with non-standard spelling
6. NUMBERS & DATES: Complex numerical expressions that benefit from pronunciation guidance

DO NOT tag:
- Common words, even if capitalized at the start of sentences
- Regular English words with standard pronunciations
- Extremely common acronyms with obvious pronunciations (e.g., TV, DVD)

XML TAG FORMAT:
<nei>entity text</nei>

IMPORTANT RULES:
- Do NOT alter the original structure of the XML document
- Only add <nei> tags around the exact text of entities
- Preserve all existing XML tags and attributes
- Do NOT nest <nei> tags inside other tags or break existing tags
- Do NOT modify any other aspect of the text
- Only tag complete words or phrases, never partial words
- Maintain all whitespace exactly as in the original

{"PREVIOUSLY IDENTIFIED ENTITIES (already in the document elsewhere):\\n" + "\\n".join(existing_entities) if existing_entities else ""}

Here is the XML text to tag with NEIs:

{chunk_text}
"""

        # Call the LLM API
        logger.info(f"Calling LLM API with model: {model}")
        response = completion(
            model=model,
            messages=[{"role": "user", "content": prompt}],
            temperature=temperature,
            max_tokens=None,  # Let the API determine the max tokens
        )

        # Extract the response text
        response_text = extract_response_text(response)
        return response_text

    except Exception as e:
        logger.error(f"Error in entity identification: {e}")
        # Return the original text if there's an error
        return chunk_text


def extract_response_text(response: Any) -> str:
    """
    Extract the text content from an LLM API response.

    Args:
        response: Response object from LLM API

    Returns:
        Extracted text content
    """
    try:
        # Handle response object based on its structure
        if hasattr(response, "choices") and response.choices:
            if hasattr(response.choices[0], "message"):
                content = response.choices[0].message.content
            else:
                content = response.choices[0].text
        elif isinstance(response, dict):
            if "choices" in response:
                if "message" in response["choices"][0]:
                    content = response["choices"][0]["message"]["content"]
                else:
                    content = response["choices"][0]["text"]
            else:
                logger.error("Unexpected response format")
                content = str(response)
        else:
            logger.error("Unexpected response object")
            content = str(response)

        return content
    except Exception as e:
        logger.error(f"Error extracting response text: {e}")
        return ""


def extract_nei_from_tags(text: str) -> dict[str, dict[str, Any]]:
    """
    Extract NEIs from <nei> tags in text and build a dictionary.

    Args:
        text: XML text containing <nei> tags

    Returns:
        Dictionary of NEIs with their attributes
    """
    nei_dict = {}
    # Pattern to match <nei> tags with optional attributes
    pattern = r"<nei(?:\s+([^>]*))?>(.*?)</nei>"

    for match in re.finditer(pattern, text, re.DOTALL):
        attributes_str = match.group(1) or ""
        content = match.group(2)

        # Skip empty content
        if not content or not content.strip():
            continue

        # Extract attributes
        attributes = {}
        for attr_match in re.finditer(r'(\w+)="([^"]*)"', attributes_str):
            attributes[attr_match.group(1)] = attr_match.group(2)

        # Use the lowercased content as the key
        key = content.lower()

        # If this NEI doesn't exist yet, add it
        if key not in nei_dict:
            nei_dict[key] = {
                "text": content,
                "count": 1,
                "new": True,
                "orig": attributes.get("orig", ""),
            }

            # Add pronunciation if it exists
            if "pronunciation" in attributes:
                nei_dict[key]["pronunciation"] = attributes["pronunciation"]
        else:
            # Update existing NEI
            nei_dict[key]["count"] += 1
            nei_dict[key]["new"] = False

            # Keep orig if it doesn't exist yet
            if not nei_dict[key].get("orig") and "orig" in attributes:
                nei_dict[key]["orig"] = attributes["orig"]

            # Keep pronunciation if it doesn't exist yet
            if not nei_dict[key].get("pronunciation") and "pronunciation" in attributes:
                nei_dict[key]["pronunciation"] = attributes["pronunciation"]

    return nei_dict


def process_chunks(
    chunks: list[etree._Element],
    model: str,
    temperature: float,
    nei_dict: dict[str, dict[str, Any]] | None = None,
    output_file: str = "",
    nei_dict_file: str | None = None,
    backup: bool = False,
) -> tuple[etree._Element | None, dict[str, dict[str, Any]]]:
    """
    Process chunks to identify and tag NEIs.

    Args:
        chunks: List of XML chunk elements
        model: LLM model to use
        temperature: Temperature setting for the LLM
        nei_dict: Existing NEI dictionary (optional)
        output_file: File to save intermediate results (optional)
        nei_dict_file: File to save NEI dictionary (optional)
        backup: Whether to create backup copies with timestamps

    Returns:
        Tuple of (updated XML root, updated NEI dictionary)
    """
    # Initialize the NEI dictionary if not provided
    if nei_dict is None:
        nei_dict = {}

    # Get the root element
    if chunks:
        root = chunks[0].getroottree().getroot()
    else:
        logger.warning("No chunks found in the document")
        return None, nei_dict

    # Process each chunk
    for i, chunk in enumerate(chunks):
        chunk_id = chunk.get("id", str(i))
        logger.info(f"Processing chunk {chunk_id} ({i + 1}/{len(chunks)})")

        # Get the text of the chunk
        chunk_text = get_chunk_text(chunk)

        # Identify entities in the chunk
        try:
            tagged_chunk_text = identify_entities(
                chunk_text, nei_dict, model, temperature
            )

            # Extract item elements from the tagged chunk
            tagged_items = extract_item_elements(tagged_chunk_text)

            # Merge tagged items back into the original chunk
            updated_chunk = merge_tagged_items(chunk, tagged_items)

            # Replace the original chunk with the updated one
            parent = chunk.getparent()
            if parent is not None:
                index = parent.index(chunk)
                parent.remove(chunk)
                parent.insert(index, updated_chunk)

            # Extract NEIs from the tagged chunk text and update the dictionary
            new_neis = extract_nei_from_tags(tagged_chunk_text)
            for key, value in new_neis.items():
                if key in nei_dict:
                    # Update existing NEI
                    nei_dict[key]["count"] += value["count"]
                    nei_dict[key]["new"] = False
                else:
                    # Add new NEI
                    nei_dict[key] = value

            # Save the current state if an output file is provided
            if output_file:
                save_current_state(
                    root, nei_dict, output_file, nei_dict_file, chunk_id, backup
                )

        except Exception as e:
            logger.error(f"Error processing chunk {chunk_id}: {e}")

    return root, nei_dict


def process_document(
    input_file: str,
    output_file: str,
    nei_dict_file: str | None = None,
    model: str = DEFAULT_MODEL,
    temperature: float = DEFAULT_TEMPERATURE,
    encoding: str = "utf-8",
    verbose: bool = False,
    backup: bool = False,
) -> dict[str, Any]:
    """
    Process a document to identify and tag Named Entities of Interest (NEIs).

    Args:
        input_file: Path to the input XML file
        output_file: Path to save the processed XML output
        nei_dict_file: Path to save/load the NEI dictionary
        model: LLM model to use
        temperature: Temperature setting for the LLM
        encoding: Character encoding of the input file
        verbose: Enable verbose logging
        backup: Create backups at intermediate stages

    Returns:
        Dictionary with processing statistics
    """
    logger.info(f"Processing document for NEI tagging: {input_file} -> {output_file}")

    try:
        # Configure logging level
        if verbose:
            logger.level("INFO")
        else:
            logger.level("WARNING")

        # Load existing NEI dictionary if available
        nei_dict = {}
        if nei_dict_file and os.path.exists(nei_dict_file):
            try:
                with open(nei_dict_file, encoding=encoding) as f:
                    nei_dict = json.load(f)
                logger.info(f"Loaded existing NEI dictionary from {nei_dict_file}")
            except Exception as e:
                logger.error(f"Error loading NEI dictionary: {e}")

        # Create a backup of the input file if requested
        if backup:
            input_backup = create_backup(input_file, "pre_entitize")
            if input_backup:
                logger.info(f"Created backup of input file: {input_backup}")

        # Parse the input XML
        with open(input_file, encoding=encoding) as f:
            xml_content = f.read()

        root = parse_xml(xml_content)
        if root is None:
            logger.error("Failed to parse input XML")
            return {
                "input_file": input_file,
                "output_file": output_file,
                "nei_dict_file": nei_dict_file,
                "success": False,
                "error": "Failed to parse input XML",
            }

        # Extract chunks from the document
        chunks = get_chunks(root)
        logger.info(f"Found {len(chunks)} chunks in the document")

        # Process the chunks
        updated_root, updated_nei_dict = process_chunks(
            chunks,
            model,
            temperature,
            nei_dict,
            output_file if backup else "",  # Only save intermediate if backup is True
            nei_dict_file,
            backup,
        )

        if updated_root is None:
            logger.error("Failed to process chunks")
            return {
                "input_file": input_file,
                "output_file": output_file,
                "nei_dict_file": nei_dict_file,
                "success": False,
                "error": "Failed to process chunks",
            }

        # Serialize the final XML
        final_xml = serialize_xml(updated_root)

        # Save the final output
        with open(output_file, "w", encoding=encoding) as f:
            f.write(final_xml)

        # Save the final NEI dictionary
        if nei_dict_file:
            with open(nei_dict_file, "w", encoding=encoding) as f:
                json.dump(updated_nei_dict, f, ensure_ascii=False, indent=2)

        # Count NEIs
        total_neis = len(updated_nei_dict)
        new_neis = sum(1 for nei in updated_nei_dict.values() if nei.get("new", False))

        logger.info(
            f"Successfully processed document: {total_neis} NEIs ({new_neis} new)"
        )
        logger.info(f"Saved processed document to {output_file}")
        if nei_dict_file:
            logger.info(f"Saved NEI dictionary to {nei_dict_file}")

        return {
            "input_file": input_file,
            "output_file": output_file,
            "nei_dict_file": nei_dict_file,
            "total_neis": total_neis,
            "new_neis": new_neis,
            "success": True,
        }

    except Exception as e:
        logger.error(f"Error processing document: {e}")
        return {
            "input_file": input_file,
            "output_file": output_file,
            "nei_dict_file": nei_dict_file,
            "success": False,
            "error": str(e),
        }
</file>

<file path="e11ocutionist/neifix.py">
#!/usr/bin/env python3
# this_file: src/e11ocutionist/neifix.py
"""
NEI fix module for e11ocutionist.

This module provides utilities to transform the text content inside
Named Entity of Interest (NEI) tags according to specific formatting rules.
"""

import re
from typing import Any
from loguru import logger


def transform_nei_content(
    input_file: str,
    output_file: str | None = None,
) -> dict[str, Any]:
    """
    Transform the content of NEI tags in an XML file.

    This function applies specific capitalization and hyphenation rules
    to the text inside NEI tags. For each word in an NEI tag:
    - Single-letter words are preserved (likely parts of acronyms)
    - Hyphens are removed
    - The first letter of each word is preserved as is
    - All subsequent letters are converted to lowercase

    Args:
        input_file: Path to the input XML file
        output_file: Path to save the transformed XML output (if None, returns the content)

    Returns:
        Dictionary with processing statistics
    """
    logger.info(f"Processing NEI tags in {input_file}")

    # Read the input file
    with open(input_file, encoding="utf-8") as f:
        content = f.read()

    # Define a function to process each NEI tag match
    def process_nei_content(match):
        # Get the opening tag and content
        tag_start = match.group(1)
        content = match.group(2)

        # Process the content
        words = content.split()
        processed_words = []

        for word in words:
            # Preserve single-letter words (likely parts of acronyms)
            if len(word) == 1:
                processed_words.append(word)
                continue

            # Remove hyphens
            word_no_hyphens = word.replace("-", "")

            # Keep first letter as is, convert the rest to lowercase
            if len(word_no_hyphens) > 0:
                processed_word = word_no_hyphens[0] + word_no_hyphens[1:].lower()
                processed_words.append(processed_word)
            else:
                # Fallback for edge cases
                processed_words.append(word)

        # Join processed words back together
        processed_content = " ".join(processed_words)

        # Return the full NEI tag with processed content
        return f"{tag_start}{processed_content}</nei>"

    # Find and process all NEI tags
    nei_pattern = r"(<nei[^>]*>)(.*?)</nei>"
    transformed_content = re.sub(
        nei_pattern, process_nei_content, content, flags=re.DOTALL
    )

    # Count the number of NEI tags processed
    nei_count = len(re.findall(nei_pattern, content))
    logger.info(f"Processed {nei_count} NEI tags")

    # Write to output file if specified
    if output_file:
        with open(output_file, "w", encoding="utf-8") as f:
            f.write(transformed_content)
        logger.info(f"Saved transformed content to {output_file}")

    return {
        "input_file": input_file,
        "output_file": output_file if output_file else None,
        "nei_count": nei_count,
        "success": True,
    }
</file>

<file path="e11ocutionist/orator.py">
#!/usr/bin/env python3
# this_file: src/e11ocutionist/orator.py
"""
Orator module for e11ocutionist.

This module enhances text for speech synthesis by restructuring sentences,
normalizing words, enhancing punctuation, and adding emotional emphasis.
"""

import re
from typing import Any
from loguru import logger
from lxml import etree
from dotenv import load_dotenv

from .utils import (
    parse_xml,
    serialize_xml,
)

# Load environment variables
load_dotenv()

# Constants
DEFAULT_MODEL = "gpt-4o"
FALLBACK_MODEL = "gpt-4o-mini"
DEFAULT_TEMPERATURE = 0.2


def extract_chunk_items(xml_content: str) -> list[tuple[str, str, str]]:
    """
    Extract items from an XML document.

    Args:
        xml_content: XML string

    Returns:
        List of tuples containing (item_id, item_content, item_xml)
    """
    items = []

    try:
        # Parse the XML
        root = parse_xml(xml_content)
        if root is None:
            logger.error("Failed to parse XML")
            return items

        # Find all items
        for item in root.xpath("//item"):
            item_id = item.get("id", "")

            # Get the item content (preserving any XML tags inside)
            item_content = etree.tostring(item, encoding="utf-8").decode("utf-8")
            # Extract just the content between the item tags
            item_content = re.sub(
                r"^<item[^>]*>(.*)</item>$", r"\1", item_content, flags=re.DOTALL
            )

            # Get the full item XML
            item_xml = etree.tostring(item, encoding="utf-8").decode("utf-8")

            items.append((item_id, item_content, item_xml))

    except Exception as e:
        logger.error(f"Error extracting items: {e}")

    return items


def restructure_sentences(
    items: list[tuple[str, str, str]], model: str, temperature: float
) -> list[tuple[str, str]]:
    """
    Restructure sentences for better speech synthesis.

    This function divides long sentences into shorter ones and
    ensures paragraphs and headings end with punctuation.

    Args:
        items: List of tuples (item_id, item_content, item_xml)
        model: LLM model to use
        temperature: Temperature setting for the LLM

    Returns:
        List of tuples (item_id, processed_content)
    """
    if not items:
        return []

    # Process items in batches to avoid token limits
    max_items_per_batch = 10
    batches = [
        items[i : i + max_items_per_batch]
        for i in range(0, len(items), max_items_per_batch)
    ]

    processed_items = []

    for batch in batches:
        # Prepare text for the LLM
        items_text = ""
        for i, (item_id, content, _) in enumerate(batch):
            items_text += f"ITEM {i + 1} (ID: {item_id}):\n{content}\n\n"

        # Construct the prompt

        try:
            # Call the LLM API (implementation will depend on your LLM library)
            # This is a placeholder for the actual API call
            logger.info(f"Calling LLM API with model: {model}")

            # Placeholder for LLM call
            response = {
                "choices": [
                    {
                        "message": {
                            "content": "ITEM 1 (ID: 000000-123456): Processed content."
                        }
                    }
                ]
            }

            # Process the response
            restructured_batch = extract_processed_items_from_response(response, batch)
            processed_items.extend(restructured_batch)

        except Exception as e:
            logger.error(f"Error restructuring sentences: {e}")
            # Add the original content as fallback
            for item_id, content, _ in batch:
                processed_items.append((item_id, content))

    return processed_items


def normalize_words(
    items: list[tuple[str, str]], model: str, temperature: float
) -> list[tuple[str, str]]:
    """
    Normalize words for better speech synthesis.

    This function converts digits to numeric words, replaces symbols,
    and spells out rare symbols.

    Args:
        items: List of tuples (item_id, item_content)
        model: LLM model to use
        temperature: Temperature setting for the LLM

    Returns:
        List of tuples (item_id, processed_content)
    """
    if not items:
        return []

    # Process items in batches to avoid token limits
    max_items_per_batch = 10
    batches = [
        items[i : i + max_items_per_batch]
        for i in range(0, len(items), max_items_per_batch)
    ]

    processed_items = []

    for batch in batches:
        # Prepare text for the LLM
        items_text = ""
        for i, (item_id, content) in enumerate(batch):
            items_text += f"ITEM {i + 1} (ID: {item_id}):\n{content}\n\n"

        # Construct the prompt

        try:
            # Call the LLM API
            logger.info(f"Calling LLM API with model: {model}")

            # Placeholder for LLM call
            response = {
                "choices": [
                    {
                        "message": {
                            "content": "ITEM 1 (ID: 000000-123456): Processed content."
                        }
                    }
                ]
            }

            # Process the response
            normalized_batch = extract_processed_items_from_response(response, batch)
            processed_items.extend(normalized_batch)

        except Exception as e:
            logger.error(f"Error normalizing words: {e}")
            # Add the original content as fallback
            for item_id, content in batch:
                processed_items.append((item_id, content))

    return processed_items


def enhance_punctuation(items: list[tuple[str, str]]) -> list[tuple[str, str]]:
    """
    Enhance punctuation for better speech synthesis.

    This function enhances punctuation algorithmically without using an LLM.

    Args:
        items: List of tuples (item_id, item_content)

    Returns:
        List of tuples (item_id, processed_content)
    """
    enhanced_items = []

    for item_id, content in items:
        # Process parenthetical expressions
        # Add commas before and after parentheses if missing
        processed = re.sub(r"(\w)\(", r"\1, (", content)
        processed = re.sub(r"\)(\w)", r"), \1", processed)

        # Replace em/en dashes with ellipses when used as pauses
        # But preserve em dashes at the beginning of dialog
        processed = re.sub(r"(?<!^)[\u2014\u2013](?!\w)", "...", processed)

        # Add a subtle pause after colons
        processed = re.sub(r":(\s*\w)", r": <hr/>\1", processed)

        # Add pauses around quoted speech
        processed = re.sub(r'([.!?])"(\s*[A-Z])', r'\1" <hr/>\2', processed)

        # Add appropriate pacing for lists
        processed = re.sub(r"(\d+\.\s*\w+);", r"\1, ", processed)

        # Clean up excessive commas
        processed = re.sub(r",\s*,", r",", processed)

        enhanced_items.append((item_id, processed))

    return enhanced_items


def add_emotional_emphasis(
    items: list[tuple[str, str]], model: str, temperature: float
) -> list[tuple[str, str]]:
    """
    Add emotional emphasis for more expressive speech.

    This function adds <em> tags for emphasis and <hr/> for dramatic pauses.

    Args:
        items: List of tuples (item_id, item_content)
        model: LLM model to use
        temperature: Temperature setting for the LLM

    Returns:
        List of tuples (item_id, processed_content)
    """
    if not items:
        return []

    # Process items in batches to avoid token limits
    max_items_per_batch = 10
    batches = [
        items[i : i + max_items_per_batch]
        for i in range(0, len(items), max_items_per_batch)
    ]

    processed_items = []

    for batch in batches:
        # Prepare text for the LLM
        items_text = ""
        for i, (item_id, content) in enumerate(batch):
            items_text += f"ITEM {i + 1} (ID: {item_id}):\n{content}\n\n"

        # Construct the prompt

        try:
            # Call the LLM API
            logger.info(f"Calling LLM API with model: {model}")

            # Placeholder for LLM call
            response = {
                "choices": [
                    {
                        "message": {
                            "content": "ITEM 1 (ID: 000000-123456): <em>Processed</em> content."
                        }
                    }
                ]
            }

            # Process the response
            emphasized_batch = extract_processed_items_from_response(response, batch)
            processed_items.extend(emphasized_batch)

        except Exception as e:
            logger.error(f"Error adding emotional emphasis: {e}")
            # Add the original content as fallback
            for item_id, content in batch:
                processed_items.append((item_id, content))

    return processed_items


def extract_processed_items_from_response(
    response, original_items
) -> list[tuple[str, str]]:
    """
    Extract processed items from LLM response.

    Args:
        response: LLM API response
        original_items: Original items that were sent to the LLM

    Returns:
        List of tuples (item_id, processed_content)
    """
    processed_items = []

    # Extract the response content
    if hasattr(response, "choices") and response.choices:
        content = response.choices[0].message.content
    elif isinstance(response, dict) and "choices" in response:
        content = response["choices"][0]["message"]["content"]
    else:
        logger.warning("Unexpected response format from LLM")
        # Use original items as fallback
        if isinstance(original_items[0], tuple) and len(original_items[0]) >= 2:
            return [(item[0], item[1]) for item in original_items]
        return []

    # Create a map of IDs to original content
    id_map = {}
    for item in original_items:
        if isinstance(item, tuple):
            if len(item) >= 3:
                id_map[item[0]] = item[1]  # (item_id, content, item_xml)
            elif len(item) >= 2:
                id_map[item[0]] = item[1]  # (item_id, content)

    # Extract processed items
    pattern = re.compile(
        r"ITEM \d+\s*\(ID:\s*([^)]+)\):\s*(.*?)(?=ITEM \d+|$)", re.DOTALL
    )
    matches = pattern.findall(content)

    for item_id, processed_content in matches:
        item_id = item_id.strip()
        if item_id in id_map:
            processed_items.append((item_id, processed_content.strip()))

    # Check if we got all items
    if len(processed_items) < len(original_items):
        logger.warning(
            f"Some items were not processed: got {len(processed_items)}, expected {len(original_items)}"
        )
        # Add missing items with original content
        found_ids = {item_id for item_id, _ in processed_items}
        for item in original_items:
            if isinstance(item, tuple) and len(item) >= 2:
                item_id = item[0]
                if item_id not in found_ids:
                    processed_items.append((item_id, id_map.get(item_id, "")))

    return processed_items


def merge_processed_items(
    original_xml: str, processed_items: list[tuple[str, str]]
) -> str:
    """
    Merge processed items back into the original XML.

    Args:
        original_xml: Original XML document
        processed_items: List of tuples (item_id, processed_content)

    Returns:
        Updated XML document
    """
    try:
        # Create a mapping of item IDs to processed content
        processed_map = dict(processed_items)

        # Parse the XML
        root = parse_xml(original_xml)
        if root is None:
            logger.error("Failed to parse original XML")
            return original_xml

        # Find all items and update their content
        for item in root.xpath("//item"):
            item_id = item.get("id", "")

            if item_id in processed_map:
                # Get the processed content
                processed_content = processed_map[item_id]

                # Clear the item's existing content
                item.text = None
                for child in item:
                    item.remove(child)

                # Create a temporary root element with the processed content
                # This preserves any XML tags in the processed content
                temp_xml = f"<root>{processed_content}</root>"
                temp_root = parse_xml(temp_xml)

                if temp_root is not None:
                    # Copy content from temp_root to item
                    if temp_root.text:
                        item.text = temp_root.text

                    for child in temp_root:
                        item.append(child)
                else:
                    # Fallback: just set the text (losing any XML tags)
                    item.text = processed_content

        # Serialize the updated XML
        updated_xml = serialize_xml(root)
        return updated_xml

    except Exception as e:
        logger.error(f"Error merging processed items: {e}")
        return original_xml


def process_document(
    input_file: str,
    output_file: str,
    steps: list[str] | None = None,
    model: str = DEFAULT_MODEL,
    temperature: float = DEFAULT_TEMPERATURE,
    encoding: str = "utf-8",
    verbose: bool = False,
    backup: bool = False,
) -> dict[str, Any]:
    """
    Process a document through the speech enhancement pipeline.

    This function enhances text for speech synthesis by:
    1. Restructuring sentences (dividing long sentences)
    2. Normalizing words (converting digits to words, replacing symbols)
    3. Enhancing punctuation (adding pauses, fixing parentheses)
    4. Adding emotional emphasis (with <em> tags and <hr/> tags)

    Args:
        input_file: Path to the input XML file
        output_file: Path to save the processed XML output
        steps: List of steps to perform (sentences, words, punctuation, emphasis)
               If None, all steps are performed
        model: LLM model to use
        temperature: Temperature setting for the LLM
        encoding: Character encoding of the input file
        verbose: Enable verbose logging
        backup: Create backups at intermediate stages

    Returns:
        Dictionary with processing statistics
    """
    logger.info(
        f"Processing document for speech enhancement: {input_file} -> {output_file}"
    )

    # Determine which steps to perform
    all_steps = ["sentences", "words", "punctuation", "emphasis"]
    if steps is None:
        steps = all_steps
    else:
        # Validate steps
        steps = [s for s in steps if s in all_steps]

    logger.info(f"Performing steps: {', '.join(steps)}")

    try:
        # Read the input file
        with open(input_file, encoding=encoding) as f:
            xml_content = f.read()

        # Extract items from the document
        items = extract_chunk_items(xml_content)
        logger.info(f"Extracted {len(items)} items for processing")

        # Create a copy of the original XML
        current_xml = xml_content

        # Process each step sequentially
        processed_items = [(item_id, content) for item_id, content, _ in items]

        # Step 1: Restructure sentences
        if "sentences" in steps:
            logger.info("Step 1: Restructuring sentences")
            processed_items = restructure_sentences(items, model, temperature)

            # Merge processed items back into XML
            current_xml = merge_processed_items(current_xml, processed_items)

            # Save backup if requested
            if backup:
                backup_file = f"{output_file}.sentences"
                with open(backup_file, "w", encoding=encoding) as f:
                    f.write(current_xml)
                logger.info(f"Saved sentences backup to {backup_file}")

        # Step 2: Normalize words
        if "words" in steps:
            logger.info("Step 2: Normalizing words")
            processed_items = normalize_words(processed_items, model, temperature)

            # Merge processed items back into XML
            current_xml = merge_processed_items(current_xml, processed_items)

            # Save backup if requested
            if backup:
                backup_file = f"{output_file}.words"
                with open(backup_file, "w", encoding=encoding) as f:
                    f.write(current_xml)
                logger.info(f"Saved words backup to {backup_file}")

        # Step 3: Enhance punctuation
        if "punctuation" in steps:
            logger.info("Step 3: Enhancing punctuation")
            processed_items = enhance_punctuation(processed_items)

            # Merge processed items back into XML
            current_xml = merge_processed_items(current_xml, processed_items)

            # Save backup if requested
            if backup:
                backup_file = f"{output_file}.punctuation"
                with open(backup_file, "w", encoding=encoding) as f:
                    f.write(current_xml)
                logger.info(f"Saved punctuation backup to {backup_file}")

        # Step 4: Add emotional emphasis
        if "emphasis" in steps:
            logger.info("Step 4: Adding emotional emphasis")
            processed_items = add_emotional_emphasis(
                processed_items, model, temperature
            )

            # Merge processed items back into XML
            current_xml = merge_processed_items(current_xml, processed_items)

            # Save backup if requested
            if backup:
                backup_file = f"{output_file}.emphasis"
                with open(backup_file, "w", encoding=encoding) as f:
                    f.write(current_xml)
                logger.info(f"Saved emphasis backup to {backup_file}")

        # Save the final output
        with open(output_file, "w", encoding=encoding) as f:
            f.write(current_xml)
        logger.info(f"Saved enhanced document to {output_file}")

        return {
            "input_file": input_file,
            "output_file": output_file,
            "steps_performed": steps,
            "items_processed": len(processed_items),
            "success": True,
        }

    except Exception as e:
        logger.error(f"Speech enhancement failed: {e}")
        return {
            "input_file": input_file,
            "output_file": output_file,
            "steps_performed": [],
            "success": False,
            "error": str(e),
        }
</file>

<file path="e11ocutionist/tonedown.py">
#!/usr/bin/env python3
# this_file: src/e11ocutionist/tonedown.py
"""
Tonedown module for e11ocutionist.

This module adjusts the tone of the text by reviewing and refining
pronunciation cues and reducing excessive emphasis.
"""

import json
from typing import Any
from loguru import logger
from lxml import etree
from dotenv import load_dotenv

from .utils import (
    count_tokens,
    parse_xml,
    serialize_xml,
)

# Load environment variables
load_dotenv()

# Constants
DEFAULT_MODEL = "gpt-4o"
FALLBACK_MODEL = "gpt-4o-mini"
DEFAULT_TEMPERATURE = 0.2


def extract_neis_from_document(xml_content: str) -> dict[str, dict[str, str]]:
    """
    Extract Named Entities of Interest (NEIs) and their attributes from a document.

    Args:
        xml_content: XML document content

    Returns:
        Dictionary of NEIs with their attributes
    """
    nei_dict = {}

    try:
        # Parse the XML
        root = parse_xml(xml_content)
        if root is None:
            logger.error("Failed to parse XML")
            return nei_dict

        # Find all NEI tags
        nei_tags = root.xpath("//nei")
        logger.info(f"Found {len(nei_tags)} NEI tags in document")

        # Process each NEI tag
        for nei in nei_tags:
            # Get the content (the entity name)
            nei_text = (nei.text or "").strip()
            if not nei_text:
                continue

            # Only process each unique NEI once (case-insensitive)
            nei_lower = nei_text.lower()

            # Get attributes
            is_new = nei.get("new", "false") == "true"
            orig_value = nei.get("orig", "")

            # Initialize or update the entry
            if nei_lower not in nei_dict:
                nei_dict[nei_lower] = {
                    "text": nei_text,  # Preserve original case
                    "new": is_new,
                    "orig": orig_value,
                    "count": 1,
                }
            else:
                # Increment count
                nei_dict[nei_lower]["count"] += 1

                # If this instance has orig when previous didn't, update it
                if orig_value and not nei_dict[nei_lower]["orig"]:
                    nei_dict[nei_lower]["orig"] = orig_value

                # If any instance is marked as new, the entity is considered new
                if is_new:
                    nei_dict[nei_lower]["new"] = True

        logger.info(f"Extracted {len(nei_dict)} unique NEIs")

    except Exception as e:
        logger.error(f"Error extracting NEIs: {e}")

    return nei_dict


def detect_language(
    xml_content: str, model: str, temperature: float
) -> tuple[str, float]:
    """
    Detect the dominant language of the document using an LLM.

    Args:
        xml_content: XML document content
        model: LLM model to use
        temperature: Temperature setting for the LLM

    Returns:
        Tuple of (language_code, confidence)
    """
    logger.info("Detecting document language")

    try:
        # Parse the XML to extract text content
        root = parse_xml(xml_content)
        if root is None:
            logger.error("Failed to parse XML")
            return ("en", 0.5)  # Default to English with low confidence

        # Extract text from items (up to a certain amount)
        text_samples = []
        sample_count = 0
        max_samples = 10

        for item in root.xpath("//item"):
            # Skip empty items
            if not (item.text and item.text.strip()):
                continue

            # Extract text content (without XML tags)
            item_text = etree.tostring(item, encoding="utf-8", method="text").decode(
                "utf-8"
            )

            # Add to samples if substantial
            if len(item_text.strip()) > 20:
                text_samples.append(item_text.strip())
                sample_count += 1

            # Limit the number of samples
            if sample_count >= max_samples:
                break

        # If we don't have enough samples, try to get more by finding text nodes
        if sample_count < 3:
            for text_node in root.xpath("//text()"):
                text = text_node.strip()
                if len(text) > 20:
                    text_samples.append(text)
                    sample_count += 1

                if sample_count >= max_samples:
                    break

        # If still not enough, default to English
        if sample_count < 2:
            logger.warning("Not enough text samples for language detection")
            return ("en", 0.5)

        # Combine samples
        "\n\n".join(text_samples[:5])  # Use up to 5 samples

        # Create the prompt

        # Call the LLM API (placeholder)
        logger.info(f"Calling LLM API with model: {model}")

        # Placeholder for LLM call
        response = {
            "choices": [
                {"message": {"content": '{"language": "en", "confidence": 0.95}'}}
            ]
        }

        # Extract the response
        if hasattr(response, "choices") and response.choices:
            content = response.choices[0].message.content
        elif isinstance(response, dict) and "choices" in response:
            content = response["choices"][0]["message"]["content"]
        else:
            logger.warning("Unexpected response format from LLM")
            return ("en", 0.5)

        # Parse the JSON response
        try:
            result = json.loads(content)
            language = result.get("language", "en")
            confidence = result.get("confidence", 0.5)

            logger.info(f"Detected language: {language} (confidence: {confidence:.2f})")
            return (language, confidence)

        except json.JSONDecodeError:
            logger.error("Failed to parse LLM response as JSON")
            return ("en", 0.5)

    except Exception as e:
        logger.error(f"Language detection failed: {e}")
        return ("en", 0.5)


def review_pronunciations(
    nei_dict: dict[str, dict[str, str]], language: str, model: str, temperature: float
) -> dict[str, dict[str, str]]:
    """
    Review and refine pronunciation cues for NEIs using an LLM.

    Args:
        nei_dict: Dictionary of NEIs with their attributes
        language: Detected language code
        model: LLM model to use
        temperature: Temperature setting for the LLM

    Returns:
        Updated NEI dictionary with refined pronunciations
    """
    if not nei_dict:
        logger.warning("No NEIs to review")
        return nei_dict

    logger.info(f"Reviewing pronunciations for {len(nei_dict)} NEIs in {language}")

    # Create a copy of the dictionary to update
    updated_dict = {k: v.copy() for k, v in nei_dict.items()}

    # Process NEIs in batches to avoid token limits
    batch_size = 25
    nei_items = list(nei_dict.items())
    batches = [
        nei_items[i : i + batch_size] for i in range(0, len(nei_items), batch_size)
    ]

    for batch_index, batch in enumerate(batches):
        logger.info(
            f"Processing batch {batch_index + 1}/{len(batches)} ({len(batch)} NEIs)"
        )

        # Prepare the batch for the LLM
        nei_json = {}
        for _key, value in batch:
            nei_json[value["text"]] = {
                "orig": value.get("orig", ""),
                "count": value.get("count", 1),
                "new": value.get("new", False),
            }

        # Create the prompt
        f"""
You are reviewing pronunciations for Named Entities of Interest (NEIs) to improve text-to-speech.
The primary language of the document is: {language}

For each entity, provide a pronunciation guide that will help a text-to-speech system
pronounce it correctly. Focus on entities that might be mispronounced
(names, foreign words, technical terms, acronyms).

Guidelines:
1. For common words with standard pronunciation, leave the pronunciation empty
2. For names and unusual words, provide phonetic spelling (e.g., "Kayleigh" → "Kay-lee")
3. For acronyms, specify whether to spell out or pronounce as a word (e.g., "NASA" → "Nasa as one word")
4. For foreign words, provide an English approximation or pronunciation note
5. For initialisms, add periods or spaces between letters (e.g., "IBM" → "I.B.M.")

The NEIs are in this format:
{{
  "Entity Text": {{
    "orig": "original form if different",
    "count": number of occurrences,
    "new": true/false if this is a new entity
  }},
  ...
}}

Return a JSON object where each key is the entity and each value is the pronunciation guide.
Return ONLY the JSON, no explanation.

NEIs to review:
{json.dumps(nei_json, ensure_ascii=False, indent=2)}
"""

        try:
            # Call the LLM API (placeholder)
            logger.info(f"Calling LLM API with model: {model}")

            # Placeholder for LLM call
            response = {
                "choices": [
                    {
                        "message": {
                            "content": '{"NASA": "Nasa as one word", "Kayleigh": "Kay-lee"}'
                        }
                    }
                ]
            }

            # Extract the response
            if hasattr(response, "choices") and response.choices:
                content = response.choices[0].message.content
            elif isinstance(response, dict) and "choices" in response:
                content = response["choices"][0]["message"]["content"]
            else:
                logger.warning("Unexpected response format from LLM")
                continue

            # Parse the JSON response
            try:
                result = json.loads(content)

                # Update pronunciations in the dictionary
                for entity, pronunciation in result.items():
                    entity_lower = entity.lower()
                    if entity_lower in updated_dict:
                        updated_dict[entity_lower]["pronunciation"] = pronunciation
                    elif entity in updated_dict:
                        updated_dict[entity]["pronunciation"] = pronunciation

                logger.info(f"Updated pronunciations for batch {batch_index + 1}")

            except json.JSONDecodeError:
                logger.error("Failed to parse LLM response as JSON")

        except Exception as e:
            logger.error(f"Error reviewing pronunciations: {e}")

    # Count how many pronunciations were added
    pronunciation_count = sum(
        1 for nei in updated_dict.values() if nei.get("pronunciation")
    )
    logger.info(f"Added or updated {pronunciation_count} pronunciations")

    return updated_dict


def update_nei_tags(xml_content: str, nei_dict: dict[str, dict[str, str]]) -> str:
    """
    Update NEI tags in the document with refined pronunciations.

    Args:
        xml_content: XML document content
        nei_dict: Dictionary of NEIs with their attributes and pronunciations

    Returns:
        Updated XML document
    """
    if not nei_dict:
        return xml_content

    logger.info("Updating NEI tags with refined pronunciations")

    try:
        # Parse the XML
        root = parse_xml(xml_content)
        if root is None:
            logger.error("Failed to parse XML")
            return xml_content

        # Create a fast lookup for NEIs (case-insensitive)
        nei_lookup = {}
        for key, value in nei_dict.items():
            nei_lookup[key.lower()] = value
            if "text" in value:
                nei_lookup[value["text"].lower()] = value

        # Find all NEI tags
        update_count = 0
        for nei in root.xpath("//nei"):
            # Get the content (the entity name)
            nei_text = (nei.text or "").strip()
            if not nei_text:
                continue

            # Look up this NEI
            nei_key = nei_text.lower()
            if nei_key in nei_lookup and "pronunciation" in nei_lookup[nei_key]:
                pronunciation = nei_lookup[nei_key]["pronunciation"]

                # Only update if there's a pronunciation
                if pronunciation:
                    # Set the content to the pronunciation
                    nei.text = pronunciation

                    # Set the orig attribute to the original text if not already set
                    if not nei.get("orig"):
                        nei.set("orig", nei_text)

                    update_count += 1

        logger.info(f"Updated {update_count} NEI tags with pronunciations")

        # Serialize the updated XML
        updated_xml = serialize_xml(root)
        return updated_xml

    except Exception as e:
        logger.error(f"Error updating NEI tags: {e}")
        return xml_content


def reduce_emphasis(xml_content: str, min_distance: int = 50) -> str:
    """
    Reduce excessive emphasis tags in the document.

    Args:
        xml_content: XML document content
        min_distance: Minimum token distance between emphasis tags

    Returns:
        Updated XML document with reduced emphasis
    """
    logger.info(f"Reducing excessive emphasis (min distance: {min_distance} tokens)")

    try:
        # Parse the XML
        root = parse_xml(xml_content)
        if root is None:
            logger.error("Failed to parse XML")
            return xml_content

        # Find all emphasis tags
        em_tags = root.xpath("//em")
        logger.info(f"Found {len(em_tags)} emphasis tags")

        if len(em_tags) <= 1:
            logger.info("No excess emphasis to reduce")
            return xml_content

        # Get the positions of emphasis tags
        em_positions = []
        for em in em_tags:
            # Get the position in the document (approximate token position)
            # This is approximate as we're not counting tokens precisely
            ancestor_text = ""
            for ancestor in em.xpath("ancestor::*"):
                if ancestor.text:
                    ancestor_text += ancestor.text

            position = count_tokens(ancestor_text)
            em_positions.append((em, position))

        # Sort by position
        em_positions.sort(key=lambda x: x[1])

        # Identify tags to remove (too close to others)
        to_remove = []
        last_position = -min_distance  # Ensure we keep the first one

        for em, position in em_positions:
            if position - last_position < min_distance:
                # Too close to previous, mark for removal
                to_remove.append(em)
            else:
                # Keep this one
                last_position = position

        # Remove marked tags (replace with their content)
        for em in to_remove:
            parent = em.getparent()
            if parent is None:
                continue

            # Get the index of this element in its parent
            index = parent.index(em)

            # Replace the emphasis tag with its content
            if em.text:
                if index == 0:
                    # First child, add to parent's text
                    parent.text = (parent.text or "") + em.text
                else:
                    # Not first child, add to previous sibling's tail
                    prev = parent[index - 1]
                    prev.tail = (prev.tail or "") + em.text

            # Handle any tail text
            if em.tail:
                if index == len(parent) - 1:
                    # Last child, add to parent's tail
                    parent.tail = (parent.tail or "") + em.tail
                else:
                    # Not last child, add to this element's replacement or next sibling
                    next_elem = parent[index + 1]
                    next_elem.tail = (next_elem.tail or "") + em.tail

            # Remove the emphasis tag
            parent.remove(em)

        logger.info(f"Removed {len(to_remove)} excessive emphasis tags")

        # Serialize the updated XML
        updated_xml = serialize_xml(root)
        return updated_xml

    except Exception as e:
        logger.error(f"Error reducing emphasis: {e}")
        return xml_content


def process_document(
    input_file: str,
    output_file: str,
    nei_dict_file: str | None = None,
    min_emphasis_distance: int = 50,
    model: str = DEFAULT_MODEL,
    temperature: float = DEFAULT_TEMPERATURE,
    encoding: str = "utf-8",
    verbose: bool = False,
    backup: bool = False,
) -> dict[str, Any]:
    """
    Process a document to adjust tone for speech synthesis.

    This function:
    1. Extracts Named Entities of Interest (NEIs) from the document
    2. Detects the document language
    3. Reviews and refines pronunciation cues for NEIs
    4. Updates NEI tags with refined pronunciations
    5. Reduces excessive emphasis tags

    Args:
        input_file: Path to the input XML file
        output_file: Path to save the processed XML output
        nei_dict_file: Path to save/load the NEI dictionary
        min_emphasis_distance: Minimum distance between emphasis tags
        model: LLM model to use
        temperature: Temperature setting for the LLM
        encoding: Character encoding of the input file
        verbose: Enable verbose logging
        backup: Create backups at intermediate stages

    Returns:
        Dictionary with processing statistics
    """
    logger.info(
        f"Processing document for tone adjustment: {input_file} -> {output_file}"
    )

    try:
        # Read the input file
        with open(input_file, encoding=encoding) as f:
            xml_content = f.read()

        # Step 1: Extract NEIs from the document
        logger.info("Step 1: Extracting NEIs from document")
        nei_dict = extract_neis_from_document(xml_content)

        # Save NEI dictionary if requested
        if nei_dict_file and backup:
            backup_file = f"{nei_dict_file}.extracted"
            with open(backup_file, "w", encoding=encoding) as f:
                json.dump(nei_dict, f, ensure_ascii=False, indent=2)
            logger.info(f"Saved extracted NEI dictionary to {backup_file}")

        # Step 2: Detect document language
        logger.info("Step 2: Detecting document language")
        language, confidence = detect_language(xml_content, model, temperature)

        # Step 3: Review pronunciations if we have NEIs
        if nei_dict:
            logger.info("Step 3: Reviewing pronunciations")
            nei_dict = review_pronunciations(nei_dict, language, model, temperature)

            # Save updated NEI dictionary if requested
            if nei_dict_file:
                with open(nei_dict_file, "w", encoding=encoding) as f:
                    json.dump(nei_dict, f, ensure_ascii=False, indent=2)
                logger.info(f"Saved NEI dictionary to {nei_dict_file}")

            # Save backup if requested
            if backup:
                backup_file = f"{output_file}.nei"
                with open(backup_file, "w", encoding=encoding) as f:
                    updated_xml = update_nei_tags(xml_content, nei_dict)
                    f.write(updated_xml)
                logger.info(f"Saved NEI-updated XML to {backup_file}")

        # Step 4: Update NEI tags in the document
        logger.info("Step 4: Updating NEI tags")
        current_xml = update_nei_tags(xml_content, nei_dict)

        # Step 5: Reduce excessive emphasis
        logger.info("Step 5: Reducing excessive emphasis")
        current_xml = reduce_emphasis(current_xml, min_emphasis_distance)

        # Save the final output
        with open(output_file, "w", encoding=encoding) as f:
            f.write(current_xml)
        logger.info(f"Saved tone-adjusted document to {output_file}")

        return {
            "input_file": input_file,
            "output_file": output_file,
            "nei_dict_file": nei_dict_file,
            "language": language,
            "language_confidence": confidence,
            "neis_processed": len(nei_dict),
            "success": True,
        }

    except Exception as e:
        logger.error(f"Tone adjustment failed: {e}")
        return {
            "input_file": input_file,
            "output_file": output_file,
            "nei_dict_file": nei_dict_file,
            "success": False,
            "error": str(e),
        }
</file>

<file path="e11ocutionist/utils.py">
#!/usr/bin/env python3
# this_file: src/e11ocutionist/utils.py
"""
Utility functions for the e11ocutionist package.

This module provides common functionality shared across multiple components.
"""

import re
import hashlib
import datetime
from pathlib import Path
from lxml import etree
import tiktoken
from functools import cache
from loguru import logger


@cache
def get_token_encoder():
    """Get the token encoder for a specific model."""
    return tiktoken.encoding_for_model("gpt-4o")


def count_tokens(text: str) -> int:
    """
    Count the number of tokens in a text string accurately using tiktoken.

    Args:
        text: Text string to count tokens for

    Returns:
        Integer token count
    """
    encoder = get_token_encoder()
    return len(encoder.encode(text))


def escape_xml_chars(text: str) -> str:
    """
    Escape special XML characters to prevent parsing errors.

    Only escapes the five special XML characters while preserving UTF-8 characters.

    Args:
        text: Input text that may contain XML special characters

    Returns:
        Text with XML special characters escaped
    """
    # Replace ampersands first (otherwise you'd double-escape the other entities)
    text = text.replace("&", "&amp;")
    # Replace other special characters
    text = text.replace("<", "&lt;")
    text = text.replace(">", "&gt;")
    text = text.replace('"', "&quot;")
    text = text.replace("'", "&apos;")
    return text


def unescape_xml_chars(text: str) -> str:
    """
    Unescape XML character entities to their original form.

    Args:
        text: Text with XML character entities

    Returns:
        Text with XML character entities unescaped
    """
    # Order is important - must unescape ampersands last to avoid double-unescaping
    text = text.replace("&lt;", "<")
    text = text.replace("&gt;", ">")
    text = text.replace("&quot;", '"')
    text = text.replace("&apos;", "'")
    text = text.replace("&amp;", "&")
    return text


def generate_hash(text: str) -> str:
    """
    Generate a 6-character base36 hash from text.

    Args:
        text: Text to hash

    Returns:
        6-character base36 hash
    """
    sha1 = hashlib.sha1(text.encode("utf-8")).hexdigest()
    # Convert to base36 (alphanumeric lowercase)
    base36 = int(sha1, 16)
    chars = "0123456789abcdefghijklmnopqrstuvwxyz"
    result = ""
    while base36 > 0:
        base36, remainder = divmod(base36, 36)
        result = chars[remainder] + result

    # Ensure we have at least 6 characters
    result = result.zfill(6)
    return result[:6]


def create_backup(file_path: str | Path, stage: str = "") -> Path | None:
    """
    Create a timestamped backup of a file.

    Args:
        file_path: Path to the file to back up
        stage: Optional stage name to include in the backup filename

    Returns:
        Path to the backup file, or None if backup failed
    """
    file_path = Path(file_path)
    if not file_path.exists():
        logger.warning(f"Cannot back up non-existent file: {file_path}")
        return None

    timestamp = datetime.datetime.now(datetime.UTC).strftime("%Y%m%d_%H%M%S")
    if stage:
        backup_path = file_path.with_name(
            f"{file_path.stem}_{stage}_{timestamp}{file_path.suffix}"
        )
    else:
        backup_path = file_path.with_name(
            f"{file_path.stem}_{timestamp}{file_path.suffix}"
        )

    try:
        import shutil

        shutil.copy2(file_path, backup_path)
        logger.debug(f"Created backup: {backup_path}")
        return backup_path
    except Exception as e:
        logger.error(f"Failed to create backup: {e}")
        return None


def parse_xml(xml_content: str | bytes, recover: bool = True) -> etree._Element | None:
    """
    Parse XML content safely with error recovery.

    Args:
        xml_content: XML content as string or bytes
        recover: Whether to try to recover from parsing errors

    Returns:
        XML root element, or None if parsing failed
    """
    try:
        # Convert to bytes if string
        if isinstance(xml_content, str):
            xml_bytes = xml_content.encode("utf-8")
        else:
            xml_bytes = xml_content

        parser = etree.XMLParser(
            remove_blank_text=False, recover=recover, encoding="utf-8"
        )
        root = etree.XML(xml_bytes, parser)
        return root
    except Exception as e:
        logger.error(f"XML parsing failed: {e}")
        return None


def serialize_xml(root: etree._Element, pretty_print: bool = True) -> str:
    """
    Serialize an XML element to string.

    Args:
        root: XML root element
        pretty_print: Whether to format the output with indentation

    Returns:
        XML string
    """
    try:
        result = etree.tostring(
            root,
            encoding="utf-8",
            method="xml",
            xml_declaration=True,
            pretty_print=pretty_print,
        ).decode("utf-8")
        return result
    except Exception as e:
        logger.error(f"XML serialization failed: {e}")
        # Return empty string in case of error
        return ""


def pretty_print_xml(xml_str: str) -> str:
    """Format XML string with proper indentation for readability."""
    try:
        root = parse_xml(xml_str)
        if root is not None:
            return serialize_xml(root, pretty_print=True)
        return xml_str
    except Exception:
        # If parsing fails, return the original string
        return xml_str


def clean_consecutive_newlines(content: str) -> str:
    """Replace more than two consecutive newlines with exactly two."""
    return re.sub(r"\n{3,}", "\n\n", content)
</file>

</files>
