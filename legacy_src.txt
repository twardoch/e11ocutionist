This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
LEGACY.md
malmo_11labs.py
malmo_all.py
malmo_chunker.py
malmo_entitizer.py
malmo_neifix.py
malmo_orator.py
malmo_tonedown.py
sapko.sh
say_11labs.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="LEGACY.md">
# TODO.md

## 1. Description of Tools in `legacy_src`

The `legacy_src` directory contains several Python scripts that form a pipeline for processing text documents for text-to-speech (TTS) applications.

### 1.1. Core Tools

#### 1.1.1. Pipeline Orchestration
- **malmo_all.py**: Main pipeline orchestrator that sequentially runs the entire workflow with robust error handling, progress tracking, and resumption capability.

#### 1.1.2. Processing Steps
1. **malmo_chunker.py**: Splits input document into semantic chunks based on content boundaries.
2. **malmo_entitizer.py**: Identifies and tags Named Entities of Interest (NEIs) for consistent pronunciation.
3. **malmo_orator.py**: Enhances text for spoken narrative through sentence restructuring, word normalization, punctuation enhancement, and emotional emphasis.
4. **malmo_tonedown.py**: Reviews and refines NEI pronunciation cues and reduces excessive emphasis.
5. **malmo_11labs.py**: Converts processed XML to ElevenLabs-compatible text format.

#### 1.1.3. Other Tools
- **say_11labs.py**: Synthesizes text using personal ElevenLabs voices and saves as MP3 files.
- **malmo_neifix.py**: Utility script for transforming NEI tag content with specific rules.
- **sapko.sh**: A shell script demonstrating the pipeline with a specific input file.

### 1.2. Main Workflow: `malmo_all.py` and `sapko.sh`

The primary workflow involves a sequence of Python scripts orchestrated by `malmo_all.py`, followed by steps executed by `sapko.sh`.

#### 1.2.1. `malmo_all.py`
* **Purpose**: This script acts as the main orchestrator for a multi-step document processing pipeline. [cite: 78, 118, 159] It sequentially runs several other Python scripts (`malmo_chunker.py`, `malmo_entitizer.py`, `malmo_orator.py`, `malmo_tonedown.py`, and `malmo_11labs.py`) to process an input file. [cite: 130, 135, 137, 138, 139]
* **Functionality**:
    * **Dependency Management**: Ensures necessary Python package dependencies for each sub-script are installed, attempting installation via `uv` or `pip` if missing. [cite: 80, 82, 83]
    * **Output Directory Management**: Creates a timestamped output directory for each input file to store intermediate and final results, as well as a progress file. [cite: 86, 101]
    * **Progress Tracking & Resumption**: Saves and loads processing progress in a JSON file (`progress.json`), allowing the workflow to be resumed from a specific step. [cite: 93, 94, 120, 149] It supports starting from a specified step and force restarting. [cite: 118, 120]
    * **Step Execution**: Runs each script in the pipeline as a subprocess, with retries on failure. [cite: 87, 106, 108]
    * **Configuration**: Accepts various command-line arguments to configure models, temperatures, and other parameters for each step. [cite: 78, 117, 158]
    * **Backup**: Optionally creates backup copies of intermediate files at each step. [cite: 118, 135]
    * **Summary Generation**: Creates a summary file (`_summary.txt`) at the end of processing, detailing the configuration and outcome of each step. [cite: 150]
* **Pipeline Steps & Associated Scripts**:
    1.  **Chunking (`malmo_chunker.py`)**: Splits the input document into semantic chunks. [cite: 130]
    2.  **Entitizing (`malmo_entitizer.py`)**: Identifies and tags named entities of interest (NEIs). [cite: 135]
    3.  **Orating (`malmo_orator.py`)**: Enhances the text for spoken narrative (sentence restructuring, word normalization, punctuation, emotional emphasis). [cite: 137]
    4.  **Toning Down (`malmo_tonedown.py`)**: Reviews and revises pronunciation cues for NEIs and reduces excessive emphasis. [cite: 138]
    5.  **11Labs Preparation (`malmo_11labs.py`)**: Converts the processed XML to a text format suitable for ElevenLabs. [cite: 139, 66]
* **Tech Stack**: Python, `fire` (for CLI), `loguru` (logging), `python-dotenv` (environment variables), `tenacity` (retries), `tqdm`/`rich` (progress bars), `litellm` (LLM interaction), `elevenlabs` (text-to-speech). [cite: 78]

#### 1.2.2. `malmo_chunker.py`
* **Purpose**: Splits a markdown document into semantically chunked XML. [cite: 165, 298] It aims to divide the text into meaningful units like chapters, scenes, or thematic sections before further processing.
* **Functionality**:
    * **Itemization**: Splits the input text into paragraphs and classifies them (heading, blockquote, list, code block, etc.) to create `<item>` elements with unique IDs. [cite: 174, 179, 182]
    * **Token Counting**: Uses `tiktoken` to count tokens for items, units, and chunks, storing this in `tok` attributes. [cite: 169, 193]
    * **Semantic Analysis (LLM-based)**: Optionally uses an LLM (e.g., GPT-4, Gemini) to identify semantic boundaries (chapters, scenes, units) based on the itemized text. [cite: 199, 200, 205] It requires a minimum number of semantic units based on document size. [cite: 202, 208]
    * **Unit Tagging**: Adds `<unit>` tags around sequences of items based on the semantic analysis or heuristics. [cite: 234, 236]
    * **Chunk Creation**: Groups `<unit>` elements into `<chunk>` elements, ensuring each chunk does not exceed a specified maximum token size. [cite: 253, 262] Chapters usually start new chunks. [cite: 261] Handles units larger than the maximum chunk size by splitting them. [cite: 272, 283]
    * **XML Output**: Produces an XML document with a hierarchical structure: `<doc>` -> `<chunk>` -> `<unit>` -> `<item>`.
    * **Backup**: Optionally creates timestamped backups at various processing stages (itemized, semantic_boundaries, units_added, chunks_created, final). [cite: 298, 308]
* **Tech Stack**: Python, `fire`, `litellm`, `tiktoken`, `lxml`, `backoff`, `python-dotenv`, `loguru`. [cite: 165]

#### 1.2.3. `malmo_entitizer.py`
* **Purpose**: Identifies and tags Named Entities of Interest (NEIs) in an XML document, preparing them for consistent pronunciation in text-to-speech. [cite: 314, 336, 376]
* **Functionality**:
    * **XML Parsing**: Parses the input XML (typically the output of `malmo_chunker.py`). [cite: 317]
    * **Chunk Processing**: Processes the document chunk by chunk. [cite: 319, 368]
    * **NEI Identification (LLM-based)**: For each chunk, it sends the text to an LLM with a prompt to identify NEIs. [cite: 336] The prompt instructs the LLM to:
        * Determine the predominant language. [cite: 341]
        * Identify NEIs relevant to the text's domain (people, locations, organizations, abbreviations, etc.). [cite: 342]
        * Tag each occurrence of an NEI with `<nei>`. [cite: 343]
        * For NEIs not consisting of common words in the predominant language, use `<nei orig="ORIGINAL">PRONUNCIATION</nei>`, where `PRONUNCIATION` is an approximation in the predominant language. [cite: 345, 346]
        * Mark newly found NEIs (not in the provided dictionary) with `new="true"`. [cite: 350]
    * **NEI Dictionary**: Maintains a dictionary of identified NEIs and their pronunciations, updating it as new NEIs are found in subsequent chunks. [cite: 336, 365, 370]
    * **XML Merging**: Merges the LLM's output (tagged items) back into the original XML structure of the chunk. [cite: 331, 371]
    * **Incremental Saves & Backups**: Optionally saves the state of the XML document and the NEI dictionary after processing each chunk and creates timestamped backups. [cite: 321, 375]
    * **Benchmark Mode**: Can process the input file with multiple LLM models in parallel for comparison, saving individual outputs and a summary. [cite: 401, 418, 419]
* **Tech Stack**: Python, `fire`, `litellm`, `lxml`, `backoff`, `python-dotenv`, `loguru`. [cite: 314]

#### 1.2.4. `malmo_orator.py`
* **Purpose**: Enhances XML text for spoken narrative by applying several LLM-based and algorithmic transformations. [cite: 437, 511]
* **Functionality**:
    * **Modular Processing**: Applies transformations chunk by chunk. [cite: 515]
    * **Sentence Restructuring (LLM-based)**: Divides long sentences into shorter ones and ensures paragraphs/headings end with punctuation, without changing words or order. [cite: 471, 473, 474, 475]
    * **Word Normalization (LLM-based)**: Converts digits to numeric words, replaces symbols like '/' and '&' with appropriate conjunctions, and spells out other rare symbols. [cite: 482, 484, 485, 486]
    * **Punctuation Enhancement (Algorithmic)**: Surrounds parenthetical expressions with spaces and ellipses, and replaces em/en dashes (used as pauses) with ellipses. [cite: 494, 496]
    * **Emotional Emphasis (LLM-based)**: Adds `<em>` tags to emphasize words/phrases, inserts `<hr/>` for dramatic pauses, and adds exclamation marks for energy, while trying to avoid excessive changes. [cite: 499, 501, 502]
    * **Selective Application**: Allows specific steps (`--sentences`, `--words`, `--punctuation`, `--emotions`) or all steps (`--all_steps`) to be run. [cite: 510, 513]
    * **XML Merging**: Similar to `malmo_entitizer.py`, it extracts item elements from LLM responses and merges them back. [cite: 444, 446]
    * **Incremental Saves & Backups**: Optionally saves the document state after each processing step for each chunk and creates timestamped backups. [cite: 451, 511]
* **Tech Stack**: Python, `fire`, `litellm`, `lxml`, `backoff`, `python-dotenv`, `loguru`. [cite: 437]

#### 1.2.5. `malmo_tonedown.py`
* **Purpose**: Reviews and revises pronunciation cues for Named Entities of Interest (NEIs) in an XML document and reduces excessive text emphasis. [cite: 527, 581]
* **Functionality**:
    * **NEI Dictionary Building**: Parses the input XML and builds a dictionary of NEIs from `<nei>` tags, noting their original spelling (`orig` attribute) and existing pronunciation cues (tag content). [cite: 534, 535] Sets `new="true"` if an `orig` value is encountered for the first time. [cite: 538]
    * **Language Detection (LLM-based)**: Extracts a middle fragment of the text and uses an LLM to detect the predominant language. [cite: 539, 543, 544]
    * **Pronunciation Cue Review (LLM-based)**: Sends the NEI dictionary and detected language to an LLM. The LLM is prompted to review each pronunciation cue and decide whether to:
        * Keep the cue (if the NEI is unusual/foreign or has non-intuitive pronunciation). [cite: 551]
        * Replace the cue with the original entity spelling (if it's native, familiar, a long English phrase, or the cue is misleading). [cite: 552]
        * Create a new, less complex adapted cue. [cite: 553]
        The LLM returns a revised JSON dictionary of NEIs. [cite: 554]
    * **XML Update**: Updates the content of `<nei>` tags in the XML document based on the revised NEI dictionary. [cite: 566, 568]
    * **Emphasis Reduction (Algorithmic)**: If the `--em <min_distance>` argument is provided, it removes `<em>` tags that are too close together (closer than `min_distance` words). [cite: 573, 577, 581]
    * **XML Serialization & Validation**: Carefully serializes the modified XML, trying to preserve formatting and validate the output. Includes repair attempts for corrupted XML. [cite: 591, 598, 603]
* **Tech Stack**: Python, `fire`, `lxml`, `loguru`, `backoff`, `python-dotenv`, `litellm`, `rich`. [cite: 527]

#### 1.2.6. `malmo_11labs.py`
* **Purpose**: Converts an XML file (typically the output of the Malmo pipeline) into a text format compatible with ElevenLabs text-to-speech, with options for processing dialog. [cite: 31, 66]
* **Functionality**:
    * **XML Parsing**: Parses the input XML file. [cite: 32]
    * **Chunk and Item Processing**: Iterates through `<chunk>` and `<item>` elements. [cite: 37, 38, 59, 63]
    * **Text Extraction and Transformation**:
        * Extracts inner XML of `<item>` elements. [cite: 55]
        * Converts `<em>...</em>` to smart double quotes (`‚Äú...‚Äù`). [cite: 56]
        * Converts `<nei new="true">...</nei>` to smart double quotes. [cite: 56]
        * Converts remaining `<nei>...</nei>` to their plain content. [cite: 57]
        * Replaces `<hr/>` with `<break time="0.5s" />` (escaped). [cite: 57]
        * Strips all other remaining HTML/XML tags. [cite: 57]
        * Unescapes HTML entities (e.g., `&lt;` to `<`). [cite: 41, 58]
        * Normalizes consecutive newlines (3+ to 2). [cite: 41]
        * Normalizes consecutive opening/closing quotation marks to single smart quotes. [cite: 39, 40, 41]
    * **Dialog Processing (Optional, via `--dialog` flag)**:
        * Identifies dialog lines starting with "‚Äî " (em dash + space). [cite: 45, 48]
        * Wraps these lines in `<q>...</q>` tags. [cite: 48]
        * Handles dialog toggles (" ‚Äî " within a line) by alternating `</q><q>`. [cite: 46, 49, 50, 52]
    * **Final Postprocessing**:
        * Replaces `<q>` at the start of a line with an opening smart quote. [cite: 42]
        * Replaces `</q>` at the end of a line with a closing smart quote. [cite: 42]
        * Replaces other `<q>` instances with "; OPEN_Q" and `</q>` with "CLOSE_Q;". [cite: 43, 44]
        * Replaces `<break time=.../>` with "...". [cite: 44]
        * Replaces single newlines with double newlines. [cite: 44]
    * **Plaintext Mode (Optional, via `--plaintext` flag)**: Processes the input as plain text instead of XML, applying dialog processing and final postprocessing if enabled. [cite: 66, 68]
* **Tech Stack**: Python, `fire`, `lxml`, `loguru`, `rich`, `litellm`. [cite: 31]

#### 1.2.7. `sapko.sh`
* **Purpose**: This is a shell script designed to run a specific sequence of the Malmo tools (`malmo_orator.py`, `malmo_tonedown.py`, `malmo_11labs.py`) on a predefined set of input/output filenames related to "Andrzej Sapkowski - WiedzÃÅmin Geralt z Rivii 0.1 - RozdrozÃáe krukoÃÅw". [cite: 614]
* **Functionality**:
    1.  Runs `malmo_orator.py` on `..._step2_entited.xml` to produce `..._step3_orated.xml`, with all steps, verbose logging, and backups enabled. [cite: 614]
    2.  Runs `malmo_tonedown.py` on `..._step3_orated.xml` to produce `..._step4_toneddown.xml`, with verbose logging. [cite: 614]
    3.  Runs `malmo_11labs.py` on `..._step4_toneddown.xml` to produce `..._step5_11.txt`, with verbose logging. [cite: 614]
* **Note**: The script uses relative paths (`../`) suggesting it's intended to be run from within the `legacy_src` directory itself, referencing scripts in a parent directory (which seems incorrect given the provided file structure where scripts are within `legacy_src`). This might be a path issue in the script. The filenames are hardcoded.
* **Tech Stack**: Bash.

### 1.3. Separate Tool: `say_11labs.py`

#### 1.3.1. `say_11labs.py`
* **Purpose**: A command-line tool to synthesize text using all personal (cloned, generated, professional) ElevenLabs voices and save the output as MP3 files. [cite: 615, 616]
* **Functionality**:
    * **ElevenLabs Client Initialization**: Initializes the ElevenLabs client using an API key (from argument or `ELEVENLABS_API_KEY` environment variable). [cite: 617, 618, 620]
    * **Voice Retrieval**: Fetches all voices belonging to the user, specifically filtering for "cloned", "generated", and "professional" categories. [cite: 622, 624]
    * **Text Synthesis**: For each retrieved voice, it synthesizes the provided input text. [cite: 628, 631]
        * Uses a specified `model_id` (default: `eleven_multilingual_v2`) and `output_format` (default: `mp3_44100_128`). [cite: 631, 634]
        * Includes retry logic for API errors during synthesis and voice retrieval. [cite: 621, 628]
    * **Output**: Saves each synthesized audio as an MP3 file in a specified output folder (default: `output_audio`). [cite: 631, 633, 636]
        * Filenames are sanitized and structured as `{voice_id}--{sanitized_voice_name}.mp3`. [cite: 626, 638]
    * **Progress & Logging**: Uses `rich` for progress bars during synthesis and `loguru` for logging. [cite: 615, 619, 637]
* **Tech Stack**: Python, `elevenlabs` SDK, `fire` (CLI), `python-dotenv`, `loguru`, `tenacity`, `rich`. [cite: 615]

### 1.4. Utility Script: `malmo_neifix.py`

#### 1.4.1. `malmo_neifix.py`
* **Purpose**: A script to parse XML files and transform the text content inside `<nei>` tags according to specific capitalization and hyphenation rules. [cite: 423]
* **Functionality**:
    * **XML Parsing**: Reads an input XML file. [cite: 429]
    * **NEI Tag Targeting**: Uses regex to find all `<nei ...>` tags and their content. [cite: 430]
    * **Content Transformation**: For the text content within each `<nei>` tag:
        * Splits the content into words.
        * For each word:
            * Preserves single-letter words (likely parts of acronyms). [cite: 426]
            * Removes hyphens. [cite: 427]
            * Keeps the first letter of the (hyphen-less) word as is.
            * Converts all subsequent letters in the word to lowercase. [cite: 427]
        * Joins the transformed words back with spaces. [cite: 428]
    * **Output**: Writes the modified XML content to an output file or to stdout if no output file is specified. [cite: 433, 434]
* **Tech Stack**: Python, `fire`, `rich`. [cite: 423]
</file>

<file path="malmo_11labs.py">
#!/usr/bin/env -S uv run -s
# /// script
# dependencies = ["fire", "lxml", "loguru", "rich", "types-lxml", "litellm<=1.67.2"]
# ///
# this_file: malmo_11labs.py

import fire
import re
import os
import html
from lxml import etree
from litellm._logging import _turn_on_debug
from loguru import logger
from rich.console import Console
from rich.progress import (
    Progress,
    TextColumn,
    BarColumn,
    TimeElapsedColumn,
    TimeRemainingColumn,
)

# Set up Rich console for pretty output
console = Console()

# Unicode smart quotes
OPEN_Q = "\u201c"  # Left double quotation mark
CLOSE_Q = "\u201d"  # Right double quotation mark

# Quote character sets for normalization
OPEN_QQ = """‚ùõ‚ùùüô∂üô∏¬ª‚Ä∫'‚Äö‚Äõ"‚Äû‚Äü‚πÇ‚åú„Äå„Äé„ÄùÔΩ¢ÔπÅÔπÉ‚†¥"""
CLOSE_QQ = """‚ùú‚ùûüô∑¬´‚Äπ'"‚åù„Äç„Äè„Äû„ÄüÔΩ£ÔπÇÔπÄ‚†¶"""


def parse_xml(input_file: str) -> etree._Element:
    """
    Parse the input XML file.

    Args:
        input_file: Path to the input XML file

    Returns:
        Parsed XML element tree

    Raises:
        ValueError: If the XML cannot be parsed
    """
    try:
        parser = etree.XMLParser(remove_blank_text=False, recover=True)
        with open(input_file, encoding="utf-8") as f:
            xml_content = f.read()

        if not xml_content.strip():
            msg = f"File is empty: {input_file}"
            raise ValueError(msg)

        tree = etree.fromstring(xml_content.encode("utf-8"), parser)
        if tree is None:
            msg = f"Failed to parse XML: {input_file}"
            raise ValueError(msg)
        return tree
    except Exception as e:
        logger.error(f"Error parsing XML: {e}")
        # Check if file exists and has readable content
        if not os.path.exists(input_file):
            msg = f"Input file not found: {input_file}"
            raise ValueError(msg)
        elif os.path.getsize(input_file) == 0:
            msg = f"Input file is empty: {input_file}"
            raise ValueError(msg)
        else:
            # Include a small sample of the file in the error message
            with open(input_file, encoding="utf-8", errors="replace") as f:
                sample = f.read(200)
            msg = (
                f"Failed to parse XML file: {input_file}\n"
                f"Error: {e!s}\n"
                f"File sample: {sample}..."
            )
            raise ValueError(msg)


def get_chunks(root: etree._Element) -> list[etree._Element]:
    """
    Extract chunks from the XML document.

    Args:
        root: Root XML element

    Returns:
        List of chunk elements
    """
    return root.xpath("//chunk")


def get_items_from_chunk(chunk: etree._Element) -> list[etree._Element]:
    """
    Extract all item elements from a chunk.

    Args:
        chunk: Chunk element

    Returns:
        List of item elements
    """
    return chunk.xpath(".//item")


def normalize_consecutive_quotes(text: str) -> str:
    """
    Replace multiple consecutive opening or closing quotes with a single
    smart quote.

    Args:
        text: Input text with potentially consecutive quotes

    Returns:
        Text with normalized quotation marks
    """
    # Create regex patterns for consecutive opening and closing quotes
    opening_pattern = f"[{re.escape(OPEN_QQ)}]+"
    closing_pattern = f"[{re.escape(CLOSE_QQ)}]+"

    # Replace consecutive opening quotes with a single smart opening quote
    text = re.sub(opening_pattern, OPEN_Q, text)

    # Replace consecutive closing quotes with a single smart closing quote
    text = re.sub(closing_pattern, CLOSE_Q, text)

    return text


def postprocess(text: str) -> str:
    """
    Final clean-ups:
    1) turn &lt; &gt; into real brackets
    2) collapse 3+ consecutive newlines -> exactly 2
    3) normalize consecutive quotation marks
    """
    text = html.unescape(text)
    text = re.sub(r"\n{3,}", "\n\n", text).strip()
    text = normalize_consecutive_quotes(text)
    return text


def final_postprocess(text: str) -> str:
    """
    Apply final text formatting rules:
    1) Replace start of line + <q> by the opening quote
    2) Replace </q> + the end of line by the closing quote
    3) Replace <q> by space + the opening quote
    4) Replace </q> by the closing quote + space
    5) Replace the portion from <break time= to the nearest /> by ...
    """
    # Replace start of line + <q> by the opening quote
    text = re.sub(r"^<q>", OPEN_Q, text, flags=re.MULTILINE)

    # Replace </q> + the end of line by the closing quote
    text = re.sub(r"</q>$", CLOSE_Q, text, flags=re.MULTILINE)

    # Replace <q> by space + the opening quote
    text = re.sub(r"<q>", f"; {OPEN_Q}", text)

    # Replace </q> by the closing quote + space
    text = re.sub(r"</q>", f"{CLOSE_Q}; ", text)

    # Replace the portion from <break time= to the nearest /> by ...
    text = re.sub(r"<break time=[^/>]+/>", "...", text)

    # Replace one newline with two
    text = re.sub(r"\n", "\n\n", text)

    return text


def process_dialog_lines(text: str) -> str:
    """
    Process dialog lines to add proper <q></q> tags.

    Dialog lines start with "‚Äî " (em dash + space).
    Dialog toggles are " ‚Äî " (space + em dash + space).

    Args:
        text: Text to process

    Returns:
        Processed text with <q></q> tags for dialogs
    """
    lines = text.split("\n")
    processed_lines = []

    for line in lines:
        # Check if this is a dialog line (starts with "‚Äî ")
        if line.startswith("‚Äî "):
            # Replace the dialog open with <q>
            processed_line = "<q>" + line[2:]

            # Track the current tag state (True = <q> was last, False = </q> was last)
            tag_open = True

            # Find all dialog toggles
            toggle_positions = [m.start() for m in re.finditer(" ‚Äî ", processed_line)]

            # Process each toggle
            offset = 0  # Adjustment for the changing string length as we replace
            for pos in toggle_positions:
                adjusted_pos = pos + offset
                if tag_open:
                    # Replace with closing tag
                    processed_line = (
                        processed_line[:adjusted_pos]
                        + "</q>"
                        + processed_line[adjusted_pos + 3 :]
                    )
                    offset += len("</q>") - 3  # -3 for the replaced " ‚Äî "
                else:
                    # Replace with opening tag
                    processed_line = (
                        processed_line[:adjusted_pos]
                        + "<q>"
                        + processed_line[adjusted_pos + 3 :]
                    )
                    offset += len("<q>") - 3  # -3 for the replaced " ‚Äî "

                tag_open = not tag_open  # Toggle the state

            # Close any open dialog tag at the end of the line
            if tag_open:
                processed_line += "</q>"

            processed_lines.append(processed_line)
        else:
            processed_lines.append(line)

    return "\n".join(processed_lines)


def process_item_content(item_element: etree._Element) -> str:
    """
    Process the content of an item element to extract and format text.

    Args:
        item_element: Item element from XML

    Returns:
        Formatted text from the item
    """
    # -------------------------------------------------
    # 0.  get the raw innerXML of <item> ‚Ä¶ </item>
    raw = etree.tostring(item_element, encoding="unicode", method="xml")
    inner = re.search(r"<item[^>]*>(.*?)</item>", raw, re.DOTALL)
    if not inner:
        return ""
    txt = inner.group(1)

    # -------------------------------------------------
    # 1.  <em> ‚Ä¶ </em>  ‚Üí  "‚Ä¶"        (smart quotes)
    txt = re.sub(r"<em>(.*?)</em>", rf"{OPEN_Q}\1{CLOSE_Q}", txt, flags=re.DOTALL)

    # -------------------------------------------------
    # 2a. <nei ‚Ä¶ new="true" ‚Ä¶> ‚Ä¶ </nei> ‚Üí  "‚Ä¶"
    txt = re.sub(
        r'<nei[^>]*\bnew="true"[^>]*>(.*?)</nei>',
        rf"{OPEN_Q}\1{CLOSE_Q}",
        txt,
        flags=re.DOTALL,
    )

    # 2b. remaining <nei ‚Ä¶> ‚Ä¶ </nei>      ‚Üí  plain content
    txt = re.sub(r"<nei[^>]*>(.*?)</nei>", r"\1", txt, flags=re.DOTALL)

    # -------------------------------------------------
    # 3.  <hr/> (any spelling) ‚Üí escaped break tag
    txt = re.sub(r"<hr\s*/?>", r'&lt;break time="0.5s" /&gt;', txt)

    # -------------------------------------------------
    # 4.  strip **all** remaining tags
    txt = re.sub(r"<[^>]+>", "", txt)

    # -------------------------------------------------
    # 5.  unescape &lt; &gt;  +  squeeze blank lines
    return postprocess(txt)


def process_chunk(chunk: etree._Element) -> list[str]:
    """
    Process all items in a chunk.

    Args:
        chunk: Chunk element from XML

    Returns:
        List of processed text lines from the chunk
    """
    processed_lines = []
    items = get_items_from_chunk(chunk)

    for item in items:
        processed_text = process_item_content(item)
        if processed_text:
            processed_lines.append(processed_text)

    return processed_lines


def process_document(
    input_file: str,
    output_file: str,
    verbose: bool = False,
    dialog: bool = False,
) -> None:
    """
    Process the entire XML document and generate output file.

    Args:
        input_file: Path to the input XML file
        output_file: Path to the output file
        verbose: Whether to enable verbose logging
        dialog: Whether to process dialog lines with <q></q> tags
    """
    # Configure logging
    log_level = "DEBUG" if verbose else "INFO"
    logger.remove()
    logger.add(lambda msg: console.print(msg, highlight=False), level=log_level)

    logger.info(f"Processing XML document: {input_file}")
    logger.info(f"Output will be saved to: {output_file}")
    if dialog:
        logger.info("Dialog processing is enabled")

    # Parse the XML
    root = parse_xml(input_file)
    chunks = get_chunks(root)
    logger.info(f"Found {len(chunks)} chunks in the document")

    all_lines = []

    # Process each chunk
    with Progress(
        TextColumn("[progress.description]{task.description}"),
        BarColumn(),
        TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
        TimeElapsedColumn(),
        TimeRemainingColumn(),
        console=console,
    ) as progress:
        process_task = progress.add_task("Processing chunks", total=len(chunks))

        for i, chunk in enumerate(chunks):
            logger.debug(
                f"Processing chunk {i + 1}/{len(chunks)} "
                f"(id: {chunk.get('id', 'unknown')})"
            )

            # Process the chunk
            processed_lines = process_chunk(chunk)
            all_lines.extend(processed_lines)

            progress.update(process_task, advance=1)

    # Join all lines
    output_text = "\n\n".join(all_lines)

    # Process dialog lines if enabled
    if dialog:
        logger.debug("Processing dialog lines")
        output_text = process_dialog_lines(output_text)

    # Apply final postprocessing
    logger.debug("Applying final postprocessing")
    output_text = final_postprocess(output_text)

    # Write to output file
    with open(output_file, "w", encoding="utf-8") as f:
        f.write(output_text)

    logger.info(f"Processing complete. Output saved to {output_file}")


def main(
    input_file: str,
    output_file: str,
    verbose: bool = True,
    dialog: bool = False,
    plaintext: bool = False,
) -> None:
    """
    Convert XML to 11Labs compatible text format.

    Args:
        input_file: Path to the input XML file
        output_file: Path to the output file
        verbose: Whether to enable verbose logging
        dialog: Whether to process dialog lines with <q></q> tags
        plaintext: Process as plain text instead of XML
    """
    if verbose:
        logger.level("DEBUG")
        _turn_on_debug()

    try:
        # Check if input file exists
        if not os.path.exists(input_file):
            console.print(
                f"[bold red]Error:[/bold red] Input file not found: {input_file}"
            )
            return

        # If plaintext option is set, process as plain text
        if plaintext:
            logger.info(f"Processing as plain text: {input_file}")
            with open(input_file, encoding="utf-8", errors="replace") as f:
                text = f.read()

            # Process dialog lines if enabled
            if dialog:
                logger.debug("Processing dialog lines")
                text = process_dialog_lines(text)

            # Apply final postprocessing
            logger.debug("Applying final postprocessing")
            text = final_postprocess(text)

            # Write to output file
            with open(output_file, "w", encoding="utf-8") as f:
                f.write(text)

            console.print(
                f"[bold green]Success:[/bold green] Processed {input_file} "
                f"as plain text"
            )
            return

        # Process the document as XML
        process_document(input_file, output_file, verbose, dialog)

        console.print(
            f"[bold green]Success:[/bold green] Converted {input_file} to {output_file}"
        )

    except ValueError as e:
        error_msg = str(e)
        console.print(f"[bold red]Error:[/bold red] {error_msg}")

        # Check if this might be a plain text file
        try:
            with open(input_file, encoding="utf-8", errors="replace") as f:
                sample = f.read(100)

            # Simple heuristic: if it doesn't start with < or has lines without tags
            if not sample.strip().startswith("<") or any(
                line.strip() and not ("<" in line and ">" in line)
                for line in sample.split("\n")
                if line.strip()
            ):
                console.print(
                    "[bold yellow]Suggestion:[/bold yellow] This appears to be a "
                    "plain text file, not XML. Try again with the --plaintext flag:"
                )
                console.print(
                    f"[bold cyan]python -m malmo_11labs {input_file} {output_file} "
                    f"--plaintext{'--dialog' if dialog else ''}[/bold cyan]"
                )
        except Exception:
            # Ignore any errors in suggestion logic
            pass

        logger.exception("An error occurred during processing")
        raise

    except Exception as e:
        console.print(f"[bold red]Error:[/bold red] {e!s}")
        logger.exception("An error occurred during processing")
        raise


if __name__ == "__main__":
    fire.Fire(main)
</file>

<file path="malmo_all.py">
#!/usr/bin/env -S uv run -s
# /// script
# dependencies = ["fire", "loguru", "python-dotenv", "tenacity", "tqdm", "rich", "litellm<=1.67.2", "elevenlabs"]
# ///
# this_file: malmo_all.py

import fire
import json
import os
import subprocess
import sys
from pathlib import Path
from typing import Any
from loguru import logger
import datetime
from dotenv import load_dotenv
from tenacity import (
    retry,
    stop_after_attempt,
    wait_exponential,
    retry_if_exception_type,
)
from rich.progress import (
    Progress,
    TextColumn,
    BarColumn,
    TimeElapsedColumn,
    SpinnerColumn,
)
from rich.console import Console
from rich.panel import Panel

# Load environment variables from .env file
load_dotenv()

# Constants
MAX_RETRIES = 3
RETRY_WAIT_MIN = 1  # seconds
RETRY_WAIT_MAX = 30  # seconds
PROGRESS_FILE_NAME = "progress.json"

# Default values from each tool
CHUNKER_DEFAULT_MODEL = "openrouter/openai/gpt-4.1"
CHUNKER_DEFAULT_CHUNK_SIZE = 12288
CHUNKER_DEFAULT_TEMPERATURE = 1.0

ENTITIZER_DEFAULT_MODEL = "openrouter/google/gemini-2.5-pro-preview-03-25"
ENTITIZER_DEFAULT_TEMPERATURE = 1.0

ORATOR_DEFAULT_MODEL = "openrouter/google/gemini-2.5-pro-preview-03-25"
ORATOR_DEFAULT_TEMPERATURE = 1.0

# Add default values for tonedown and 11labs steps
TONEDOWN_DEFAULT_MODEL = "openrouter/google/gemini-2.5-pro-preview-03-25"
TONEDOWN_DEFAULT_TEMPERATURE = 1.0

# Rich console for pretty output
console = Console()

# Required dependencies for each script
CHUNKER_DEPENDENCIES = [
    "backoff",
    "fire",
    "loguru",
    "python-dotenv",
    "openai",
    "tiktoken",
    "litellm",
]
ENTITIZER_DEPENDENCIES = [
    "fire",
    "loguru",
    "python-dotenv",
    "openai",
    "lxml",
    "litellm",
]
ORATOR_DEPENDENCIES = ["fire", "loguru", "python-dotenv", "openai", "lxml", "litellm"]

# Required dependencies for new steps
TONEDOWN_DEPENDENCIES = ["fire", "loguru", "python-dotenv", "openai", "lxml", "litellm"]
ELEVENLABS_DEPENDENCIES = ["fire", "loguru", "python-dotenv", "lxml"]


def ensure_dependencies_installed(dependencies: list[str]) -> bool:
    """
    Ensure that all required dependencies are installed.

    Args:
        dependencies: List of dependency package names

    Returns:
        True if all dependencies are installed or successfully installed
    """
    missing_deps = []

    # Check which dependencies are missing
    for dep in dependencies:
        try:
            __import__(
                dep.replace("-", "_")
            )  # Replace hyphen with underscore for import
        except ImportError:
            missing_deps.append(dep)

    if not missing_deps:
        return True

    # Install missing dependencies
    if missing_deps:
        deps_str = " ".join(missing_deps)
        console.print(f"[yellow]Installing missing dependencies: {deps_str}[/yellow]")

        try:
            # Try using uv
            uv_cmd = ["uv", "pip", "install", *missing_deps]
            subprocess.run(uv_cmd, check=True, capture_output=True, text=True)
            console.print("[green]Successfully installed dependencies using uv[/green]")
            return True
        except (subprocess.CalledProcessError, FileNotFoundError):
            try:
                # Fall back to pip if uv is not available
                pip_cmd = [sys.executable, "-m", "pip", "install", *missing_deps]
                subprocess.run(pip_cmd, check=True, capture_output=True, text=True)
                console.print(
                    "[green]Successfully installed dependencies using pip[/green]"
                )
                return True
            except subprocess.CalledProcessError as e:
                console.print(
                    "[bold red]Failed to install dependencies. Please install manually:[/bold red]"
                )
                console.print(f"[bold yellow]pip install {deps_str}[/bold yellow]")
                logger.error(f"Failed to install dependencies: {e}")
                return False

    return True


def create_output_directory(input_file: str) -> Path:
    """
    Create an output directory based on the input file name.

    Args:
        input_file: Path to the input file

    Returns:
        Path object for the output directory
    """
    input_path = Path(input_file)
    # Create directory with input filename (without extension) + timestamp
    timestamp = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
    output_dir = input_path.parent / f"{input_path.stem}_malmo_{timestamp}"
    output_dir.mkdir(exist_ok=True)
    return output_dir


@retry(
    stop=stop_after_attempt(MAX_RETRIES),
    wait=wait_exponential(multiplier=1, min=RETRY_WAIT_MIN, max=RETRY_WAIT_MAX),
    retry=retry_if_exception_type(subprocess.CalledProcessError),
    before_sleep=lambda retry_state: logger.warning(
        f"Retrying command (attempt {retry_state.attempt_number}/{MAX_RETRIES}) after {retry_state.outcome.exception() if retry_state.outcome else 'error'}"
    ),
)
def run_command(cmd: list[str], verbose: bool = False) -> dict:
    """
    Run a command as a subprocess with retries on failure.

    Args:
        cmd: Command to run, as a list of strings
        verbose: Whether to print verbose output

    Returns:
        Dict with stdout, stderr, and return code
    """
    # Ensure all parts of the command are properly encoded strings
    sanitized_cmd = []
    for part in cmd:
        if part is not None:
            sanitized_cmd.append(str(part))

    cmd_str = " ".join(
        [
            f'"{c}"'
            if " " in c or any(special in c for special in "()[]{}*?!$&|<>;'\"")
            else c
            for c in sanitized_cmd
        ]
    )
    logger.debug(f"Running command: {cmd_str}")

    try:
        # Use shell=False for better security and handling of arguments with special characters
        result = subprocess.run(
            sanitized_cmd, check=True, capture_output=True, text=True, encoding="utf-8"
        )

        if verbose and result.stdout:
            logger.debug(result.stdout)

        if result.stderr:
            logger.warning(result.stderr)

        return {
            "stdout": result.stdout,
            "stderr": result.stderr,
            "returncode": result.returncode,
        }
    except UnicodeEncodeError as ue:
        # Handle Unicode encoding errors explicitly
        logger.error(f"Unicode encoding error: {ue}")
        logger.debug(
            "Command contained non-ASCII characters that couldn't be encoded properly."
        )
        raise


def save_progress(output_dir: Path, progress_state: dict[str, Any]) -> None:
    """
    Save the current progress state to a file.

    Args:
        output_dir: Directory to save the progress file in
        progress_state: Current progress state
    """
    progress_file = output_dir / PROGRESS_FILE_NAME
    with open(progress_file, "w") as f:
        json.dump(progress_state, f, indent=2, default=str)
    logger.debug(f"Progress saved to {progress_file}")


def load_progress(output_dir: Path) -> dict[str, Any]:
    """
    Load progress state from a file.

    Args:
        output_dir: Directory containing the progress file

    Returns:
        Progress state as a dict, or empty dict if file not found
    """
    progress_file = output_dir / PROGRESS_FILE_NAME
    if not progress_file.exists():
        return {}

    try:
        with open(progress_file) as f:
            return json.load(f)
    except (json.JSONDecodeError, OSError) as e:
        logger.warning(f"Could not load progress file: {e}")
        return {}


def step_completed(progress_state: dict[str, Any], step: int) -> bool:
    """
    Check if a step has been completed successfully.

    Args:
        progress_state: Current progress state
        step: Step number to check

    Returns:
        True if the step is completed, False otherwise
    """
    return progress_state.get(f"step{step}_completed", False)


def initialize_progress(
    input_file: str,
    output_dir: Path,
    step1_output: Path,
    step2_output: Path,
    step3_output: Path,
    step4_output: Path,
    step5_output: Path,
    chunker_model: str,
    chunker_temperature: float,
    chunker_chunk_size: int,
    entitizer_model: str,
    entitizer_temperature: float,
    orator_model: str,
    orator_temperature: float,
    tonedown_model: str,
    tonedown_temperature: float,
) -> dict[str, Any]:
    """
    Initialize or load the progress state.

    Args:
        input_file: Path to the input file
        output_dir: Output directory
        step1_output: Path to step 1 output file
        step2_output: Path to step 2 output file
        step3_output: Path to step 3 output file
        step4_output: Path to step 4 output file (tonedown)
        step5_output: Path to step 5 output file (11labs)
        chunker_model: Model to use for chunker
        chunker_temperature: Temperature for chunker
        chunker_chunk_size: Chunk size for chunker
        entitizer_model: Model to use for entitizer
        entitizer_temperature: Temperature for entitizer
        orator_model: Model to use for orator
        orator_temperature: Temperature for orator
        tonedown_model: Model to use for tonedown
        tonedown_temperature: Temperature for tonedown

    Returns:
        Progress state as a dict
    """
    # Try to load existing progress
    progress_state = load_progress(output_dir)

    # If no existing progress or configuration has changed, init new progress
    if not progress_state or progress_state.get("input_file") != str(input_file):
        progress_state = {
            "input_file": str(input_file),
            "start_time": datetime.datetime.now().isoformat(),
            "output_dir": str(output_dir),
            "step1_output": str(step1_output),
            "step2_output": str(step2_output),
            "step3_output": str(step3_output),
            "step4_output": str(step4_output),
            "step5_output": str(step5_output),
            "chunker_model": chunker_model,
            "chunker_temperature": chunker_temperature,
            "chunker_chunk_size": chunker_chunk_size,
            "entitizer_model": entitizer_model,
            "entitizer_temperature": entitizer_temperature,
            "orator_model": orator_model,
            "orator_temperature": orator_temperature,
            "tonedown_model": tonedown_model,
            "tonedown_temperature": tonedown_temperature,
            "step1_attempts": 0,
            "step2_attempts": 0,
            "step3_attempts": 0,
            "step4_attempts": 0,
            "step5_attempts": 0,
            "step1_completed": False,
            "step2_completed": False,
            "step3_completed": False,
            "step4_completed": False,
            "step5_completed": False,
            "step1_error": None,
            "step2_error": None,
            "step3_error": None,
            "step4_error": None,
            "step5_error": None,
        }
        save_progress(output_dir, progress_state)

    return progress_state


def run_step(
    step: int,
    cmd: list[str],
    output_file: Path,
    progress_state: dict[str, Any],
    output_dir: Path,
    verbose: bool = False,
) -> tuple[bool, dict[str, Any]]:
    """
    Run a processing step with proper error handling and progress tracking.

    Args:
        step: Step number (1, 2, or 3)
        cmd: Command to run
        output_file: Expected output file
        progress_state: Current progress state
        output_dir: Directory to save progress file
        verbose: Whether to print verbose output

    Returns:
        Tuple of (success, updated_progress_state)
    """
    step_key = f"step{step}"
    attempts_key = f"{step_key}_attempts"
    completed_key = f"{step_key}_completed"
    error_key = f"{step_key}_error"

    # If step is already completed and output file exists, skip
    if progress_state.get(completed_key, False) and output_file.exists():
        logger.info(f"Step {step} already completed, skipping")
        return True, progress_state

    # Increment attempt counter
    progress_state[attempts_key] = progress_state.get(attempts_key, 0) + 1
    save_progress(output_dir, progress_state)

    # Run the command with retries
    try:
        run_command(cmd, verbose)

        # Verify output file exists
        if not output_file.exists():
            msg = f"Output file {output_file} was not created"
            raise FileNotFoundError(msg)

        # Update progress
        progress_state[completed_key] = True
        progress_state[error_key] = None
        progress_state[f"{step_key}_completed_time"] = (
            datetime.datetime.now().isoformat()
        )
        save_progress(output_dir, progress_state)

        logger.success(f"Step {step} completed. Output saved to {output_file}")
        return True, progress_state

    except Exception as e:
        # Capture detailed error information
        error_detail = str(e)

        # For subprocess errors, try to get more details
        if isinstance(e, subprocess.CalledProcessError):
            error_detail = f"Command failed with exit code {e.returncode}:\n"
            if e.stderr:
                error_detail += f"STDERR: {e.stderr}\n"
            if e.stdout:
                error_detail += f"STDOUT: {e.stdout}\n"

        # For tenacity RetryError, unwrap to get the real exception
        if "RetryError" in str(e):
            try:
                # Look for CalledProcessError in the exception message
                (str(e).replace("RetryError[<Future at ", "").split(" state=")[0])
                logger.error(
                    f"Command failed after {MAX_RETRIES} retries. Run with --verbose for more details."
                )

                # Just extract any useful information from the error string
                if "CalledProcessError" in str(e):
                    # Try to get the exit code
                    if "exit code" in str(e):
                        exit_code = str(e).split("exit code")[1].split()[0].strip()
                        error_detail += f"Command failed with exit code {exit_code}\n"

                # Additional debugging info
                if verbose:
                    logger.debug(f"Full error: {e!s}")
                    logger.debug(f"Command was: {' '.join(cmd)}")
            except (AttributeError, IndexError) as parse_error:
                # Couldn't parse the error message
                logger.debug(f"Failed to parse error details: {parse_error}")

        # Update progress with error
        progress_state[completed_key] = False
        progress_state[error_key] = error_detail
        save_progress(output_dir, progress_state)

        logger.error(f"Error in Step {step}: {e}")
        if verbose:
            logger.error(f"Detailed error information:\n{error_detail}")
        return False, progress_state


def process_file(
    input_file: str,
    output_dir: Path | None = None,
    chunker_model: str = CHUNKER_DEFAULT_MODEL,
    chunker_temperature: float = CHUNKER_DEFAULT_TEMPERATURE,
    chunker_chunk_size: int = CHUNKER_DEFAULT_CHUNK_SIZE,
    entitizer_model: str = ENTITIZER_DEFAULT_MODEL,
    entitizer_temperature: float = ENTITIZER_DEFAULT_TEMPERATURE,
    orator_model: str = ORATOR_DEFAULT_MODEL,
    orator_temperature: float = ORATOR_DEFAULT_TEMPERATURE,
    tonedown_model: str = TONEDOWN_DEFAULT_MODEL,
    tonedown_temperature: float = TONEDOWN_DEFAULT_TEMPERATURE,
    verbose: bool = True,
    backup: bool = True,
    start_from_step: int = 1,
    force_restart: bool = False,
) -> None:
    """
    Process a file through all malmo tools with robust error handling and progress tracking.

    Args:
        input_file: Path to the input file
        output_dir: Output directory (created automatically if not provided)
        chunker_model: Model to use for chunker
        chunker_temperature: Temperature for chunker
        chunker_chunk_size: Chunk size for chunker
        entitizer_model: Model to use for entitizer
        entitizer_temperature: Temperature for entitizer
        orator_model: Model to use for orator
        orator_temperature: Temperature for orator
        tonedown_model: Model to use for tonedown
        tonedown_temperature: Temperature for tonedown
        verbose: Enable verbose logging
        backup: Create backups at each step
        start_from_step: Start processing from this step (1-5)
        force_restart: Ignore previous progress and start from scratch
    """
    # Setup output directory if not provided
    if output_dir is None:
        output_dir = create_output_directory(input_file)
    else:
        output_dir.mkdir(exist_ok=True)

    console.print(
        Panel.fit(
            f"[bold green]Processing:[/bold green] [yellow]{input_file}[/yellow]\n"
            f"[bold green]Output directory:[/bold green] [yellow]{output_dir}[/yellow]",
            title="Malmo All-in-One Processor",
            border_style="blue",
        )
    )

    # Check dependencies based on what steps will be run
    all_dependencies = set()
    if start_from_step <= 1:
        all_dependencies.update(CHUNKER_DEPENDENCIES)
    if start_from_step <= 2:
        all_dependencies.update(ENTITIZER_DEPENDENCIES)
    if start_from_step <= 3:
        all_dependencies.update(ORATOR_DEPENDENCIES)
    if start_from_step <= 4:
        all_dependencies.update(TONEDOWN_DEPENDENCIES)
    if start_from_step <= 5:
        all_dependencies.update(ELEVENLABS_DEPENDENCIES)

    if not ensure_dependencies_installed(list(all_dependencies)):
        console.print(
            "[bold red]Critical dependencies are missing. Cannot continue.[/bold red]"
        )
        return

    # Normalize input file path to handle special characters
    input_path = Path(input_file).resolve()

    # Directly check file existence and readability
    if not input_path.exists():
        error_msg = f"Input file does not exist: {input_path}"
        logger.error(error_msg)
        console.print(f"[bold red]{error_msg}[/bold red]")
        if output_dir:
            progress_state = {"step1_error": error_msg}
            save_progress(output_dir, progress_state)
        return

    if not input_path.is_file():
        error_msg = f"Input path is not a file: {input_path}"
        logger.error(error_msg)
        console.print(f"[bold red]{error_msg}[/bold red]")
        if output_dir:
            progress_state = {"step1_error": error_msg}
            save_progress(output_dir, progress_state)
        return

    # Try to read the file to verify it's accessible
    try:
        with open(input_path, encoding="utf-8") as f:
            # Just read a bit to make sure it's accessible
            sample = f.read(1024)
            logger.debug(
                f"Successfully read sample from {input_path} ({len(sample)} bytes)"
            )
    except (OSError, UnicodeDecodeError) as e:
        error_msg = f"Error reading input file: {e}"
        logger.error(error_msg)
        console.print(f"[bold red]{error_msg}[/bold red]")
        if output_dir:
            progress_state = {"step1_error": error_msg}
            save_progress(output_dir, progress_state)
        return

    # Define file paths for each step
    step1_output = output_dir / f"{input_path.stem}_step1_chunked.xml"
    step2_output = output_dir / f"{input_path.stem}_step2_entited.xml"
    step3_output = output_dir / f"{input_path.stem}_step3_orated.xml"
    step4_output = output_dir / f"{input_path.stem}_step4_toneddown.xml"
    step5_output = output_dir / f"{input_path.stem}_step5_11labs.txt"

    # Get current directory for relative paths to python scripts
    current_dir = Path(__file__).parent.absolute()

    # Initialize/load progress tracking
    if force_restart:
        # Remove existing progress file if forcing restart
        progress_file = output_dir / PROGRESS_FILE_NAME
        if progress_file.exists():
            progress_file.unlink()

    progress_state = initialize_progress(
        input_file=str(input_path),
        output_dir=output_dir,
        step1_output=step1_output,
        step2_output=step2_output,
        step3_output=step3_output,
        step4_output=step4_output,
        step5_output=step5_output,
        chunker_model=chunker_model,
        chunker_temperature=chunker_temperature,
        chunker_chunk_size=chunker_chunk_size,
        entitizer_model=entitizer_model,
        entitizer_temperature=entitizer_temperature,
        orator_model=orator_model,
        orator_temperature=orator_temperature,
        tonedown_model=tonedown_model,
        tonedown_temperature=tonedown_temperature,
    )

    # Create the steps with their respective commands
    steps = []

    # Step 1: Run malmo_chunker
    chunker_script = current_dir / "malmo_chunker.py"

    # Check if the chunker script exists
    if not chunker_script.exists():
        error_msg = f"Chunker script not found: {chunker_script}"
        logger.error(error_msg)
        console.print(f"[bold red]{error_msg}[/bold red]")
        progress_state["step1_error"] = error_msg
        save_progress(output_dir, progress_state)
        return

    chunker_cmd = [
        sys.executable,
        str(chunker_script),
        input_file,
        str(step1_output),
        "--chunk",
        str(chunker_chunk_size),
        "--model",
        chunker_model,
        "--temperature",
        str(chunker_temperature),
    ]

    # Try to run malmo_chunker with --help to check if it's working correctly
    if start_from_step == 1:
        try:
            logger.info("Testing if the chunker script can be executed correctly...")
            help_cmd = [sys.executable, str(chunker_script), "--help"]
            result = subprocess.run(
                help_cmd, capture_output=True, text=True, check=False
            )
            if result.returncode != 0:
                logger.warning(f"Chunker help command failed: {result.stderr}")
                # If the help command fails, check if the script is executable
                if not os.access(chunker_script, os.X_OK):
                    logger.warning(
                        "The chunker script is not executable. Attempting to fix permissions."
                    )
                    os.chmod(chunker_script, 0o755)  # Make executable
        except Exception as e:
            logger.warning(f"Error testing chunker script: {e}")

    if verbose:
        chunker_cmd.append("--verbose")
    if backup:
        chunker_cmd.append("--backup")
    steps.append((1, chunker_cmd, step1_output, "Running chunker"))

    # Step 2: Run malmo_entitizer
    entitizer_script = current_dir / "malmo_entitizer.py"
    entitizer_cmd = [
        sys.executable,
        str(entitizer_script),
        str(step1_output),
        str(step2_output),
        "--model",
        entitizer_model,
        "--temperature",
        str(entitizer_temperature),
    ]
    if verbose:
        entitizer_cmd.append("--verbose")
    if backup:
        entitizer_cmd.append("--backup")
    steps.append((2, entitizer_cmd, step2_output, "Running entitizer"))

    # Step 3: Run malmo_orator
    orator_script = current_dir / "malmo_orator.py"
    orator_cmd = [
        sys.executable,
        str(orator_script),
        str(step2_output),
        str(step3_output),
        "--model",
        orator_model,
        "--temperature",
        str(orator_temperature),
        "--all_steps",  # Apply all enhancement steps
    ]
    if verbose:
        orator_cmd.append("--verbose")
    if backup:
        orator_cmd.append("--backup")
    steps.append((3, orator_cmd, step3_output, "Running orator"))

    # Step 4: Run malmo_tonedown
    tonedown_script = current_dir / "malmo_tonedown.py"
    tonedown_cmd = [
        sys.executable,
        str(tonedown_script),
        str(step3_output),
        str(step4_output),
        "--model",
        tonedown_model,
        "--temperature",
        str(tonedown_temperature),
    ]
    if verbose:
        tonedown_cmd.append("--verbose")
    if backup:
        tonedown_cmd.append("--backup")
    steps.append((4, tonedown_cmd, step4_output, "Running tonedown"))

    # Step 5: Run malmo_11labs
    elevenlabs_script = current_dir / "malmo_11labs.py"
    elevenlabs_cmd = [
        sys.executable,
        str(elevenlabs_script),
        str(step4_output),
        str(step5_output),
    ]
    if verbose:
        elevenlabs_cmd.append("--verbose")
    steps.append((5, elevenlabs_cmd, step5_output, "Running 11labs"))

    # Process all steps in sequence with progress tracking
    with Progress(
        SpinnerColumn(),
        TextColumn("[bold blue]{task.description}"),
        BarColumn(),
        TextColumn("[bold green]{task.completed}/{task.total}"),
        TimeElapsedColumn(),
    ) as progress:
        overall_task = progress.add_task("[yellow]Overall progress", total=len(steps))

        for step_num, cmd, output_file, description in steps:
            # Skip steps before start_from_step
            if step_num < start_from_step:
                if step_completed(progress_state, step_num):
                    progress.advance(overall_task)
                continue

            # Check for dependency (previous step must be completed)
            if step_num > 1 and not step_completed(progress_state, step_num - 1):
                error_msg = f"Cannot run step {step_num} because step {step_num - 1} has not been completed"
                progress_state[f"step{step_num}_error"] = error_msg
                logger.error(error_msg)
                save_progress(output_dir, progress_state)
                break

            step_task = progress.add_task(
                f"[cyan]Step {step_num}: {description}", total=1
            )
            logger.info(f"Step {step_num}: {description}")

            success, progress_state = run_step(
                step=step_num,
                cmd=cmd,
                output_file=output_file,
                progress_state=progress_state,
                output_dir=output_dir,
                verbose=verbose,
            )

            progress.update(step_task, completed=1)
            progress.advance(overall_task)

            if not success:
                break

    # Check if all steps completed successfully
    all_completed = all(
        step_completed(progress_state, i + 1) for i in range(len(steps))
    )

    if all_completed:
        logger.success(f"Processing complete! Final output: {step5_output}")
        console.print(
            Panel.fit(
                f"[bold green]Final output:[/bold green] [yellow]{step5_output}[/yellow]",
                title="Processing Complete",
                border_style="green",
            )
        )
    else:
        logger.warning("Processing did not complete successfully.")
        failed_step = next(
            (
                i + 1
                for i in range(len(steps))
                if not step_completed(progress_state, i + 1)
            ),
            None,
        )
        error_msg = progress_state.get(f"step{failed_step}_error", "Unknown error")

        console.print(
            Panel.fit(
                f"[bold red]Processing stopped at step {failed_step}[/bold red]\n"
                f"[bold yellow]Error:[/bold yellow] {error_msg}\n\n"
                f"To resume from this step, run again with:\n"
                f"--start_from_step={failed_step}",
                title="Processing Incomplete",
                border_style="red",
            )
        )

    # Create a summary file
    summary_file = output_dir / f"{input_path.stem}_summary.txt"
    with open(summary_file, "w") as f:
        f.write("Malmo processing summary\n")
        f.write("======================\n\n")
        f.write(f"Input file: {input_file}\n")

        # Add start time
        start_time = datetime.datetime.fromisoformat(
            progress_state.get("start_time", datetime.datetime.now().isoformat())
        )
        f.write(f"Start time: {start_time.strftime('%Y-%m-%d %H:%M:%S')}\n")

        # Add end time
        end_time = datetime.datetime.now()
        f.write(f"End time: {end_time.strftime('%Y-%m-%d %H:%M:%S')}\n")

        # Calculate total duration
        duration = end_time - start_time
        hours, remainder = divmod(duration.total_seconds(), 3600)
        minutes, seconds = divmod(remainder, 60)
        f.write(f"Total duration: {int(hours)}h {int(minutes)}m {int(seconds)}s\n\n")

        # Step details
        f.write("Step 1 (Chunker):\n")
        f.write(f"  - Output: {step1_output}\n")
        f.write(f"  - Model: {chunker_model}\n")
        f.write(f"  - Temperature: {chunker_temperature}\n")
        f.write(f"  - Chunk size: {chunker_chunk_size}\n")
        f.write(f"  - Completed: {progress_state.get('step1_completed', False)}\n")
        f.write(f"  - Attempts: {progress_state.get('step1_attempts', 0)}\n")
        if progress_state.get("step1_error"):
            f.write(f"  - Error: {progress_state.get('step1_error')}\n")
        f.write("\n")

        f.write("Step 2 (Entitizer):\n")
        f.write(f"  - Output: {step2_output}\n")
        f.write(f"  - Model: {entitizer_model}\n")
        f.write(f"  - Temperature: {entitizer_temperature}\n")
        f.write(f"  - Completed: {progress_state.get('step2_completed', False)}\n")
        f.write(f"  - Attempts: {progress_state.get('step2_attempts', 0)}\n")
        if progress_state.get("step2_error"):
            f.write(f"  - Error: {progress_state.get('step2_error')}\n")
        f.write("\n")

        f.write("Step 3 (Orator):\n")
        f.write(f"  - Output: {step3_output}\n")
        f.write(f"  - Model: {orator_model}\n")
        f.write(f"  - Temperature: {orator_temperature}\n")
        f.write(f"  - Completed: {progress_state.get('step3_completed', False)}\n")
        f.write(f"  - Attempts: {progress_state.get('step3_attempts', 0)}\n")
        if progress_state.get("step3_error"):
            f.write(f"  - Error: {progress_state.get('step3_error')}\n")
        f.write("\n")

        f.write("Step 4 (Tonedown):\n")
        f.write(f"  - Output: {step4_output}\n")
        f.write(f"  - Model: {tonedown_model}\n")
        f.write(f"  - Temperature: {tonedown_temperature}\n")
        f.write(f"  - Completed: {progress_state.get('step4_completed', False)}\n")
        f.write(f"  - Attempts: {progress_state.get('step4_attempts', 0)}\n")
        if progress_state.get("step4_error"):
            f.write(f"  - Error: {progress_state.get('step4_error')}\n")
        f.write("\n")

        f.write("Step 5 (11labs):\n")
        f.write(f"  - Output: {step5_output}\n")
        f.write(f"  - Completed: {progress_state.get('step5_completed', False)}\n")
        f.write(f"  - Attempts: {progress_state.get('step5_attempts', 0)}\n")
        if progress_state.get("step5_error"):
            f.write(f"  - Error: {progress_state.get('step5_error')}\n")


def main(
    input_file: str,
    output_dir: str | None = None,
    chunker_model: str = CHUNKER_DEFAULT_MODEL,
    chunker_temperature: float = CHUNKER_DEFAULT_TEMPERATURE,
    chunker_chunk_size: int = CHUNKER_DEFAULT_CHUNK_SIZE,
    entitizer_model: str = ENTITIZER_DEFAULT_MODEL,
    entitizer_temperature: float = ENTITIZER_DEFAULT_TEMPERATURE,
    orator_model: str = ORATOR_DEFAULT_MODEL,
    orator_temperature: float = ORATOR_DEFAULT_TEMPERATURE,
    tonedown_model: str = TONEDOWN_DEFAULT_MODEL,
    tonedown_temperature: float = TONEDOWN_DEFAULT_TEMPERATURE,
    verbose: bool = True,
    backup: bool = True,
    start_from_step: int = 1,
    force_restart: bool = False,
) -> None:
    """
    Run all five malmo tools (chunker, entitizer, orator, tonedown, 11labs) in sequence on a file.

    Args:
        input_file: Path to the input file
        output_dir: Output directory (created automatically if not provided)
        chunker_model: Model to use for chunker
        chunker_temperature: Temperature for chunker
        chunker_chunk_size: Chunk size for chunker
        entitizer_model: Model to use for entitizer
        entitizer_temperature: Temperature for entitizer
        orator_model: Model to use for orator
        orator_temperature: Temperature for orator
        tonedown_model: Model to use for tonedown
        tonedown_temperature: Temperature for tonedown
        verbose: Enable verbose logging
        backup: Create backups at each step
        start_from_step: Start processing from this step (1-5)
        force_restart: Ignore previous progress and start from scratch
    """
    if verbose:
        logger.level("DEBUG")

    # Validate inputs
    if start_from_step < 1 or start_from_step > 5:
        logger.error("start_from_step must be between 1 and 5")
        sys.exit(1)

    output_dir_path = Path(output_dir) if output_dir else None

    try:
        process_file(
            input_file=input_file,
            output_dir=output_dir_path,
            chunker_model=chunker_model,
            chunker_temperature=chunker_temperature,
            chunker_chunk_size=chunker_chunk_size,
            entitizer_model=entitizer_model,
            entitizer_temperature=entitizer_temperature,
            orator_model=orator_model,
            orator_temperature=orator_temperature,
            tonedown_model=tonedown_model,
            tonedown_temperature=tonedown_temperature,
            verbose=verbose,
            backup=backup,
            start_from_step=start_from_step,
            force_restart=force_restart,
        )
    except KeyboardInterrupt:
        logger.warning("Process interrupted by user")
        sys.exit(1)
    except Exception as e:
        logger.error(f"Unhandled error: {e}")
        if verbose:
            logger.exception("Detailed error information:")
        sys.exit(1)


if __name__ == "__main__":
    fire.Fire(main)
</file>

<file path="malmo_chunker.py">
#!/usr/bin/env -S uv run -s
# /// script
# dependencies = ["fire", "litellm>=1.67.2", "tiktoken", "lxml", "backoff", "python-dotenv", "loguru"]
# ///
# this_file: malmo_chunker.py

import fire
import re
import hashlib
import os
import backoff
import tiktoken
import datetime
from pathlib import Path
from lxml import etree
from litellm import completion
from litellm._logging import _turn_on_debug
from dotenv import load_dotenv
from loguru import logger
from functools import cache

# Load environment variables from .env file
load_dotenv()

# Global variables
DEFAULT_MODEL = "openrouter/openai/gpt-4.1"
FALLBACK_MODEL = "openrouter/google/gemini-2.5-pro-preview-03-25"

DEFAULT_CHUNK_SIZE = 12288
DEFAULT_TEMPERATURE = 1.0

# Check if API key is set
if not os.getenv("OPENROUTER_API_KEY"):
    logger.warning("No API key found in environment variables. API calls may fail.")


def escape_xml_chars(text: str) -> str:
    """
    Escape special XML characters to prevent parsing errors.

    Only escapes the five special XML characters while preserving UTF-8 characters.

    Args:
        text: Input text that may contain XML special characters

    Returns:
        Text with XML special characters escaped
    """
    # Replace ampersands first (otherwise you'd double-escape the other entities)
    text = text.replace("&", "&amp;")
    # Replace other special characters
    text = text.replace("<", "&lt;")
    text = text.replace(">", "&gt;")
    text = text.replace('"', "&quot;")
    text = text.replace("'", "&apos;")
    return text


@cache
def get_token_encoder():
    """Get the token encoder."""
    return tiktoken.encoding_for_model("gpt-4o")


def count_tokens(text: str) -> int:
    """
    Count the number of tokens in a text string accurately using tiktoken.

    Args:
        text: Text string to count tokens for

    Returns:
        Integer token count
    """
    encoder = get_token_encoder()
    return len(encoder.encode(text))


def replace_tok_placeholder(text: str, placeholder: str = 'tok="____"') -> str:
    """
    Replace token count placeholder with actual count.

    This function accurately counts tokens in XML elements and ensures
    the tok attribute reflects the actual token count.
    """
    if placeholder not in text:
        return text

    # Temporarily replace placeholder with a unique string unlikely to appear in text
    temp_placeholder = "-987"
    text_with_temp = text.replace(placeholder, f'tok="{temp_placeholder}"')

    # Count actual tokens
    token_count = count_tokens(text_with_temp.replace(temp_placeholder, "0"))

    # Replace temp placeholder with actual count
    return text_with_temp.replace(temp_placeholder, str(token_count))


def generate_hash(text: str) -> str:
    """Generate a 6-character base36 hash from text."""
    sha1 = hashlib.sha1(text.encode("utf-8")).hexdigest()
    # Convert to base36 (alphanumeric lowercase)
    base36 = int(sha1, 16)
    chars = "0123456789abcdefghijklmnopqrstuvwxyz"
    result = ""
    while base36 > 0:
        base36, remainder = divmod(base36, 36)
        result = chars[remainder] + result

    # Ensure we have at least 6 characters
    result = result.zfill(6)
    return result[:6]


def generate_id(prev_id: str, content: str) -> str:
    """Generate a unique ID for an item based on previous ID and content."""
    if not prev_id:
        # First item
        return f"000000-{generate_hash(content)}"
    else:
        # Subsequent items
        prev_suffix = prev_id.split("-")[-1]
        return f"{prev_suffix}-{generate_hash(content)}"


def split_into_paragraphs(text: str) -> list[str]:
    """Split text into paragraphs based on blank lines while preserving all whitespace."""
    # Remove Windows line endings
    text = text.replace("\r\n", "\n")
    # Split on blank lines (one or more newlines) and capture the delimiters
    parts = re.split(r"(\n\s*\n)", text)
    # Join each paragraph with its delimiter and keep everything, no stripping
    return ["".join(parts[i : i + 2]) for i in range(0, len(parts), 2)]


def is_heading(paragraph: str) -> bool:
    """Check if paragraph is a markdown heading or chapter heading in Polish/English."""
    # Check for markdown heading format
    if bool(re.match(r"^#{1,6}\s+", paragraph)):
        return True

    # Check for chapter headings in Polish and English
    if bool(re.search(r'(?i)^[‚Äû"]?(?:rozdzia≈Ç|chapter)\s+\w+', paragraph.strip())):
        return True

    return False


def is_blockquote(paragraph: str) -> bool:
    """Check if paragraph is a markdown blockquote."""
    return paragraph.startswith("> ")


def is_list(paragraph: str) -> bool:
    """Check if paragraph is a markdown list of at least two items."""
    # Strip trailing blank line delimiters
    text = paragraph.rstrip("\n")
    lines = text.split("\n")
    bullet_pattern = re.compile(r"^[\*\-\+]\s+")
    number_pattern = re.compile(r"^\d+\.\s+")
    # Count lines that look like list items
    bullet_count = sum(
        1
        for line in lines
        if bullet_pattern.match(line.strip()) or number_pattern.match(line.strip())
    )
    # Treat as list only if two or more items
    return bullet_count >= 2


def is_code_block(paragraph: str) -> bool:
    """Check if paragraph is a markdown code block."""
    return paragraph.startswith("```") and paragraph.endswith("```")


def is_horizontal_rule(paragraph: str) -> bool:
    """Check if paragraph is a markdown horizontal rule."""
    return bool(re.match(r"^[\*\-_]{3,}\s*$", paragraph))


def is_table(paragraph: str) -> bool:
    """Check if paragraph is a markdown table."""
    lines = paragraph.split("\n")
    if len(lines) < 2:
        return False
    # Check for pipe character and separator line
    return "|" in lines[0] and bool(re.match(r"^[\|\s\-:]+$", lines[1].strip()))


def is_html_block(paragraph: str) -> bool:
    """Check if paragraph is an HTML block."""
    return paragraph.startswith("<") and paragraph.endswith(">")


def is_image_or_figure(paragraph: str) -> bool:
    """Check if paragraph is a markdown image or figure."""
    return bool(re.match(r"!\[.*\]\(.*\)", paragraph))


def itemize_document(doc_text: str) -> list[tuple[str, str]]:
    """
    Split document into items according to rules.

    Returns:
        List of (item_text, attachment_type) where attachment_type is:
        - "following" for elements that attach to the following item
        - "preceding" for elements that attach to the preceding item
        - "normal" for regular paragraphs
    """
    paragraphs = split_into_paragraphs(doc_text)
    items = []

    for paragraph in paragraphs:
        # Skip empty paragraphs
        if not paragraph.strip():
            continue

        if is_heading(paragraph):
            # Treat chapter and markdown headings as standalone items, not attached to following text
            items.append((paragraph, "normal"))
        elif (
            is_blockquote(paragraph)
            or is_list(paragraph)
            or is_code_block(paragraph)
            or is_table(paragraph)
            or is_horizontal_rule(paragraph)
            or is_image_or_figure(paragraph)
            or is_html_block(paragraph)
        ):
            items.append((paragraph, "preceding"))
        else:
            items.append((paragraph, "normal"))

    return items


def create_item_elements(items: list[tuple[str, str]]) -> list[tuple[str, str]]:
    """
    Create XML item elements with appropriate attachments.

    Returns:
        List of (item_xml, item_id) tuples
    """
    result = []
    prev_id = ""
    buffer = []
    buffer_type = None

    # If document starts with a "following" element, create an empty item
    if items and items[0][1] == "following":
        buffer.append(items[0][0])
        buffer_type = "following"
        items = items[1:]

    for item_text, attachment_type in items:
        # Escape special characters in item_text to avoid XML parsing errors
        item_text = escape_xml_chars(item_text)

        if attachment_type == "following":
            if buffer and buffer_type != "following":
                # Finalize previous item and start a new buffer
                content = "\n\n".join(buffer)
                item_id = generate_id(prev_id, content)
                item_xml = f'<item xml:space="preserve" tok="____" id="{item_id}">\n{content}\n</item>'
                result.append((item_xml, item_id))
                prev_id = item_id
                buffer = [item_text]
                buffer_type = "following"
            else:
                # Add to existing "following" buffer
                buffer.append(item_text)
        elif attachment_type == "preceding":
            if buffer:
                # Add to existing buffer
                buffer.append(item_text)
            else:
                # Create a new buffer
                buffer = [item_text]
                buffer_type = "preceding"
        elif buffer and buffer_type == "following":
            # Add to existing "following" buffer
            buffer.append(item_text)
            # Finalize this item
            content = "\n\n".join(buffer)
            item_id = generate_id(prev_id, content)
            item_xml = f'<item xml:space="preserve" tok="____" id="{item_id}">\n{content}\n</item>'
            result.append((item_xml, item_id))
            prev_id = item_id
            buffer = []
            buffer_type = None
        elif buffer and buffer_type == "preceding":
            # Add normal text to preceding elements and finalize
            buffer.append(item_text)
            content = "\n\n".join(buffer)
            item_id = generate_id(prev_id, content)
            item_xml = f'<item xml:space="preserve" tok="____" id="{item_id}">\n{content}\n</item>'
            result.append((item_xml, item_id))
            prev_id = item_id
            buffer = []
            buffer_type = None
        else:
            # Just a normal paragraph by itself
            content = item_text
            item_id = generate_id(prev_id, content)
            item_xml = f'<item xml:space="preserve" tok="____" id="{item_id}">\n{content}\n</item>'
            result.append((item_xml, item_id))
            prev_id = item_id

    # Process any remaining buffer
    if buffer:
        content = "\n\n".join(buffer)
        item_id = generate_id(prev_id, content)
        item_xml = (
            f'<item xml:space="preserve" tok="____" id="{item_id}">\n{content}\n</item>'
        )
        result.append((item_xml, item_id))

    return result


def update_token_counts(xml_text: str) -> str:
    """
    Update all tok attributes with accurate token counts.

    Processes the XML document to replace token placeholders with
    actual token counts based on the element content.

    Args:
        xml_text: XML document with tok="____" placeholders

    Returns:
        XML document with accurate token counts
    """
    # Try parsing the XML to maintain structure
    try:
        parser = etree.XMLParser(
            remove_blank_text=False, recover=True, encoding="utf-8"
        )
        root = etree.XML(xml_text.encode("utf-8"), parser)

        # Process each element with a tok attribute in a bottom-up order
        # This ensures nested elements are counted first
        for element in reversed(list(root.xpath("//*[@tok]"))):
            # Get the string representation of this element
            element_xml = etree.tostring(
                element, encoding="utf-8", method="xml"
            ).decode("utf-8")

            # Count tokens for the element with temp placeholder
            element_text = element_xml.replace('tok="____"', 'tok="0"')
            token_count = count_tokens(element_text)

            # Update the element's tok attribute
            element.set("tok", str(token_count))

        # Convert back to string with proper formatting
        result = etree.tostring(root, encoding="utf-8", method="xml").decode("utf-8")
        return result
    except Exception as e:
        logger.warning(f"XML parsing for token counting failed: {e}")
        # Fall back to regex replacement method
        while 'tok="____"' in xml_text:
            xml_text = replace_tok_placeholder(xml_text)
        return xml_text


@backoff.on_exception(backoff.expo, Exception, max_tries=3)
def semantic_analysis(
    doc_text: str, model: str, temperature: float
) -> list[tuple[str, str]]:
    """
    Use LLM to identify semantic boundaries.

    Returns:
        List of (item_id, boundary_type) tuples
    """
    # Check if API key is available
    if not os.getenv("OPENROUTER_API_KEY"):
        logger.warning("No API key available for LLM requests")
        # Return default semantic boundaries - first item gets a unit
        items_match = re.findall(r'<item\s+[^>]*?id="([^"]*)"', doc_text)
        if items_match:
            return [(items_match[0], "unit")]
        return []

    # Calculate total document length in tokens
    doc_token_count = count_tokens(doc_text)
    logger.info(f"Document total token count: {doc_token_count}")

    # Calculate minimum number of chunks based on document size and max chunk size
    min_chunks = max(
        1,
        (doc_token_count // DEFAULT_CHUNK_SIZE)
        + (1 if doc_token_count % DEFAULT_CHUNK_SIZE > 0 else 0),
    )

    # Calculate minimum number of units (5 * minimum chunks)
    min_units = 5 * min_chunks

    logger.info(
        f"Minimum chunks needed: {min_chunks}, minimum units required: {min_units}"
    )

    # Extract all item IDs from the document for validation
    all_item_ids = re.findall(r'<item\s+[^>]*?id="([^"]*)"', doc_text)

    prompt = f"""
You are analyzing a document with semantic markup. The document has been split into items with unique IDs.
Your task is to identify meaningful semantic boundaries within this document.

Follow these strict rules:
1. Identify the beginning of chapters, scenes, and other significant semantic units
2. Mark EXACTLY {min_units} OR MORE semantic units - this is a HARD REQUIREMENT
3. Always mark the first item as a "unit" if it's not a chapter or scene
4. Chapters are typically titled with patterns like:
   - "Chapter X" (English)
   - "Rozdzia≈Ç X" (Polish)
   - X can be a number (1, 2, 3) or a word (one, two, three, pierwszy, drugi, trzeci, czwarty, piƒÖty, etc.)
5. Scenes are sections that represent a distinct setting, time, or viewpoint.
6. Units are other significant semantic divisions: sections, passages with a common theme or author or form (eg. poem vs prose). They may be separated by a line consisting only of non-letter characters like `‚óè` or `***` or similar.

This document requires AT LEAST {min_units} semantic units due to its length ({doc_token_count} tokens).
Your analysis must identify at least this minimum number or your response will be rejected.

For each item where a new semantic unit begins, return the item's ID and the type of boundary:
- "chapter" - For major divisions like chapters
- "scene" - For scene breaks within chapters
- "unit" - For other significant semantic units

Return your response as a line-by-line list in the format:
item_id: boundary_type

Here is the document:

{doc_text}
"""

    # Function to extract LLM response
    def extract_and_parse_llm_response(response) -> list[tuple[str, str]]:
        try:
            result_text = ""
            if hasattr(response, "choices") and response.choices:
                try:
                    # Try to get content from message
                    result_text = response.choices[0].message.content
                except (AttributeError, IndexError):
                    try:
                        # Try to access text attribute safely without triggering type errors
                        choice = response.choices[0]
                        if hasattr(choice, "text"):
                            result_text = choice.text
                        else:
                            # Fall back to string representation
                            result_text = str(choice)
                    except (AttributeError, IndexError):
                        # Fall back to string representation
                        result_text = str(response.choices[0])
            elif hasattr(response, "content") and response.content:
                result_text = response.content
            else:
                # Attempt to convert the entire response to string
                result_text = str(response)

            # Ensure we have a string, not None
            result_text = result_text or ""

            logger.debug(f"LLM response: {result_text}")
            logger.info(f"LLM response: {result_text}")

            # Parse the response
            semantic_boundaries = []
            for line in result_text.strip().split("\n"):
                line = line.strip()
                if not line or ":" not in line:
                    continue

                parts = line.split(":", 1)
                if len(parts) != 2:
                    continue

                item_id = parts[0].strip().lower()
                boundary_type = parts[1].strip().lower()

                # Validate boundary type
                if boundary_type not in ["chapter", "scene", "unit"]:
                    logger.warning(
                        f"Unrecognized boundary type: {boundary_type} for ID: {item_id}"
                    )
                    continue

                semantic_boundaries.append((item_id, boundary_type))

            # Deduplicate while preserving first occurrence
            seen_ids = set()
            unique_boundaries = []
            for item_id, boundary_type in semantic_boundaries:
                if item_id not in seen_ids:
                    unique_boundaries.append((item_id, boundary_type))
                    seen_ids.add(item_id)

            return unique_boundaries
        except Exception as e:
            logger.error(f"Failed to extract and parse LLM response: {e}")
            return []

    try:
        logger.info(f"Sending request to LLM model: {model}")
        response = completion(
            model=model,
            messages=[{"role": "user", "content": prompt}],
            temperature=temperature,
        )

        # Extract and parse the LLM response
        unique_boundaries = extract_and_parse_llm_response(response)

    except Exception as e:
        logger.error(f"Error in semantic analysis with model {model}: {e}")

        # Try with fallback model if different from the current model
        if model != FALLBACK_MODEL:
            try:
                logger.warning(f"Attempting to use fallback model: {FALLBACK_MODEL}")
                fallback_response = completion(
                    model=FALLBACK_MODEL,
                    messages=[{"role": "user", "content": prompt}],
                    temperature=temperature,
                )

                # Extract and parse the fallback response
                unique_boundaries = extract_and_parse_llm_response(fallback_response)

                if unique_boundaries:
                    logger.info(
                        f"Successfully analyzed with fallback model: {FALLBACK_MODEL}"
                    )
                else:
                    logger.warning("Fallback model did not return valid boundaries")
                    # Fall back to heuristic approach
                    if all_item_ids:
                        return [(all_item_ids[0], "unit")]
                    return []

            except Exception as fallback_error:
                logger.error(f"Fallback model also failed: {fallback_error}")
                # Fall back to heuristic approach
                if all_item_ids:
                    return [(all_item_ids[0], "unit")]
                return []
        else:
            # If model and fallback are the same, just fall back to heuristic approach
            if all_item_ids:
                return [(all_item_ids[0], "unit")]
            return []

    # Check if we have enough boundaries
    if len(unique_boundaries) < min_units:
        logger.warning(
            f"LLM identified only {len(unique_boundaries)} units, which is less than the required {min_units}. Will attempt to add more."
        )

        # Add additional units by choosing items at regular intervals
        available_ids = [
            item_id
            for item_id in all_item_ids
            if item_id not in {id for id, _ in unique_boundaries}
        ]

        if available_ids:
            # Calculate how many more units we need
            needed = min_units - len(unique_boundaries)

            # Choose items at regular intervals
            step = max(1, len(available_ids) // needed)
            additional_ids = [
                available_ids[i] for i in range(0, len(available_ids), step)
            ][:needed]

            # Add as units
            for item_id in additional_ids:
                unique_boundaries.append((item_id, "unit"))

            logger.info(
                f"Added {len(additional_ids)} additional units to meet minimum requirement"
            )

    # If we still don't have any boundaries, fall back to the default
    if not unique_boundaries and all_item_ids:
        return [(all_item_ids[0], "unit")]

    return unique_boundaries


def add_unit_tags(itemized_doc: str, semantic_boundaries: list[tuple[str, str]]) -> str:
    """
    Add unit tags to the document based on semantic boundaries.

    Args:
        itemized_doc: XML document with items
        semantic_boundaries: List of (item_id, boundary_type) tuples

    Returns:
        Document with unit tags added
    """
    # Convert to dict for easy lookup
    boundaries_dict = dict(semantic_boundaries)

    # Parse the document
    parser = etree.XMLParser(remove_blank_text=False, recover=True, encoding="utf-8")
    try:
        doc_root = etree.XML(itemized_doc.encode("utf-8"), parser)
    except Exception as e:
        logger.error(f"Error parsing document: {e}")
        # Fall back to regex-based approach
        return add_unit_tags_regex(itemized_doc, semantic_boundaries)

    # Get all item elements
    items = doc_root.xpath("//item")

    # If we have no semantic boundaries, use heuristics to create them
    if not semantic_boundaries:
        # Detect chapter boundaries based on text patterns
        for item in items:
            item_text = (
                etree.tostring(item, encoding="utf-8", method="text")
                .decode("utf-8")
                .strip()
            )
            # Match both numeric and word-based chapter numbers in Polish and English
            if re.search(r'(?i)^[‚Äû"]?(?:rozdzia≈Ç|chapter)\s+\w+', item_text):
                item_id = item.get("id", "")
                if item_id:
                    boundaries_dict[item_id] = "chapter"
            elif item.getprevious() is None or item.getparent().index(item) == 0:
                # First item gets a unit boundary
                item_id = item.get("id", "")
                if item_id:
                    boundaries_dict[item_id] = "unit"

    # Ensure we have at least one unit
    if not boundaries_dict and items:
        # Get first item's ID
        first_item_id = items[0].get("id", "")
        if first_item_id:
            boundaries_dict[first_item_id] = "unit"

    # Add unit tags
    current_unit = None
    item_parent_map = {child: parent for parent in doc_root.iter() for child in parent}

    # Group items into units based on boundaries
    for _i, item in enumerate(items):
        item_id = item.get("id", "")

        # Check if this item starts a new unit
        if item_id in boundaries_dict:
            # Close previous unit if exists
            if current_unit is not None:
                # Already in a unit, need to close and open new
                parent = item_parent_map[item]
                unit_idx = parent.index(item)

                # Create new unit element with proper semantic type
                new_unit = etree.Element("unit")
                new_unit.set("type", boundaries_dict[item_id])
                new_unit.set("tok", "____")  # Placeholder

                # Insert new unit before item
                parent.insert(unit_idx, new_unit)

                # Move item to new unit
                parent.remove(item)
                new_unit.append(item)

                current_unit = new_unit
            else:
                # First unit in document
                parent = item_parent_map[item]
                unit_idx = parent.index(item)

                # Create unit element
                unit = etree.Element("unit")
                unit.set("type", boundaries_dict[item_id])
                unit.set("tok", "____")  # Placeholder

                # Insert unit before item
                parent.insert(unit_idx, unit)

                # Move item to unit
                parent.remove(item)
                unit.append(item)

                current_unit = unit
        elif current_unit is not None:
            # Already in a unit, move this item to current unit
            parent = item_parent_map[item]
            parent.remove(item)
            current_unit.append(item)

    # Convert back to string
    result = etree.tostring(doc_root, encoding="utf-8", method="xml").decode("utf-8")

    # Update token counts
    result = update_token_counts(result)

    return result


def add_unit_tags_regex(
    itemized_doc: str, semantic_boundaries: list[tuple[str, str]]
) -> str:
    """
    Fallback method to add unit tags using regex.

    Args:
        itemized_doc: XML document with items
        semantic_boundaries: List of (item_id, boundary_type) tuples

    Returns:
        Document with unit tags added
    """
    # Convert to dict for easy lookup
    boundaries_dict = dict(semantic_boundaries)

    # Find all item IDs
    item_matches = list(re.finditer(r'<item tok="[^"]*" id="([^"]*)">', itemized_doc))

    if not item_matches:
        return itemized_doc

    # Process in reverse order to avoid invalidating match positions
    result = itemized_doc
    open_unit = False

    # Ensure first item has a unit if not already defined
    first_item_id = item_matches[0].group(1) if item_matches else None
    if first_item_id and first_item_id not in boundaries_dict:
        boundaries_dict[first_item_id] = "unit"

    for i in range(len(item_matches) - 1, -1, -1):
        match = item_matches[i]
        item_id = match.group(1)

        if item_id in boundaries_dict:
            # Add unit close/open tags
            if open_unit:
                # Close previous unit and open new one
                unit_tag = (
                    f'</unit>\n<unit type="{boundaries_dict[item_id]}" tok="____">\n'
                )
            else:
                # Just open a unit
                unit_tag = f'<unit type="{boundaries_dict[item_id]}" tok="____">\n'
                open_unit = True

            result = result[: match.start()] + unit_tag + result[match.start() :]

    # Close the last unit at document end
    if open_unit:
        result += "\n</unit>"

    # Ensure document starts with a unit if needed
    if not result.strip().startswith("<unit"):
        first_item_pos = item_matches[0].start() if item_matches else 0
        result = (
            result[:first_item_pos]
            + '<unit type="unit" tok="____">\n'
            + result[first_item_pos:]
        )
        result += "\n</unit>"

    # Update token counts
    result = update_token_counts(result)

    return result


def create_chunks(doc_with_units: str, max_chunk_size: int) -> str:
    """
    Create chunks based on token limit and unit boundaries.

    Args:
        doc_with_units: XML document with items and units
        max_chunk_size: Maximum chunk size in tokens

    Returns:
        Document with chunk tags added
    """
    try:
        # Parse document
        parser = etree.XMLParser(
            remove_blank_text=False, recover=True, encoding="utf-8"
        )
        doc_root = etree.XML(doc_with_units.encode("utf-8"), parser)

        # Get all unit elements
        units = doc_root.xpath("//unit")

        # Verify that all items are inside a unit
        all_items = doc_root.xpath("//item")
        items_in_units = doc_root.xpath("//unit//item")

        if len(all_items) != len(items_in_units):
            logger.warning(
                f"Found {len(all_items) - len(items_in_units)} items not in units. Adding them to a default unit."
            )
            # Find all items not in units and add them to a default unit
            for item in all_items:
                # Check if this item has a unit parent
                parent = item.getparent()
                if parent is not None and parent.tag != "unit":
                    # Create a new unit
                    default_unit = etree.Element("unit")
                    default_unit.set("type", "unit")
                    default_unit.set("tok", "____")

                    # Replace the item with the unit containing the item
                    item_idx = parent.index(item)
                    parent.remove(item)
                    default_unit.append(item)
                    parent.insert(item_idx, default_unit)

        # Get updated list of units
        units = doc_root.xpath("//unit")

        if not units:
            # No units, just wrap everything in one chunk
            chunk = etree.Element("chunk")
            chunk.set("id", "c0")
            chunk.set("tok", "____")

            # Move all direct children to the chunk
            for child in list(doc_root):
                doc_root.remove(child)
                chunk.append(child)

            doc_root.append(chunk)
        else:
            # Process units into chunks
            current_chunk = None
            current_chunk_id = 0
            current_chunk_tokens = 0

            # Find units of type "chapter" to ensure they start new chunks
            for unit in units:
                # Calculate unit tokens (use actual token count instead of placeholder)
                unit_tokens = int(unit.get("tok", "0"))
                is_chapter = unit.get("type") == "chapter"

                # Start a new chunk if:
                # 1. We don't have a current chunk
                # 2. This unit is a chapter (always starts a new chunk)
                # 3. Adding this unit would exceed the max chunk size
                if (
                    current_chunk is None
                    or is_chapter
                    or current_chunk_tokens + unit_tokens > max_chunk_size
                ):
                    # Create new chunk
                    current_chunk = etree.Element("chunk")
                    current_chunk.set("id", f"c{current_chunk_id}")
                    current_chunk.set("tok", "____")
                    current_chunk_id += 1

                    # Get parent of the unit
                    parent = unit.getparent()
                    if parent is not None:
                        # Move unit to chunk
                        idx = parent.index(unit)
                        parent.remove(unit)
                        current_chunk.append(unit)

                        # Add chunk to document at the same position
                        parent.insert(idx, current_chunk)
                    else:
                        # Unit is at root level
                        doc_root.remove(unit)
                        current_chunk.append(unit)
                        doc_root.append(current_chunk)

                    current_chunk_tokens = unit_tokens
                else:
                    # Add to current chunk
                    parent = unit.getparent()
                    if parent is not None:
                        parent.remove(unit)
                        current_chunk.append(unit)
                    current_chunk_tokens += unit_tokens

        # Convert back to string
        result = etree.tostring(doc_root, encoding="utf-8", method="xml").decode(
            "utf-8"
        )

        # Update token counts
        result = update_token_counts(result)

        return result

    except Exception as e:
        logger.error(f"Error creating chunks: {e}")
        # Fall back to regex approach
        return create_chunks_regex(doc_with_units, max_chunk_size)


def handle_oversized_unit(
    doc_root, unit, max_chunk_size: int, chunk_id_start: int
) -> None:
    """
    Handle a unit that exceeds the maximum chunk size by splitting it.

    Args:
        doc_root: XML document root
        unit: The oversized unit element
        max_chunk_size: Maximum chunk size in tokens
        chunk_id_start: Starting chunk ID number
    """
    # Get unit's parent
    parent = unit.getparent()
    if parent is None:
        parent = doc_root

    # Get unit's position
    unit_pos = parent.index(unit) if unit in parent else -1

    # Remove unit from parent
    if unit_pos >= 0:
        parent.remove(unit)

    # Get items in unit
    items = list(unit.xpath(".//item"))
    unit_type = unit.get("type", "unit")

    # Create chunks with split units
    current_chunk = None
    current_unit = None
    current_chunk_id = chunk_id_start
    current_unit_id = 0
    current_chunk_tokens = 0
    current_unit_tokens = 0

    for item in items:
        # Get item tokens
        item_tokens = int(item.get("tok", "0"))

        # Check if adding this item would exceed limits
        if current_unit is None:
            # Create new unit and chunk
            current_chunk = etree.Element("chunk")
            current_chunk.set("id", f"c{current_chunk_id}")
            current_chunk.set("tok", "____")

            current_unit = etree.Element("unit")
            current_unit.set("type", unit_type)
            current_unit.set("tok", "____")

            if unit.get("id"):
                current_unit.set("id", f"{unit.get('id')}-split-{current_unit_id}")

            current_chunk.append(current_unit)
            if unit_pos >= 0:
                parent.insert(
                    unit_pos + current_chunk_id - chunk_id_start, current_chunk
                )
            else:
                parent.append(current_chunk)

            current_chunk_tokens = 0
            current_unit_tokens = 0
            current_chunk_id += 1
            current_unit_id += 1

        # Remove item from original unit
        item_parent = item.getparent()
        if item_parent is not None:
            item_parent.remove(item)

        # Add item to current unit
        current_unit.append(item)
        current_unit_tokens += item_tokens
        current_chunk_tokens += item_tokens

        # Check if we need a new chunk/unit after adding this item
        if current_chunk_tokens >= max_chunk_size:
            # This chunk is full, create a new one
            current_chunk = None
            current_unit = None


def create_chunks_regex(doc_with_units: str, max_chunk_size: int) -> str:
    """
    Fallback method to create chunks using regex.

    Args:
        doc_with_units: XML document with items and units
        max_chunk_size: Maximum chunk size in tokens

    Returns:
        Document with chunk tags added
    """
    # Find all units with their token counts
    unit_matches = list(
        re.finditer(
            r'<unit type="([^"]*)" tok="(\d+)"[^>]*>(.*?)</unit>',
            doc_with_units,
            re.DOTALL,
        )
    )

    if not unit_matches:
        # No units found, wrap everything in one chunk
        result = (
            f'<doc>\n<chunk id="c0" tok="____">\n{doc_with_units}\n</chunk>\n</doc>'
        )
        return update_token_counts(result)

    # Extract document start/end
    doc_start_match = re.match(r"^(.*?)<unit", doc_with_units, re.DOTALL)
    doc_start = doc_start_match.group(1) if doc_start_match else "<doc>\n"

    doc_end_match = re.search(r"</unit>(.*?)$", doc_with_units, re.DOTALL)
    doc_end = doc_end_match.group(1) if doc_end_match else "\n</doc>"

    # Process units into chunks
    chunks = []
    current_chunk: list[str] = []
    current_chunk_id = 0
    current_chunk_tokens = 0

    for match in unit_matches:
        unit_type = match.group(1)
        unit_tokens = int(match.group(2))
        unit_content = match.group(3)

        if unit_tokens > max_chunk_size:
            # Handle oversized unit by splitting
            split_chunks = split_oversized_unit_regex(
                unit_type, unit_content, max_chunk_size, current_chunk_id
            )
            chunks.extend(split_chunks)
            current_chunk_id += len(split_chunks)
        elif current_chunk_tokens + unit_tokens > max_chunk_size:
            # Finalize current chunk and start a new one
            if current_chunk:
                chunk_xml = f'<chunk id="c{current_chunk_id}" tok="____">\n{"".join(current_chunk)}\n</chunk>'
                chunks.append(chunk_xml)
                current_chunk_id += 1
                current_chunk = []
                current_chunk_tokens = 0

            # Add unit to new chunk
            current_chunk.append(
                f'<unit type="{unit_type}" tok="{unit_tokens}">{unit_content}</unit>'
            )
            current_chunk_tokens += unit_tokens
        else:
            # Add to current chunk
            current_chunk.append(
                f'<unit type="{unit_type}" tok="{unit_tokens}">{unit_content}</unit>'
            )
            current_chunk_tokens += unit_tokens

    # Add remaining chunk
    if current_chunk:
        chunk_xml = f'<chunk id="c{current_chunk_id}" tok="____">\n{"".join(current_chunk)}\n</chunk>'
        chunks.append(chunk_xml)

    # Combine everything
    result = f"{doc_start}{''.join(chunks)}{doc_end}"

    # Update token counts
    return update_token_counts(result)


def split_oversized_unit_regex(
    unit_type: str, unit_content: str, max_chunk_size: int, chunk_id_start: int
) -> list[str]:
    """
    Split an oversized unit into multiple chunks.

    Args:
        unit_type: Type of the unit
        unit_content: Content of the unit
        max_chunk_size: Maximum chunk size in tokens
        chunk_id_start: Starting chunk ID

    Returns:
        List of chunk XML strings
    """
    # Find all items in the unit
    item_matches = list(
        re.finditer(
            r'<item tok="(\d+)" id="([^"]*)">(.*?)</item>', unit_content, re.DOTALL
        )
    )

    if not item_matches:
        # No items, just return the whole unit in one chunk
        return [
            f'<chunk id="c{chunk_id_start}" tok="____">\n<unit type="{unit_type}" tok="____">{unit_content}</unit>\n</chunk>'
        ]

    # Split into chunks
    chunks = []
    current_chunk_items: list[str] = []
    current_chunk_id = chunk_id_start
    current_unit_id = 0
    current_chunk_tokens = 0

    for match in item_matches:
        item_tokens = int(match.group(1))
        item_id = match.group(2)
        item_content = match.group(3)

        # Check if adding this item would exceed chunk size
        if current_chunk_tokens + item_tokens > max_chunk_size and current_chunk_items:
            # Finalize current chunk
            items_xml = "".join(current_chunk_items)
            chunk_xml = (
                f'<chunk id="c{current_chunk_id}" tok="____">\n'
                f'<unit type="{unit_type}" id="{item_id}-split-{current_unit_id}" tok="____">'
                f"{items_xml}</unit>\n</chunk>"
            )
            chunks.append(chunk_xml)

            # Reset for next chunk
            current_chunk_id += 1
            current_unit_id += 1
            current_chunk_items = []
            current_chunk_tokens = 0

        # Add item to current chunk
        current_chunk_items.append(
            f'<item xml:space="preserve" tok="{item_tokens}" id="{item_id}">{item_content}</item>'
        )
        current_chunk_tokens += item_tokens

    # Add remaining items
    if current_chunk_items:
        items_xml = "".join(current_chunk_items)
        chunk_xml = (
            f'<chunk id="c{current_chunk_id}" tok="____">\n'
            f'<unit type="{unit_type}" id="split-{current_unit_id}" tok="____">'
            f"{items_xml}</unit>\n</chunk>"
        )
        chunks.append(chunk_xml)

    return chunks


def clean_consecutive_newlines(content: str) -> str:
    """
    Clean up content by removing more than 2 consecutive newlines.

    Args:
        content: The XML document content

    Returns:
        The content with excessive newlines removed
    """
    # First, normalize all newlines to \n (convert Windows \r\n to \n)
    content = content.replace("\r\n", "\n")

    # Replace 3 or more consecutive newlines with just 2 newlines
    return re.sub(r"\n{3,}", "\n\n", content)


def pretty_print_xml(xml_str: str) -> str:
    """
    Return XML as is to preserve original whitespace and formatting.
    No pretty printing is performed to ensure exact whitespace preservation.
    """
    return xml_str


def process_document(
    input_file: str,
    output_file: str,
    chunk_size: int = DEFAULT_CHUNK_SIZE,
    model: str = DEFAULT_MODEL,
    temperature: float = DEFAULT_TEMPERATURE,
    encoding: str = "utf-8",
    verbose: bool = False,
    backup: bool = False,
) -> None:
    """
    Process a markdown document into a semantically chunked XML document.

    Args:
        input_file: Path to the input markdown file
        output_file: Path for the XML output
        chunk_size: Maximum chunk size in tokens
        model: LLM model identifier
        temperature: LLM temperature setting
        encoding: File encoding
        verbose: Enable detailed logging
        backup: Whether to create backup copies with timestamps
    """
    if verbose:
        logger.level("DEBUG")

    try:
        # Read input file
        logger.info(f"Reading input file: {input_file}")
        with open(input_file, encoding=encoding) as f:
            input_text = f.read()

        # Wrap in doc tag
        logger.debug("Wrapping document")

        # Itemize document
        logger.info("Itemizing document")
        items = itemize_document(input_text)
        item_elements = create_item_elements(items)
        itemized_doc = f"<doc>\n{''.join([item[0] for item in item_elements])}\n</doc>"

        # Update token counts for items
        logger.debug("Updating token counts")
        itemized_doc = update_token_counts(itemized_doc)

        # Create backup of initial itemized document if requested
        if backup:
            save_backup(output_file, itemized_doc.encode(encoding), "itemized")

        # Semantic analysis
        logger.info(f"Performing semantic analysis with model: {model}")
        try:
            semantic_boundaries = semantic_analysis(itemized_doc, model, temperature)
            logger.debug(f"Identified semantic boundaries: {semantic_boundaries}")

            # Create backup after semantic analysis if requested
            if backup:
                itemized_doc_with_boundaries = (
                    itemized_doc  # We don't modify the document yet
                )
                save_backup(
                    output_file,
                    itemized_doc_with_boundaries.encode(encoding),
                    "semantic_boundaries",
                )
        except Exception as e:
            logger.error(f"Semantic analysis failed: {e}")
            logger.warning("Using a single unit for the whole document")
            # Get first item ID
            first_item_id = item_elements[0][1] if item_elements else None
            semantic_boundaries = [(first_item_id, "unit")] if first_item_id else []

        # Add unit tags
        logger.info("Adding unit tags")
        doc_with_units = add_unit_tags(itemized_doc, semantic_boundaries)

        # Create backup after adding unit tags if requested
        if backup:
            save_backup(output_file, doc_with_units.encode(encoding), "units_added")

        # Create chunks
        logger.info(f"Creating chunks with max size: {chunk_size} tokens")
        chunked_doc = create_chunks(doc_with_units, chunk_size)

        # Create backup after creating chunks if requested
        if backup:
            save_backup(output_file, chunked_doc.encode(encoding), "chunks_created")

        # Format as nice XML
        logger.debug("Formatting XML")
        final_xml = pretty_print_xml(chunked_doc)

        # Clean up consecutive newlines
        logger.debug("Cleaning up consecutive newlines")
        final_xml = clean_consecutive_newlines(final_xml)

        # Write output file
        logger.info(f"Writing output to: {output_file}")
        with open(output_file, "wb") as f:
            f.write(final_xml.encode(encoding))

        # Create final backup if requested
        if backup:
            save_backup(output_file, final_xml.encode(encoding), "final")

        logger.success("Processing complete!")

    except Exception as e:
        logger.error(f"Error processing document: {e}")
        raise


def save_backup(output_file: str, content: bytes, stage: str) -> None:
    """
    Save a backup of the current document state with a timestamp.

    Args:
        output_file: Path to the output file
        content: Document content as bytes
        stage: Processing stage name
    """
    try:
        output_path = Path(output_file)
        backup_dir = output_path.parent / f"{output_path.stem}_backups"
        backup_dir.mkdir(exist_ok=True)

        # Generate timestamp
        timestamp = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")

        # Create backup filename with stage and timestamp
        backup_filename = f"{output_path.stem}-{timestamp}-{stage}{output_path.suffix}"
        backup_path = backup_dir / backup_filename

        # Save backup
        with open(backup_path, "wb") as f:
            f.write(content)

        logger.debug(f"Created backup for {stage} stage at: {backup_path}")
    except Exception as e:
        logger.error(f"Error creating backup: {e}")


def main(
    input_file: str,
    output_file: str,
    chunk: int = DEFAULT_CHUNK_SIZE,
    model: str = DEFAULT_MODEL,
    temperature: float = DEFAULT_TEMPERATURE,
    encoding: str = "utf-8",
    verbose: bool = True,
    backup: bool = True,
) -> None:
    """
    Main entry point for malmo_chunker.

    Args:
        input_file: Path to the input markdown file
        output_file: Path for the XML output
        chunk: Maximum chunk size in tokens
        model: LLM model identifier
        temperature: LLM temperature setting
        encoding: File encoding
        verbose: Enable detailed logging
        backup: Whether to create backup copies with timestamps
    """
    if verbose:
        logger.level("DEBUG")
        _turn_on_debug()

    process_document(
        input_file, output_file, chunk, model, temperature, encoding, verbose, backup
    )


if __name__ == "__main__":
    fire.Fire(main)
</file>

<file path="malmo_entitizer.py">
#!/usr/bin/env -S uv run -s
# /// script
# dependencies = ["fire", "litellm<=1.67.2", "lxml", "backoff", "python-dotenv", "loguru"]
# ///
# this_file: malmo_entitizer.py

import fire
import re
import os
import json
import backoff
import concurrent.futures
from datetime import datetime
from pathlib import Path
from lxml import etree
from litellm import completion
from litellm._logging import _turn_on_debug
from dotenv import load_dotenv
from loguru import logger

# Load environment variables from .env file
load_dotenv()

# Global variables
DEFAULT_MODEL = "openrouter/openai/gpt-4.1"
FALLBACK_MODEL = "openrouter/google/gemini-2.5-pro-preview-03-25"
DEFAULT_TEMPERATURE = 1.0

# Dictionary mapping model keys to their actual model identifiers
MODEL_DICT = {
    "geminipro": "openrouter/google/gemini-2.5-pro-preview-03-25",
    "claudeopus": "openrouter/anthropic/claude-3-opus",
    "gpt41": "openrouter/openai/gpt-4.1",
    "gpt41mini": "openrouter/openai/gpt-4.1-mini",
    "r1chimera": "openrouter/tngtech/deepseek-r1t-chimera:free",
    "msr1": "openrouter/microsoft/mai-ds-r1:free",
    "geminiflash": "openrouter/google/gemini-2.5-flash-preview",
    "gpt41nano": "openrouter/openai/gpt-4.1-nano",
    "llama4mav": "openrouter/meta-llama/llama-4-maverick:free",
    "llama4sc": "openrouter/meta-llama/llama-4-scout:free",
    "claudesonnet": "openrouter/anthropic/claude-3.7-sonnet",
    "nemotron": "openrouter/nvidia/llama-3.1-nemotron-ultra-253b-v1:free",
    "qwenturbo": "openrouter/qwen/qwen-turbo",
    "geminiflash2": "openrouter/google/gemini-2.0-flash-001",
    "r1": "openrouter/deepseek/deepseek-r1:free",
    "deepseek3": "openrouter/deepseek/deepseek-chat:free",
    "geminiflash2e": "openrouter/google/gemini-2.0-flash-exp:free",
    "geminiflash1": "openrouter/google/gemini-flash-1.5-8b",
}

# Check if API key is set
if not os.getenv("OPENROUTER_API_KEY"):
    logger.warning("No API key found in environment variables. API calls may fail.")


def parse_xml(input_file: str) -> etree._Element:
    """
    Parse the input XML file.

    Args:
        input_file: Path to the input XML file

    Returns:
        Parsed XML element tree
    """
    try:
        parser = etree.XMLParser(remove_blank_text=False, recover=True)
        with open(input_file, encoding="utf-8") as f:
            xml_content = f.read()
        return etree.fromstring(xml_content.encode("utf-8"), parser)
    except Exception as e:
        logger.error(f"Error parsing XML: {e}")
        raise


def get_chunks(root: etree._Element) -> list[etree._Element]:
    """
    Extract chunks from the XML document.

    Args:
        root: Root XML element

    Returns:
        List of chunk elements
    """
    return root.xpath("//chunk")


def get_chunk_text(chunk: etree._Element) -> str:
    """
    Extract the raw text content of a chunk while preserving whitespace.

    Args:
        chunk: XML chunk element

    Returns:
        Raw text content of the chunk with all whitespace preserved
    """
    # Using etree.tostring to preserve all whitespace in the XML structure
    chunk_xml = etree.tostring(chunk, encoding="utf-8", method="xml").decode("utf-8")
    return chunk_xml


def save_current_state(
    root: etree._Element,
    nei_dict: dict[str, str],
    output_file: str,
    chunk_id: str,
    backup: bool = False,
) -> None:
    """
    Save current state to the output file after processing a chunk.

    Args:
        root: Current XML root element
        nei_dict: Current NEI dictionary
        output_file: File to save output
        chunk_id: ID of the chunk that was processed
        backup: Whether to create backup copies with timestamps
    """
    try:
        # Save current XML state
        with open(output_file, "wb") as f:
            xml_bytes = etree.tostring(
                root,
                encoding="utf-8",
                method="xml",
                pretty_print=False,
                xml_declaration=True,
            )
            f.write(xml_bytes)

        # Save current NEI dictionary to a JSON file next to the output file
        nei_path = f"{os.path.splitext(output_file)[0]}_nei_dict.json"
        with open(nei_path, "w", encoding="utf-8") as f:
            json.dump(nei_dict, f, ensure_ascii=False, indent=2)

        # Create backup if requested
        if backup:
            output_path = Path(output_file)
            backup_dir = output_path.parent / f"{output_path.stem}_backups"
            backup_dir.mkdir(exist_ok=True)

            # Generate timestamp
            timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")

            # Backup XML file
            backup_filename = f"{output_path.stem}-{timestamp}{output_path.suffix}"
            backup_path = backup_dir / backup_filename
            with open(backup_path, "wb") as f:
                f.write(xml_bytes)

            # Backup NEI dictionary
            nei_backup_filename = f"{output_path.stem}-{timestamp}_nei_dict.json"
            nei_backup_path = backup_dir / nei_backup_filename
            with open(nei_backup_path, "w", encoding="utf-8") as f:
                json.dump(nei_dict, f, ensure_ascii=False, indent=2)

            logger.debug(f"Created backups at: {backup_path} and {nei_backup_path}")

        logger.info(f"Saved current state after processing chunk {chunk_id}")
        logger.debug(f"Files saved to {output_file} and {nei_path}")

    except Exception as e:
        logger.error(f"Error saving current state: {e}")


def extract_item_elements(xml_text: str) -> dict[str, str]:
    """
    Extract item elements from XML text and map them by ID.

    Args:
        xml_text: XML text containing item elements

    Returns:
        Dictionary mapping item IDs to their full XML content
    """
    item_map = {}
    # Pattern to match complete <item> elements including their attributes and content
    pattern = r"<item\s+([^>]*)>(.*?)</item>"

    # Find all item elements
    for match in re.finditer(pattern, xml_text, re.DOTALL):
        # Extract attributes and content
        attributes = match.group(1)
        content = match.group(2)

        # Extract ID attribute
        id_match = re.search(r'id="([^"]*)"', attributes)
        if id_match:
            item_id = id_match.group(1)
            # Store the complete item element
            item_map[item_id] = f"<item {attributes}>{content}</item>"

    return item_map


def merge_tagged_items(
    original_chunk: etree._Element, tagged_items: dict[str, str]
) -> etree._Element:
    """
    Merge tagged items back into the original chunk structure.

    Args:
        original_chunk: Original chunk element
        tagged_items: Dictionary mapping item IDs to their tagged XML content

    Returns:
        Updated chunk element with tagged items merged
    """
    # Clone the original chunk to avoid modifying it directly
    parser = etree.XMLParser(remove_blank_text=False, recover=True)
    updated_chunk = etree.fromstring(etree.tostring(original_chunk), parser)

    # Find all item elements in the updated chunk
    for item in updated_chunk.xpath(".//item"):
        item_id = item.get("id")
        if item_id and item_id in tagged_items:
            # Parse the tagged item
            try:
                parser = etree.XMLParser(remove_blank_text=False, recover=True)
                tagged_item = etree.fromstring(
                    tagged_items[item_id].encode("utf-8"), parser
                )

                # Replace the original item with the tagged one
                parent = item.getparent()
                if parent is not None:
                    item_idx = parent.index(item)
                    parent[item_idx] = tagged_item
            except Exception as e:
                logger.error(f"Error replacing item {item_id}: {e}")
                # If parsing fails, keep the original item

    return updated_chunk


@backoff.on_exception(backoff.expo, Exception, max_tries=3)
def identify_entities(
    chunk_text: str, nei_list: dict[str, str], model: str, temperature: float
) -> str:
    """
    Use LLM to identify and tag named entities of interest (NEIs) in the chunk.

    Args:
        chunk_text: Raw text of the chunk
        nei_list: Dictionary mapping NEIs to their pronunciations
        model: LLM model identifier
        temperature: LLM temperature setting

    Returns:
        Chunk text with NEIs tagged
    """
    # Check if API key is available
    if not os.getenv("OPENROUTER_API_KEY"):
        logger.warning("No API key available for LLM requests")
        return chunk_text

    # Construct formatted NEI list for the prompt
    nei_list_str = (
        ", ".join(
            [
                f'"{nei}" (pronunciation: "{pronunciation}")'
                for nei, pronunciation in nei_list.items()
            ]
        )
        if nei_list
        else "none yet"
    )

    # Log the current state of the NEI list before LLM call
    logger.info(f"Current NEI dictionary size before LLM call: {len(nei_list)} entries")
    if nei_list:
        recent_neis = list(nei_list.items())[-min(5, len(nei_list)) :]
        logger.info(
            f"Recent NEIs: {', '.join([f'{nei} ({pron})' for nei, pron in recent_neis])}"
        )

    prompt = f"""
You are a "named entity of interest" recognizer for a document processing system. You need to identify and tag "named entities of interest" (NEIs) in an XML document.

RULES:

1. First, identify the predominant language of the text.

2. Analyze the domain of the text to determine what KINDS of named entities are "of interest" to the domain. Typically your NEIs will be people, locations, organizations, abbreviations, but for example in a biological text, the NEIs will be species, genes, proteins, etc.

3. Go item by item through the text. If you come across a NEI:

a. Mark EACH occurrence of the NEI (not just the first one) with a <nei> XML tag.
b. If the NEI only consists of common words from the predominant language, just surround the NEI with the <nei> tag.
c. Otherwise use the notation `<nei orig="ORIGINAL">PRONUNCIATION</nei>`, where `ORIGINAL` is the actual original spelling of the NEI as it appears in the text, and `PRONUNCIATION` is the pronunciation of the NEI approximated into the predominant language.
d. Build the approximated pronunciation as follows: Use the orthography of the predominant language. If the NEI has letter combinations that are pronounced differently in the predominant language, split syllables with a hyphen.
e. Write out abbreviations. Spell out acronyms.
f. If the predominant language uses "i" to indicate palatalization (like Polish), transcribe words like English "sea" as "sji" and not "si".

> For example, if the NEI is "Jos√© Scaglione" in a Polish text, the tag would be <nei orig="Jos√© Scaglione">Hose Skaljone</nei>
> If the NEI is "New York Knicks" in a Polish text, the tag would be <nei orig="New York Knicks">Nju Jork Niks</nei>
> If the NEI is "CIA" in a Polish text, the tag would be <nei orig="ONZ">Sji-Aj-Ej</nei>
> If the NEI is "Honolulu" in a Polish text, the tag would be <nei orig="Honolulu">Honolulu</nei> ‚Äî because that‚Äôs an established name in Polish.
> If the NEI is being mentioned for the first time (not in the NEI list), add the attribute new="true": <nei orig="NEI" new="true">PRONUNCIATION</nei>

4. Preserve ALL whitespace, line breaks, pre-existing tags, and formatting EXACTLY as in the original item!

5. Only print the items that contain a NEI that you've marked with an <nei> tag.

Here is the dictionary of previously found NEIs with their pronunciations. If you come across any of them in the text, transcribe their pronunciation in a consistent way. And remember: if a NEI is not in this dictionary, add the `new="true"` attribute to the <nei> tag!

{nei_list_str}

Here is the XML chunk to process:

{chunk_text}
"""

    try:
        logger.info(f"Sending request to LLM model: {model}")
        response = completion(
            model=model,
            messages=[{"role": "user", "content": prompt}],
            temperature=temperature,
        )

        # Extract the content from the response
        result_text = extract_response_text(response)
        logger.debug(f"LLM response length: {len(result_text)}")

        return result_text
    except Exception as e:
        logger.error(f"Error in entity identification: {e}")

        # Try fallback model if different from current model
        if model != FALLBACK_MODEL:
            try:
                logger.warning(f"Attempting to use fallback model: {FALLBACK_MODEL}")
                fallback_response = completion(
                    model=FALLBACK_MODEL,
                    messages=[{"role": "user", "content": prompt}],
                    temperature=temperature,
                )

                # Extract content from fallback response
                fallback_result = extract_response_text(fallback_response)

                if fallback_result:
                    logger.info(
                        f"Successfully processed with fallback model: {FALLBACK_MODEL}"
                    )
                    return fallback_result
            except Exception as fallback_error:
                logger.error(f"Fallback model also failed: {fallback_error}")

        # Return the original text if both primary and fallback fail
        return chunk_text


def extract_response_text(response) -> str:
    """
    Extract text content from an LLM response safely handling different response structures.

    Args:
        response: The LLM response object

    Returns:
        The extracted text content as a string
    """
    result_text = ""
    try:
        if hasattr(response, "choices") and response.choices:
            try:
                result_text = response.choices[0].message.content
            except (AttributeError, IndexError):
                try:
                    choice = response.choices[0]
                    if hasattr(choice, "text"):
                        result_text = choice.text
                    else:
                        result_text = str(choice)
                except (AttributeError, IndexError):
                    result_text = str(response.choices[0])
        elif hasattr(response, "content") and response.content:
            result_text = response.content
        else:
            result_text = str(response)

        # Ensure we have a string, not None
        return result_text or ""
    except Exception:
        # Fallback to string representation if all else fails
        return str(response)


def extract_nei_from_tag(text: str) -> dict[str, str]:
    """
    Extract NEIs and their pronunciations from nei tags in the text.

    Args:
        text: Text with nei tags

    Returns:
        Dictionary mapping NEIs to their pronunciations
    """
    nei_dict = {}
    pattern = r'<nei orig="([^"]+)"(?:\s+new="true")?>(.*?)</nei>'
    matches = re.finditer(pattern, text)

    for match in matches:
        nei = match.group(1).strip()
        pronunciation = match.group(2).strip()
        if nei and pronunciation:
            nei_dict[nei] = pronunciation

    return nei_dict


def process_chunks(
    chunks: list[etree._Element],
    model: str,
    temperature: float,
    output_file: str = "",
    backup: bool = False,
) -> tuple[etree._Element, dict[str, str]]:
    """
    Process chunks to identify and tag NEIs.

    Args:
        chunks: List of XML chunk elements
        model: LLM model identifier
        temperature: LLM temperature setting
        output_file: File to save intermediate outputs
        backup: Whether to create backup copies with timestamps

    Returns:
        Tuple of (root element with tagged NEIs, dictionary of all NEIs found with their pronunciations)
    """
    nei_list: dict[str, str] = {}

    # Log whether we'll be saving intermediate results
    if output_file:
        logger.info(f"Will save progress after each chunk to {output_file}")

    for i, chunk in enumerate(chunks):
        chunk_id = chunk.get("id", f"unknown_{i}")
        logger.info(f"Processing chunk {chunk_id} ({i + 1}/{len(chunks)})")
        logger.info(f"Current NEI dictionary has {len(nei_list)} entries")

        # Get the chunk text
        chunk_text = get_chunk_text(chunk)

        # Identify NEIs in the chunk
        tagged_chunk_text = identify_entities(chunk_text, nei_list, model, temperature)

        # Extract newly found NEIs to add to the nei list
        new_neis = extract_nei_from_tag(tagged_chunk_text)

        # Log the new NEIs found
        if new_neis:
            logger.info(f"Found {len(new_neis)} new NEIs in chunk {chunk_id}")
            for nei, pronunciation in new_neis.items():
                logger.info(f"  - NEI: '{nei}', Pronunciation: '{pronunciation}'")

            # Update the overall NEI list
            original_size = len(nei_list)
            nei_list.update(new_neis)
            logger.info(
                f"NEI dictionary updated: {original_size} ‚Üí {len(nei_list)} entries"
            )

            # Extract tagged items from the LLM response
            tagged_items = extract_item_elements(tagged_chunk_text)

            if tagged_items:
                logger.debug(f"Found {len(tagged_items)} tagged items")

                # Merge tagged items into the original chunk
                updated_chunk = merge_tagged_items(chunk, tagged_items)

                # Replace the original chunk with the updated one
                parent = chunk.getparent()
                if parent is not None:
                    chunk_idx = parent.index(chunk)
                    parent[chunk_idx] = updated_chunk
        else:
            logger.info(f"No new NEIs found in chunk {chunk_id}")

        # Save current state after processing this chunk
        if output_file:
            save_current_state(
                chunks[0].getroottree().getroot(),
                nei_list,
                output_file,
                chunk_id,
                backup,
            )

    # Log the final NEI dictionary statistics
    logger.info(f"Final NEI dictionary contains {len(nei_list)} entries")

    return chunks[0].getroottree().getroot(), nei_list


def process_document(
    input_file: str,
    output_file: str,
    model: str = DEFAULT_MODEL,
    temperature: float = DEFAULT_TEMPERATURE,
    verbose: bool = False,
    save_incremental: bool = True,
    backup: bool = False,
) -> None:
    """
    Process an XML document to identify and tag named entities of interest.

    Args:
        input_file: Path to the input XML file
        output_file: Path for the XML output
        model: LLM model identifier
        temperature: LLM temperature setting
        verbose: Enable detailed logging
        save_incremental: Save incremental progress after each chunk
        backup: Whether to create backup copies with timestamps
    """
    if verbose:
        logger.level("DEBUG")

    try:
        # Parse input XML
        logger.info(f"Reading input file: {input_file}")
        root = parse_xml(input_file)

        # Get chunks
        chunks = get_chunks(root)
        logger.info(f"Found {len(chunks)} chunks")

        # Process chunks
        logger.info(f"Processing chunks with model: {model}")

        # If save_incremental is true, pass the output file to process_chunks
        chunk_output_file = output_file if save_incremental else ""
        root, nei_dict = process_chunks(
            chunks, model, temperature, chunk_output_file, backup
        )

        # Save final NEI dictionary
        nei_path = f"{os.path.splitext(output_file)[0]}_nei_dict.json"
        with open(nei_path, "w", encoding="utf-8") as f:
            json.dump(nei_dict, f, ensure_ascii=False, indent=2)
        logger.info(f"Final NEI dictionary saved to {nei_path}")

        # Create final backup if requested
        if backup:
            output_path = Path(output_file)
            backup_dir = output_path.parent / f"{output_path.stem}_backups"
            backup_dir.mkdir(exist_ok=True)

            # Generate timestamp
            timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")

            # Backup XML file
            backup_filename = (
                f"{output_path.stem}-{timestamp}-final{output_path.suffix}"
            )
            backup_path = backup_dir / backup_filename

            # Use tostring with xml_declaration to get a proper XML document
            xml_bytes = etree.tostring(
                root,
                encoding="utf-8",
                method="xml",
                pretty_print=False,  # Preserve exact whitespace
                xml_declaration=True,
            )

            with open(backup_path, "wb") as f:
                f.write(xml_bytes)

            # Backup NEI dictionary
            nei_backup_filename = f"{output_path.stem}-{timestamp}-final_nei_dict.json"
            nei_backup_path = backup_dir / nei_backup_filename
            with open(nei_backup_path, "w", encoding="utf-8") as f:
                json.dump(nei_dict, f, ensure_ascii=False, indent=2)

            logger.debug(
                f"Created final backups at: {backup_path} and {nei_backup_path}"
            )

        # Write output file
        logger.info(f"Writing final output to: {output_file}")
        with open(output_file, "wb") as f:
            # Use tostring with xml_declaration to get a proper XML document
            xml_bytes = etree.tostring(
                root,
                encoding="utf-8",
                method="xml",
                pretty_print=False,  # Preserve exact whitespace
                xml_declaration=True,
            )
            f.write(xml_bytes)

        logger.success(f"Processing complete! Tagged {len(nei_dict)} NEIs.")

    except Exception as e:
        logger.error(f"Error processing document: {e}")
        raise


def process_document_with_model(
    input_file: str,
    output_file: str,
    model: str,
    model_key: str = "",
    temperature: float = DEFAULT_TEMPERATURE,
    verbose: bool = False,
    save_incremental: bool = True,
    max_retries: int = 3,
    backup: bool = False,
) -> bool:
    """
    Process an XML document with a specific model, with retry logic for failures.

    Args:
        input_file: Path to the input XML file
        output_file: Path for the XML output
        model: LLM model identifier
        model_key: Key name for the model (used in logging)
        temperature: LLM temperature setting
        verbose: Enable detailed logging
        save_incremental: Save incremental progress after each chunk
        max_retries: Maximum number of retries before giving up
        backup: Whether to create backup copies with timestamps

    Returns:
        bool: True if processing succeeded, False if it failed after retries
    """
    model_display = f"{model_key} ({model})" if model_key else model

    # Track partial output files to clean up on failure
    partial_files = []
    output_path = Path(output_file)
    nei_path = output_path.with_name(f"{output_path.stem}_nei_dict.json")

    for attempt in range(1, max_retries + 1):
        try:
            logger.info(
                f"Processing with model {model_display}, attempt {attempt}/{max_retries}"
            )

            # Process document with the specified model
            process_document(
                input_file,
                output_file,
                model,
                temperature,
                verbose,
                save_incremental,
                backup,
            )

            logger.success(f"Successfully processed with model {model_display}")
            return True

        except Exception as e:
            # Clean up partial output files if they exist
            if os.path.exists(output_file):
                partial_files.append(output_file)

            if os.path.exists(nei_path):
                partial_files.append(str(nei_path))

            # Also check for and add backup directory to cleanup list if it exists
            backup_dir = output_path.parent / f"{output_path.stem}_backups"
            if backup and backup_dir.exists():
                partial_files.append(str(backup_dir))

            if attempt < max_retries:
                logger.warning(
                    f"Attempt {attempt} failed with model {model_display}: {e}. Retrying..."
                )
            else:
                logger.error(
                    f"All {max_retries} attempts failed with model {model_display}. Skipping this model."
                )

                # Clean up any partial output files on final failure
                for file_path in set(partial_files):
                    try:
                        if os.path.exists(file_path):
                            if os.path.isdir(file_path):
                                import shutil

                                shutil.rmtree(file_path)
                                logger.debug(
                                    f"Cleaned up backup directory: {file_path}"
                                )
                            else:
                                os.remove(file_path)
                                logger.debug(
                                    f"Cleaned up partial output file: {file_path}"
                                )
                    except Exception as cleanup_err:
                        logger.warning(
                            f"Failed to clean up file/directory {file_path}: {cleanup_err}"
                        )

                return False


def process_document_with_all_models(
    input_file: str,
    output_folder: str,
    temperature: float = DEFAULT_TEMPERATURE,
    verbose: bool = False,
    save_incremental: bool = True,
    max_workers: int = 4,
    backup: bool = False,
) -> None:
    """
    Process an XML document with all models in parallel.

    Args:
        input_file: Path to the input XML file
        output_folder: Folder to save output files
        temperature: LLM temperature setting
        verbose: Enable detailed logging
        save_incremental: Save incremental progress after each chunk
        max_workers: Maximum number of worker threads
        backup: Whether to create backup copies with timestamps
    """
    # Create output folder if it doesn't exist
    os.makedirs(output_folder, exist_ok=True)
    logger.info(f"Created output folder: {output_folder}")

    # Get input file basename without extension
    input_basename = os.path.basename(input_file)
    input_name_without_ext = os.path.splitext(input_basename)[0]
    logger.info(f"Processing input file: {input_file}")

    # Determine optimal number of workers based on available models
    model_count = len(MODEL_DICT)
    effective_workers = min(max_workers, model_count)
    logger.info(f"Using {effective_workers} workers to process {model_count} models")

    # Start time for overall processing
    start_time = datetime.now()

    # Set up threading pool for parallel processing
    with concurrent.futures.ThreadPoolExecutor(
        max_workers=effective_workers
    ) as executor:
        # Start a processing task for each model
        future_to_model = {}

        logger.info(f"Launching processing tasks for {model_count} models")
        for model_key, model_id in MODEL_DICT.items():
            # Generate output filename: basename_modelkey.xml
            output_file = os.path.join(
                output_folder, f"{input_name_without_ext}_{model_key}.xml"
            )
            logger.debug(f"Queuing model {model_key} ‚Üí {output_file}")

            # Submit the processing task to the executor
            future = executor.submit(
                process_document_with_model,
                input_file=input_file,
                output_file=output_file,
                model=model_id,
                model_key=model_key,
                temperature=temperature,
                verbose=verbose,
                save_incremental=save_incremental,
                backup=backup,
            )

            future_to_model[future] = (model_key, model_id)

        # Process results as they complete
        completed = 0
        successful_models = []
        failed_models = []

        logger.info(f"Waiting for {model_count} processing tasks to complete...")
        for future in concurrent.futures.as_completed(future_to_model):
            model_key, model_id = future_to_model[future]
            completed += 1

            try:
                success = future.result()
                if success:
                    successful_models.append(model_key)
                    output_file = os.path.join(
                        output_folder, f"{input_name_without_ext}_{model_key}.xml"
                    )
                    logger.info(
                        f"[{completed}/{model_count}] ‚úÖ Model {model_key} completed successfully ‚Üí {output_file}"
                    )
                else:
                    failed_models.append(model_key)
                    logger.warning(
                        f"[{completed}/{model_count}] ‚ùå Model {model_key} failed after multiple retries"
                    )
            except Exception as e:
                failed_models.append(model_key)
                logger.error(
                    f"[{completed}/{model_count}] ‚ùå Exception processing with model {model_key}: {e}"
                )

        # Calculate processing time
        elapsed_time = datetime.now() - start_time

        # Log summary
        success_count = len(successful_models)
        failed_count = len(failed_models)

        logger.info(
            f"Processing complete in {elapsed_time.total_seconds():.2f} seconds"
        )
        logger.info(
            f"Successful models ({success_count}/{model_count}): {', '.join(successful_models) if successful_models else 'None'}"
        )

        if failed_count > 0:
            logger.warning(
                f"Failed models ({failed_count}/{model_count}): {', '.join(failed_models)}"
            )

        # Generate a summary file
        summary_file = os.path.join(
            output_folder, f"{input_name_without_ext}_summary.json"
        )
        try:
            summary = {
                "input_file": input_file,
                "timestamp": datetime.now().isoformat(),
                "processing_time_seconds": elapsed_time.total_seconds(),
                "total_models": model_count,
                "successful_models": successful_models,
                "failed_models": failed_models,
                "models": dict(MODEL_DICT.items()),
                "backup_enabled": backup,
            }

            with open(summary_file, "w", encoding="utf-8") as f:
                json.dump(summary, f, ensure_ascii=False, indent=2)

            logger.info(f"Summary saved to {summary_file}")
        except Exception as e:
            logger.error(f"Error saving summary: {e}")


def main(
    input_file: str,
    output_file: str,
    model: str = DEFAULT_MODEL,
    temperature: float = DEFAULT_TEMPERATURE,
    verbose: bool = True,
    save_incremental: bool = True,
    benchmark: bool = False,
    max_workers: int = 4,
    backup: bool = True,
) -> None:
    """
    Main entry point for malmo_entitizer.

    Args:
        input_file: Path to the input XML file
        output_file: Path for the XML output file or folder (if --benchmark is used)
        model: LLM model identifier (ignored if --benchmark is used)
        temperature: LLM temperature setting
        verbose: Enable detailed logging
        save_incremental: Save incremental progress after each chunk
        benchmark: Process with all models in parallel
        max_workers: Maximum number of parallel workers when using --benchmark
        backup: Whether to create backup copies with timestamps
    """
    if verbose:
        logger.level("DEBUG")
        _turn_on_debug()

    if benchmark:
        logger.info(
            f"--benchmark flag specified, will process with all {len(MODEL_DICT)} models in parallel"
        )
        logger.info(f"Treating '{output_file}' as output folder")
        process_document_with_all_models(
            input_file=input_file,
            output_folder=output_file,
            temperature=temperature,
            verbose=verbose,
            save_incremental=save_incremental,
            max_workers=max_workers,
            backup=backup,
        )
    else:
        process_document(
            input_file=input_file,
            output_file=output_file,
            model=model,
            temperature=temperature,
            verbose=verbose,
            save_incremental=save_incremental,
            backup=backup,
        )


if __name__ == "__main__":
    fire.Fire(main)
</file>

<file path="malmo_neifix.py">
#!/usr/bin/env -S uv run -s
# /// script
# dependencies = ["fire", "rich"]
# ///
# this_file: neifix.py
"""
Script to parse XML files and transform content inside <nei> tags.
Transforms text by keeping initial uppercase letters for words,
converting other uppercase letters to lowercase, and removing hyphens.
"""

import re
import sys
from pathlib import Path

import fire
from rich.console import Console


console = Console()
error_console = Console(stderr=True)


def transform_nei_content(match, verbose=False):
    """
    Transform the content inside <nei> tags according to rules:
    - Keep initial uppercase letter for every word
    - Convert every uppercase inside a word to lowercase
    - Eliminate hyphens

    Special handling for:
    - Preserve acronyms (words consisting of single letters separated by spaces)
    """
    # Extract the full tag, the opening tag with attributes, and the content
    full_tag = match.group(0)
    opening_tag = match.group(1)
    content = match.group(2)

    if verbose:
        console.print(f"Processing tag: [bold cyan]{full_tag}[/]")
        console.print(f"Opening tag: [bold blue]{opening_tag}[/]")
        console.print(f"Content: [bold green]{content}[/]")

    # Process each word in the content
    words = content.split()
    transformed_words = []

    for word in words:
        # Check if it's likely an acronym (single letters separated by spaces)
        if len(word) == 1 or (
            word.replace("-", "").isalpha() and len(word.replace("-", "")) == 1
        ):
            # Preserve single letters (likely parts of acronyms)
            transformed_words.append(word)
            continue

        # Remove hyphens and process capitalization
        word_without_hyphens = word.replace("-", "")

        if len(word_without_hyphens) > 0:
            # Keep first letter as is, convert rest to lowercase
            transformed_word = (
                word_without_hyphens[0] + word_without_hyphens[1:].lower()
            )
            transformed_words.append(transformed_word)

    # Join the transformed words and rebuild the tag
    transformed_content = " ".join(transformed_words)
    transformed_tag = f"{opening_tag}{transformed_content}</nei>"

    if verbose:
        console.print(f"Transformed content: [bold green]{transformed_content}[/]")
        console.print(f"Transformed tag: [bold cyan]{transformed_tag}[/]")

    return transformed_tag


def process_file(input_path, output_path=None, verbose=False):
    """Process the input file and write the transformed content to the output file."""
    try:
        input_path = Path(input_path)
        if not input_path.exists():
            error_console.print(f"[bold red]Input file not found: {input_path}[/]")
            return False

        with open(input_path, encoding="utf-8") as f:
            content = f.read()

        if verbose:
            console.print(f"[bold]Processing file:[/] {input_path}")
            console.print(f"[bold]File size:[/] {len(content)} bytes")

        # Pattern to match <nei> tags with or without attributes
        # This handles both <nei>content</nei> and <nei attr="value">content</nei>
        pattern = r"(<nei(?:\s+[^>]*)?>)(.*?)</nei>"

        # Create a function that includes the verbose parameter
        def transform_with_verbose(match):
            return transform_nei_content(match, verbose)

        transformed_content = re.sub(
            pattern, transform_with_verbose, content, flags=re.DOTALL
        )

        # Count the number of replacements
        original_matches = re.findall(pattern, content, flags=re.DOTALL)
        transformed_matches = re.findall(pattern, transformed_content, flags=re.DOTALL)

        if verbose:
            console.print(
                f"Number of <nei> tags found: [bold cyan]{len(original_matches)}[/]"
            )
            console.print(
                f"Number of <nei> tags transformed: [bold green]{len(transformed_matches)}[/]"
            )

        if output_path:
            with open(output_path, "w", encoding="utf-8") as f:
                f.write(transformed_content)
            console.print(
                f"Transformed content written to [bold green]{output_path}[/]"
            )
            console.print(
                f"Number of <nei> tags processed: [bold cyan]{len(original_matches)}[/]"
            )
        else:
            pass

        return True
    except Exception as e:
        # Use error_console for error messages
        error_console.print(f"[bold red]Error processing file: {e}[/]")
        return False


def transform(input_file: str, output_file: str | None = None, verbose: bool = False):
    """
    Transform content inside <nei> tags according to rules.

    Args:
        input_file: Path to the input XML file
        output_file: Path to the output file. If not provided, output to stdout
        verbose: Enable verbose output

    Returns:
        0 for success, 1 for failure
    """
    success = process_file(input_file, output_file, verbose)
    return 0 if success else 1


def main():
    """Entry point for the CLI."""
    return fire.Fire(transform)


if __name__ == "__main__":
    sys.exit(main())
</file>

<file path="malmo_orator.py">
#!/usr/bin/env -S uv run -s
# /// script
# dependencies = ["fire", "litellm<=1.67.2", "lxml", "backoff", "python-dotenv", "loguru", "types-lxml"]
# ///
# this_file: malmo_orator.py

import fire
import re
import os
import backoff
import datetime
from pathlib import Path
from lxml import etree
from litellm import completion
from litellm._logging import _turn_on_debug
from dotenv import load_dotenv
from loguru import logger
from typing import Any, cast

# Load environment variables from .env file
load_dotenv()

# Global variables
DEFAULT_MODEL = "openrouter/openai/gpt-4.1"
FALLBACK_MODEL = "openrouter/google/gemini-2.5-pro-preview-03-25"
DEFAULT_TEMPERATURE = 1.0

# Dictionary mapping model keys to their actual model identifiers
MODEL_DICT = {
    "geminipro": "openrouter/google/gemini-2.5-pro-preview-03-25",
    "claudeopus": "openrouter/anthropic/claude-3-opus",
    "gpt41": "openrouter/openai/gpt-4.1",
    "gpt41mini": "openrouter/openai/gpt-4.1-mini",
    "r1chimera": "openrouter/tngtech/deepseek-r1t-chimera:free",
    "msr1": "openrouter/microsoft/mai-ds-r1:free",
    "geminiflash": "openrouter/google/gemini-2.5-flash-preview",
    "gpt41nano": "openrouter/openai/gpt-4.1-nano",
    "llama4mav": "openrouter/meta-llama/llama-4-maverick:free",
    "llama4sc": "openrouter/meta-llama/llama-4-scout:free",
    "claudesonnet": "openrouter/anthropic/claude-3.7-sonnet",
    "nemotron": "openrouter/nvidia/llama-3.1-nemotron-ultra-253b-v1:free",
    "qwenturbo": "openrouter/qwen/qwen-turbo",
    "geminiflash2": "openrouter/google/gemini-2.0-flash-001",
    "r1": "openrouter/deepseek/deepseek-r1:free",
    "deepseek3": "openrouter/deepseek/deepseek-chat:free",
    "geminiflash2e": "openrouter/google/gemini-2.0-flash-exp:free",
    "geminiflash1": "openrouter/google/gemini-flash-1.5-8b",
}

# Check if API key is set
if not os.getenv("OPENROUTER_API_KEY"):
    logger.warning("No API key found in environment variables. API calls may fail.")


def parse_xml(input_file: str) -> etree._Element:
    """
    Parse the input XML file.

    Args:
        input_file: Path to the input XML file

    Returns:
        Parsed XML element tree
    """
    try:
        parser = etree.XMLParser(remove_blank_text=False, recover=True)
        with open(input_file, encoding="utf-8") as f:
            xml_content = f.read()
        return etree.fromstring(xml_content.encode("utf-8"), parser)
    except Exception as e:
        logger.error(f"Error parsing XML: {e}")
        raise


def get_chunks(root: etree._Element) -> list[etree._Element]:
    """
    Extract chunks from the XML document.

    Args:
        root: Root XML element

    Returns:
        List of chunk elements
    """
    return root.xpath("//chunk")


def get_items_from_chunk(chunk: etree._Element) -> list[etree._Element]:
    """
    Extract all item elements from a chunk.

    Args:
        chunk: Chunk element

    Returns:
        List of item elements
    """
    return chunk.xpath(".//item")


def extract_item_elements(xml_text: str) -> dict[str, str]:
    """
    Extract item elements from XML text and map them by ID.

    Args:
        xml_text: XML text containing item elements

    Returns:
        Dictionary mapping item IDs to their full XML content
    """
    item_map = {}
    # Pattern to match complete <item> elements including their attributes and content
    pattern = r"<item\s+([^>]*)>(.*?)</item>"

    # Find all item elements
    for match in re.finditer(pattern, xml_text, re.DOTALL):
        # Extract attributes and content
        attributes = match.group(1)
        content = match.group(2)

        # Extract ID attribute
        id_match = re.search(r'id="([^"]*)"', attributes)
        if id_match:
            item_id = id_match.group(1)
            # Store the complete item element
            item_map[item_id] = f"<item {attributes}>{content}</item>"

    return item_map


def merge_tagged_items(
    original_chunk: etree._Element, tagged_items: dict[str, str]
) -> etree._Element:
    """
    Merge tagged items back into the original chunk structure.

    Args:
        original_chunk: Original chunk element
        tagged_items: Dictionary mapping item IDs to their tagged XML content

    Returns:
        Updated chunk element with tagged items merged
    """
    # Clone the original chunk to avoid modifying it directly
    parser = etree.XMLParser(remove_blank_text=False, recover=True)
    updated_chunk = etree.fromstring(etree.tostring(original_chunk), parser)

    # Find all item elements in the updated chunk
    for item in updated_chunk.xpath(".//item"):
        item_id = item.get("id")
        if item_id and item_id in tagged_items:
            # Parse the tagged item
            try:
                parser = etree.XMLParser(remove_blank_text=False, recover=True)
                tagged_item = etree.fromstring(
                    tagged_items[item_id].encode("utf-8"), parser
                )

                # Replace the original item with the tagged one
                parent = item.getparent()
                if parent is not None:
                    item_idx = parent.index(item)
                    parent[item_idx] = tagged_item
            except etree.XMLSyntaxError as e:
                logger.error(f"Error parsing item {item_id} XML: {e}")
                logger.debug(f"Malformed XML: {tagged_items[item_id]}")
                # Keep the original item if parsing fails
            except Exception as e:
                logger.error(f"Error replacing item {item_id}: {e}")
                # Keep the original item if there's any other error

    return updated_chunk


def save_current_state(
    root: etree._Element,
    output_file: str,
    chunk_id: str,
    step_name: str = "",
    backup: bool = False,
) -> None:
    """
    Save current state to the output file after processing a chunk.

    Args:
        root: Current XML root element
        output_file: File to save output
        chunk_id: ID of the chunk that was processed
        step_name: Name of the processing step (optional)
        backup: Whether to create backup copies with timestamps
    """
    try:
        # Save current XML state
        with open(output_file, "wb") as f:
            xml_bytes = etree.tostring(
                root,
                encoding="utf-8",
                xml_declaration=True,
                pretty_print=False,
            )
            f.write(xml_bytes)

        # Create backup if requested
        if backup:
            output_path = Path(output_file)
            backup_dir = output_path.parent / f"{output_path.stem}_backups"
            backup_dir.mkdir(exist_ok=True)

            # Generate timestamp
            timestamp = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
            backup_filename = f"{output_path.stem}-{timestamp}{output_path.suffix}"
            backup_path = backup_dir / backup_filename

            # Save backup
            with open(backup_path, "wb") as f:
                f.write(xml_bytes)

            logger.debug(f"Created backup at: {backup_path}")

        if step_name:
            logger.info(
                f"Saved current state after processing chunk {chunk_id} - {step_name} step"
            )
        else:
            logger.info(f"Saved current state after processing chunk {chunk_id}")

    except Exception as e:
        logger.error(f"Error saving current state: {e}")


@backoff.on_exception(backoff.expo, Exception, max_tries=3)
def run_llm_processing(
    chunk_text: str, prompt_template: str, model: str, temperature: float
) -> str:
    """
    Send text to LLM for processing using a specific prompt template.

    Args:
        chunk_text: XML text of the chunk
        prompt_template: Prompt template for the LLM
        model: LLM model identifier
        temperature: LLM temperature setting

    Returns:
        Processed text from the LLM
    """
    # Check if API key is available
    if not os.getenv("OPENROUTER_API_KEY"):
        logger.warning("No API key available for LLM requests")
        return chunk_text

    # Construct the full prompt with the text
    prompt = prompt_template + f"\n```xml\n{chunk_text}\n```"

    logger.info(f"Sending request to LLM model: {model}")
    logger.debug(f"Prompt: {prompt}")

    try:
        # Call the LLM and handle the response in a way that satisfies the type checker
        response = completion(
            model=model,
            messages=[{"role": "user", "content": prompt}],
            temperature=temperature,
        )

        # Use a safer approach with type checking
        try:
            # Cast response to Any to bypass type checking for attribute access
            response_any = cast(Any, response)

            # Check if we can safely access the content
            if (
                hasattr(response_any, "choices")
                and response_any.choices
                and hasattr(response_any.choices[0], "message")
                and hasattr(response_any.choices[0].message, "content")
            ):
                content = response_any.choices[0].message.content
                if content is not None:
                    return str(content)

            logger.warning("Unexpected response structure from LLM")
            return chunk_text
        except (AttributeError, IndexError):
            logger.warning("Unable to access expected attributes in LLM response")
            return chunk_text
    except Exception as e:
        logger.error(f"Error during LLM processing: {e}")

        # Attempt to use fallback model if different from current model
        if model != FALLBACK_MODEL:
            logger.warning(f"Attempting to use fallback model: {FALLBACK_MODEL}")
            try:
                response = completion(
                    model=FALLBACK_MODEL,
                    messages=[{"role": "user", "content": prompt}],
                    temperature=temperature,
                )

                # Try to extract content from fallback response
                try:
                    response_any = cast(Any, response)
                    if (
                        hasattr(response_any, "choices")
                        and response_any.choices
                        and hasattr(response_any.choices[0], "message")
                        and hasattr(response_any.choices[0].message, "content")
                    ):
                        content = response_any.choices[0].message.content
                        if content is not None:
                            logger.info(
                                f"Successfully processed with fallback model: {FALLBACK_MODEL}"
                            )
                            return str(content)
                except (AttributeError, IndexError):
                    logger.warning(
                        "Unable to access expected attributes in fallback LLM response"
                    )
            except Exception as fallback_error:
                logger.error(f"Fallback model also failed: {fallback_error}")

        # If primary model and fallback both fail, or they're the same model, just return the original
        raise


def process_sentences(
    chunk: etree._Element,
    model: str,
    temperature: float,
    root: etree._Element | None = None,
    output_file: str | None = None,
    backup: bool = False,
) -> etree._Element:
    """
    Process a chunk to restructure sentences using LLM.

    Args:
        chunk: XML chunk element
        model: LLM model identifier
        temperature: LLM temperature setting
        root: Root element (optional, for saving intermediate results)
        output_file: Output file path (optional, for saving intermediate results)
        backup: Whether to create backup copies with timestamps

    Returns:
        Updated chunk with restructured sentences
    """
    chunk_id = chunk.get("id", "unknown_chunk")
    logger.info(f"Processing sentences for chunk {chunk_id}")

    # Get the chunk XML as text
    chunk_text = etree.tostring(chunk).decode("utf-8").strip()

    # Define the prompt template
    prompt_template = """
    Take the input text. Divide long sentences into shorter sentences (split at a comma or a semicolon), but DO NOT change any words or word order. Only insert periods (or exclamation marks) at logical places to break up very long sentences.

    Also divide long paragraphs into shorter paragraphs using two newlines.

    Each paragraph and heading must end with a punctuation mark: period or exclamation mark.

    The text must identical to the original in terms of words. Preserve all existing XML tags, punctuation, and structure.

    IMPORTANT: Return the complete new text.

    Input text:
    """

    # Process the chunk with LLM
    try:
        llm_response = run_llm_processing(
            chunk_text, prompt_template, model, temperature
        )

        # Extract item elements from the LLM response
        tagged_items = extract_item_elements(llm_response)

        # Merge the tagged items back into the original chunk
        if tagged_items:
            updated_chunk = merge_tagged_items(chunk, tagged_items)
            logger.info(f"Successfully processed sentences for chunk {chunk_id}")

            # Save intermediate results if root and output_file are provided
            if root is not None and output_file:
                # Replace the chunk in the root
                parent = chunk.getparent()
                if parent is not None:
                    chunk_idx = parent.index(chunk)
                    parent[chunk_idx] = updated_chunk

                # Save the current state
                save_current_state(root, output_file, chunk_id, "sentences", backup)

                # Return the updated chunk
                return updated_chunk
            else:
                return updated_chunk
        else:
            logger.warning(f"No valid items found in LLM response for chunk {chunk_id}")
            return chunk
    except Exception as e:
        logger.error(f"Error processing sentences: {e}")
        return chunk


def process_words(
    chunk: etree._Element,
    model: str,
    temperature: float,
    root: etree._Element | None = None,
    output_file: str | None = None,
    backup: bool = False,
) -> etree._Element:
    """
    Process a chunk to normalize words using LLM.

    Args:
        chunk: XML chunk element
        model: LLM model identifier
        temperature: LLM temperature setting
        root: Root element (optional, for saving intermediate results)
        output_file: Output file path (optional, for saving intermediate results)
        backup: Whether to create backup copies with timestamps

    Returns:
        Updated chunk with normalized words
    """
    chunk_id = chunk.get("id", "unknown_chunk")
    logger.info(f"Processing words for chunk {chunk_id}")

    # Get the chunk XML as text
    chunk_text = etree.tostring(chunk).decode("utf-8").strip()

    # Define the prompt template
    prompt_template = """
    We're converting written text into a script for spoken narrative. Process the text according to these rules:

    1. Convert digits to numeric words. Do it in a context-aware and language-specific way, e.g., '42' ‚Üí 'forty-two' if the text is in English, '20 maja 2004, 22.11' ‚Üí 'dwudziestego maja dwa tysiƒÖce czwartego roku, dwudziesta druga jedena≈õcie'.

    2. Replace '/' and '&' with appropriate conjunctions, in a language-specific way, e.g., if the text is in English: 'up/down' ‚Üí 'up or down', 'X & Y' ‚Üí 'X and Y').

    3. Similarly resolve other rare symbols in the text, spelling them out. But don't change the text if it's already in a readable form.

    Make these changes without altering the content beyond the specified transformations. Preserve all existing XML tags and structure.

    IMPORTANT: Return the complete new text.

    Input text:
    """

    # Process the chunk with LLM
    try:
        llm_response = run_llm_processing(
            chunk_text, prompt_template, model, temperature
        )

        # Extract item elements from the LLM response
        tagged_items = extract_item_elements(llm_response)

        # Merge the tagged items back into the original chunk
        if tagged_items:
            updated_chunk = merge_tagged_items(chunk, tagged_items)
            logger.info(f"Successfully processed words for chunk {chunk_id}")

            # Save intermediate results if root and output_file are provided
            if root is not None and output_file:
                # Replace the chunk in the root
                parent = chunk.getparent()
                if parent is not None:
                    chunk_idx = parent.index(chunk)
                    parent[chunk_idx] = updated_chunk

                # Save the current state
                save_current_state(root, output_file, chunk_id, "words", backup)

                # Return the updated chunk
                return updated_chunk
            else:
                return updated_chunk
        else:
            logger.info(f"No items needed word processing for chunk {chunk_id}")
            return chunk
    except Exception as e:
        logger.error(f"Error processing words: {e}")
        return chunk


def process_punctuation(
    chunk: etree._Element,
    root: etree._Element | None = None,
    output_file: str | None = None,
    backup: bool = False,
) -> etree._Element:
    """
    Process a chunk to enhance punctuation algorithmically (not using LLM).

    Args:
        chunk: XML chunk element
        root: Root element (optional, for saving intermediate results)
        output_file: Output file path (optional, for saving intermediate results)
        backup: Whether to create backup copies with timestamps

    Returns:
        Updated chunk with enhanced punctuation
    """
    chunk_id = chunk.get("id", "unknown_chunk")
    logger.info(f"Processing punctuation for chunk {chunk_id}")

    # Clone the chunk to avoid modifying it directly
    parser = etree.XMLParser(remove_blank_text=False, recover=True)
    updated_chunk = etree.fromstring(etree.tostring(chunk), parser)

    # Find all item elements in the updated chunk
    for item in updated_chunk.xpath(".//item"):
        if item.text:
            # Process parenthetical expressions - surround with spaces and ellipses
            item.text = re.sub(r"\(([^)]+)\)", r" ... (\1) ... ", item.text)

            # Process em dashes, en dashes and hyphens when used as pauses (with spaces around them)
            # Only target dashes that have spaces before and after them, preserving hyphenated words
            item.text = re.sub(r"(?<=\s)[‚Äî‚Äì-](?=\s)", r"...", item.text)

    logger.info(f"Successfully processed punctuation for chunk {chunk_id}")

    # Save intermediate results if root and output_file are provided
    if root is not None and output_file:
        # Replace the chunk in the root
        parent = chunk.getparent()
        if parent is not None:
            chunk_idx = parent.index(chunk)
            parent[chunk_idx] = updated_chunk

        # Save the current state
        save_current_state(root, output_file, chunk_id, "punctuation", backup)

    return updated_chunk


def process_emotions(
    chunk: etree._Element,
    model: str,
    temperature: float,
    root: etree._Element | None = None,
    output_file: str | None = None,
    backup: bool = False,
) -> etree._Element:
    """
    Process a chunk to add emotional emphasis using LLM.

    Args:
        chunk: XML chunk element
        model: LLM model identifier
        temperature: LLM temperature setting
        root: Root element (optional, for saving intermediate results)
        output_file: Output file path (optional, for saving intermediate results)
        backup: Whether to create backup copies with timestamps

    Returns:
        Updated chunk with emotional emphasis
    """
    chunk_id = chunk.get("id", "unknown_chunk")
    logger.info(f"Processing emotions for chunk {chunk_id}")

    # Get the chunk XML as text
    chunk_text = etree.tostring(chunk).decode("utf-8").strip()

    # Define the prompt template
    prompt_template = """
    Take the text and add emotional cues. In each paragraph, select at least one word or phrase and surround it with <em> tags, like an actor emphasizing that part of speech. Sparingly insert <hr/> tags to add dramatic pauses where appropriate. Add exclamation marks where you need additional energy or drama.

    However: avoid excessive emphasis, quotation marks or ellipses. The text must remain fluid while gaining expressiveness.

    DO NOT REMOVE ANY WORDS! Don't shorten or paraphrase the text. Only enhance its delivery with punctuation and accents, considering its original tone.

    Preserve all existing XML tags and structure.

    IMPORTANT: Return the complete new text.

    Input text:
    """

    # Process the chunk with LLM
    try:
        llm_response = run_llm_processing(
            chunk_text, prompt_template, model, temperature
        )

        # Extract item elements from the LLM response
        tagged_items = extract_item_elements(llm_response)

        # Merge the tagged items back into the original chunk
        if tagged_items:
            updated_chunk = merge_tagged_items(chunk, tagged_items)
            logger.info(f"Successfully processed emotions for chunk {chunk_id}")

            # Save intermediate results if root and output_file are provided
            if root is not None and output_file:
                # Replace the chunk in the root
                parent = chunk.getparent()
                if parent is not None:
                    chunk_idx = parent.index(chunk)
                    parent[chunk_idx] = updated_chunk

                # Save the current state
                save_current_state(root, output_file, chunk_id, "emotions", backup)

                # Return the updated chunk
                return updated_chunk
            else:
                return updated_chunk
        else:
            logger.info(f"No items needed emotional processing for chunk {chunk_id}")
            return chunk
    except Exception as e:
        logger.error(f"Error processing emotions: {e}")
        return chunk


def process_document(
    input_file: str,
    output_file: str,
    model: str = DEFAULT_MODEL,
    temperature: float = DEFAULT_TEMPERATURE,
    sentences: bool = False,
    words: bool = False,
    punctuation: bool = False,
    emotions: bool = False,
    all_steps: bool = False,
    verbose: bool = False,
    backup: bool = False,
) -> None:
    """
    Process the document through the specified enhancement steps.

    Args:
        input_file: Path to the input XML file
        output_file: Path to the output XML file
        model: LLM model identifier
        temperature: LLM temperature setting
        sentences: Whether to apply sentence restructuring
        words: Whether to apply word normalization
        punctuation: Whether to apply punctuation enhancement
        emotions: Whether to apply emotional emphasis
        all_steps: Whether to apply all enhancement steps
        verbose: Whether to enable verbose logging
        backup: Whether to create backup copies with timestamps
    """
    # Set up logging level
    if verbose:
        logger.remove()
        logger.add(lambda msg: print(msg, end=""), level="DEBUG")
    else:
        logger.remove()
        logger.add(lambda msg: print(msg, end=""), level="INFO")

    # Apply all steps if all_steps is True
    if all_steps:
        sentences = words = punctuation = emotions = True

    # Check if at least one step is specified
    if not any([sentences, words, punctuation, emotions]):
        logger.warning(
            "No processing steps specified. Use --sentences, --words, --punctuation, --emotions, or --all."
        )
        return

    try:
        # Parse input XML
        logger.info(f"Parsing input file: {input_file}")
        root = parse_xml(input_file)

        # Get chunks
        chunks = get_chunks(root)
        logger.info(f"Found {len(chunks)} chunks in the document")

        # Process each chunk
        for i, chunk in enumerate(chunks):
            chunk_id = chunk.get("id", f"chunk_{i}")
            logger.info(f"Processing chunk {chunk_id} ({i + 1}/{len(chunks)})")

            # Get the current chunk from the document (it may have been updated)
            current_chunks = get_chunks(root)
            current_chunk = current_chunks[i]

            # Apply each selected enhancement step
            if sentences:
                logger.info("Applying sentence restructuring")
                current_chunk = process_sentences(
                    current_chunk, model, temperature, root, output_file, backup
                )

            if words:
                logger.info("Applying word normalization")
                current_chunk = process_words(
                    current_chunk, model, temperature, root, output_file, backup
                )

            if punctuation:
                logger.info("Applying punctuation enhancement")
                current_chunk = process_punctuation(
                    current_chunk, root, output_file, backup
                )

            if emotions:
                logger.info("Applying emotional emphasis")
                current_chunk = process_emotions(
                    current_chunk, model, temperature, root, output_file, backup
                )

            # Final save for the chunk
            save_current_state(
                root, output_file, chunk_id, "all steps complete", backup
            )

        # Final save
        save_current_state(root, output_file, "final", "processing complete", backup)
        logger.info(f"Document processing completed. Output saved to {output_file}")

    except Exception as e:
        logger.error(f"Error processing document: {e}")
        raise


def main(
    input_file: str,
    output_file: str,
    model: str = DEFAULT_MODEL,
    temperature: float = DEFAULT_TEMPERATURE,
    sentences: bool = False,
    words: bool = False,
    punctuation: bool = False,
    emotions: bool = False,
    all_steps: bool = False,
    verbose: bool = True,
    backup: bool = True,
) -> None:
    """
    Main entry point for the malmo_orator tool.

    Args:
        input_file: Path to the input XML file
        output_file: Path to the output XML file
        model: LLM model identifier or key from MODEL_DICT
        temperature: LLM temperature setting
        sentences: Whether to apply sentence restructuring
        words: Whether to apply word normalization
        punctuation: Whether to apply punctuation enhancement
        emotions: Whether to apply emotional emphasis
        all_steps: Whether to apply all enhancement steps
        verbose: Whether to enable verbose logging
        backup: Whether to create backup copies with timestamps
    """
    # Resolve model name if it's a key in MODEL_DICT
    if model in MODEL_DICT:
        model = MODEL_DICT[model]
    if verbose:
        logger.level("DEBUG")
        _turn_on_debug()

    # Process the document
    try:
        process_document(
            input_file=input_file,
            output_file=output_file,
            model=model,
            temperature=temperature,
            sentences=sentences,
            words=words,
            punctuation=punctuation,
            emotions=emotions,
            all_steps=all_steps,
            verbose=verbose,
            backup=backup,
        )
    except Exception as e:
        logger.error(f"Error in main function: {e}")
        raise


if __name__ == "__main__":
    fire.Fire(main)
</file>

<file path="malmo_tonedown.py">
#!/usr/bin/env -S uv run -s
# /// script
# dependencies = ["fire", "lxml", "loguru", "backoff", "python-dotenv", "litellm<=1.67.2", "rich", "types-lxml"]
# ///
# this_file: malmo_tonedown.py

import fire
import re
import os
import json
import backoff
from lxml import etree
from loguru import logger
from rich.console import Console
from typing import Any, cast
from dotenv import load_dotenv
from litellm import completion
from litellm._logging import _turn_on_debug

# Load environment variables from .env file
load_dotenv()

console = Console()

# Global variables
DEFAULT_MODEL = "openrouter/openai/gpt-4.1"
FALLBACK_MODEL = "openrouter/google/gemini-2.5-pro-preview-03-25"
DEFAULT_TEMPERATURE = 1.0

# Dictionary mapping model keys to their actual model identifiers
MODEL_DICT = {
    "geminipro": "openrouter/google/gemini-2.5-pro-preview-03-25",
    "claudeopus": "openrouter/anthropic/claude-3-opus",
    "gpt41": "openrouter/openai/gpt-4.1",
    "gpt41mini": "openrouter/openai/gpt-4.1-mini",
    "claudesonnet": "openrouter/anthropic/claude-3.7-sonnet",
}

# Check if API key is set
if not os.getenv("OPENROUTER_API_KEY"):
    logger.warning("No API key found in environment variables. API calls may fail.")


# --- XML Parsing Utilities ---
def parse_xml(input_file: str) -> etree._Element:
    """
    Parse the input XML file.

    Args:
        input_file: Path to the input XML file

    Returns:
        Parsed XML element tree
    """
    try:
        # Use a more robust parser configuration
        parser = etree.XMLParser(
            remove_blank_text=False,
            recover=True,
            resolve_entities=False,
            huge_tree=True,
            remove_comments=False,  # Preserve comments
            remove_pis=False,  # Preserve processing instructions
        )
        with open(input_file, encoding="utf-8") as f:
            xml_content = f.read()

        # Parse the content
        root = etree.fromstring(xml_content.encode("utf-8"), parser)

        # Log some basic info about the parsed structure
        logger.debug(
            f"XML parsed successfully: {len(root.xpath('//*'))} elements total"
        )
        logger.debug(f"Found {len(root.xpath('//nei'))} nei elements")

        return root
    except Exception as e:
        logger.error(f"Error parsing XML: {e}")
        raise


# --- NEI Dict Extraction and Tag Processing ---
def process_nei_tags(root: etree._Element) -> dict[str, str]:
    """
    Build the NEIs dict and update <nei> tags in-place according to the rules.

    Args:
        root: XML root element

    Returns:
        neis: dict mapping orig attribute to pronunciation cue
    """
    neis = {}
    for nei in root.xpath("//nei"):
        # Skip processing if element is None or doesn't have attributes
        if nei is None or not hasattr(nei, "attrib"):
            continue

        orig = nei.get("orig")
        if not orig:
            continue

        # Get the text content safely by joining all text content, ignoring nested tags
        content_parts = []

        # First add the direct text of the nei element
        if nei.text:
            content_parts.append(nei.text)

        # Then add text from any children, recursively
        for elem in nei.iter():
            if elem != nei and elem.text:
                content_parts.append(elem.text)

        # Join all text parts and normalize whitespace
        content = " ".join(content_parts).strip()
        content = re.sub(r"\s+", " ", content)

        if not content:
            # Fallback to just getting all text
            content = "".join(nei.itertext()).strip()
            content = re.sub(r"\s+", " ", content)

        if orig not in neis:
            neis[orig] = content
            # Set new="true" if not already present with that value
            if nei.get("new") != "true":
                nei.set("new", "true")
        elif "new" in nei.attrib:
            del nei.attrib["new"]

    return neis


# --- Language Detection with LLM ---
def extract_middle_fragment(xml_text: str, length: int = 1024) -> str:
    """
    Extract a fragment of the given length from the middle of the XML text.
    Handles XML cleaning in a safe way.

    Args:
        xml_text: Full XML text
        length: Length of fragment to extract

    Returns:
        Text fragment from the middle of the document
    """
    if not xml_text:
        return ""

    try:
        # Clean XML tags to get plain text for better language detection
        # Uses a more resilient approach to handle potentially malformed XML
        plain_text = re.sub(r"<[^>]*>", " ", xml_text)
        plain_text = re.sub(r"\s+", " ", plain_text).strip()

        n = len(plain_text)
        if n <= length:
            return plain_text
        start = (n - length) // 2
        return plain_text[start : start + length]
    except Exception as e:
        logger.warning(f"Error extracting middle fragment: {e}")
        # Fallback to a simpler approach
        return xml_text[:length] if len(xml_text) > length else xml_text


@backoff.on_exception(backoff.expo, Exception, max_tries=3)
def detect_language_llm(
    fragment: str, model: str = DEFAULT_MODEL, temperature: float = 0.2
) -> str:
    """
    Use LLM to detect the predominant language of the text fragment.

    Args:
        fragment: Text fragment to analyze
        model: LLM model to use
        temperature: Temperature setting for the LLM

    Returns:
        Detected language code or name
    """
    # Check if API key is available
    if not os.getenv("OPENROUTER_API_KEY"):
        logger.warning("No API key available for LLM language detection")
        return "unknown"

    prompt = f"""
    Analyze the following text fragment and determine its predominant language.
    Respond with ONLY the language name in English e.g. "English", "Polish", "German").
    Do not include any explanation or additional text in your response.

    Text fragment:
    ```
    {fragment}
    ```
    """

    logger.info(f"Sending language detection request to LLM model: {model}")

    try:
        response = call_llm(prompt, model, temperature)

        # Extract just the language name (remove any quotation marks, periods, etc.)
        language = response.strip().lower()
        language = re.sub(r'["\'\.,]', "", language)
        logger.info(f"LLM detected language: {language}")
        return language
    except Exception as e:
        logger.error(f"Error during language detection: {e}")
        return "unknown"


# --- Pronunciation Cue Review with LLM ---
@backoff.on_exception(backoff.expo, Exception, max_tries=3)
def revise_neis_llm(
    neis: dict[str, str],
    language: str,
    model: str = DEFAULT_MODEL,
    temperature: float = 1.0,
) -> dict[str, str]:
    """
    Use LLM to review and potentially revise pronunciation cues for named entities.

    Args:
        neis: Dictionary mapping original entity names to pronunciation cues
        language: Detected language of the document
        model: LLM model to use
        temperature: Temperature setting for the LLM

    Returns:
        Revised dictionary of named entities and their pronunciation cues
    """
    if not neis:
        logger.info("No named entities to review")
        return {}

    # Check if API key is available
    if not os.getenv("OPENROUTER_API_KEY"):
        logger.warning("No API key available for LLM pronunciation review")
        return neis

    neis_json = json.dumps(neis, ensure_ascii=False, indent=2)

    prompt = f"""
    You are reviewing pronunciation cues for named entities in a {language} text-to-speech system. Here is a dictionary where:
    - Keys are the original named entities of interest (NEIs)
    - Values are simplified pronunciation cues for speakers of {language}

    For each entry, determine if the pronunciation cue is necessary, and produce a new dictionary with the original entities as keys and the following values:

    Option 1. KEEP the simplified cue as value if the original entity:
        - is a name or phrase that is very unusual, foreign, or ambiguous for speakers of {language}
        - has a completely non-intuitive pronunciation in {language} or actually means something different in {language}

    Option 2. REPLACE cues with the ORIGINAL entity spelling if:
        - the original entity is actually a native phrase in {language}
        - the original entity would be familiar to average {language} readers
        - the original entity is a longer phrase in English
        - the pronunciation cue is misleading or unnecessarily complicated

    Option 3. MAKE a new ADAPTED value (cue) that is not as arcane or complex as the cue we have

    Return ONLY a valid JSON dictionary with the original entities as keys and the new values (which are either the old cues or the original entities or the adapted cues).

    Named entities dictionary:
    {neis_json}
    """

    logger.info(f"Submitting pronunciation review request to LLM model: {model}")
    logger.debug("Waiting for LLM response...")

    try:
        response = call_llm(prompt, model, temperature)
        logger.debug("LLM response received, processing...")

        # Extract JSON dictionary from response
        json_pattern = r"\{[\s\S]*\}"
        json_match = re.search(json_pattern, response)

        if json_match:
            json_str = json_match.group(0)
            revised_neis = json.loads(json_str)
            logger.info(f"Successfully revised {len(revised_neis)} named entities")
            return revised_neis
        else:
            logger.warning("Could not extract valid JSON from LLM response")
            return neis
    except json.JSONDecodeError as e:
        logger.error(f"Error decoding JSON from LLM response: {e}")
        return neis
    except Exception as e:
        logger.error(f"Error during pronunciation cue review: {e}")
        return neis


# --- Common LLM Call Function ---
def call_llm(prompt: str, model: str, temperature: float) -> str:
    """
    Generic function to call the LLM and extract the response text.

    Args:
        prompt: The prompt to send to the LLM
        model: LLM model to use
        temperature: Temperature setting for the LLM

    Returns:
        Text response from the LLM
    """
    try:
        # Call the LLM and handle the response
        logger.debug(f"Calling LLM API with model={model}, temperature={temperature}")

        response = completion(
            model=model,
            messages=[{"role": "user", "content": prompt}],
            temperature=temperature,
        )

        logger.debug("LLM API call completed successfully")

        # Extract content from response
        response_any = cast(Any, response)
        if (
            hasattr(response_any, "choices")
            and response_any.choices
            and hasattr(response_any.choices[0], "message")
            and hasattr(response_any.choices[0].message, "content")
        ):
            content = response_any.choices[0].message.content
            if content is not None:
                return str(content)

        logger.warning("Unexpected response structure from LLM")
        msg = "Could not extract content from LLM response"
        raise ValueError(msg)

    except Exception as e:
        logger.error(f"Error calling LLM: {e}")

        # Attempt to use fallback model if different from current model
        if model != FALLBACK_MODEL:
            logger.warning(f"Attempting to use fallback model: {FALLBACK_MODEL}")
            try:
                logger.debug(
                    f"Calling fallback LLM API with model={FALLBACK_MODEL}, temperature={temperature}"
                )

                response = completion(
                    model=FALLBACK_MODEL,
                    messages=[{"role": "user", "content": prompt}],
                    temperature=temperature,
                )

                logger.debug("Fallback LLM API call completed successfully")

                # Extract content from fallback response
                response_any = cast(Any, response)
                if (
                    hasattr(response_any, "choices")
                    and response_any.choices
                    and hasattr(response_any.choices[0], "message")
                    and hasattr(response_any.choices[0].message, "content")
                ):
                    content = response_any.choices[0].message.content
                    if content is not None:
                        logger.info("Successfully processed with fallback model")
                        return str(content)
            except Exception as fallback_error:
                logger.error(f"Fallback model also failed: {fallback_error}")

        raise


# --- Replace Pronunciation Cues in XML ---
def update_nei_cues(root: etree._Element, revised_neis: dict[str, str]) -> None:
    """
    Update <nei> tag contents in-place using revised_neis dict.

    Args:
        root: XML root element
        revised_neis: Dictionary mapping original entities to revised pronunciation cues
    """
    for nei in root.xpath("//nei"):
        orig = nei.get("orig")
        if orig and orig in revised_neis:
            # Save all attributes and the tail
            attrs = dict(nei.attrib)
            tail = nei.tail  # Save the tail text that appears after this element

            if tail:
                logger.debug(f"Preserving tail text for '{orig}': {tail[:30]}...")

            # Clear all children and text
            for child in nei:
                nei.remove(child)

            # Set the new text directly, removing any previous content
            nei.text = revised_neis[orig]

            # Preserve the original tail text
            nei.tail = tail

            # Restore all attributes that might have been affected
            for key, value in attrs.items():
                nei.set(key, value)

            # Verify the update was successful
            verification_text = "content not set"
            if nei.text:
                verification_text = (
                    nei.text[:30] + "..." if len(nei.text) > 30 else nei.text
                )

            verification_tail = "no tail"
            if nei.tail:
                verification_tail = (
                    nei.tail[:30] + "..." if len(nei.tail) > 30 else nei.tail
                )

            logger.debug(
                f"Updated nei tag for '{orig}' - content: '{verification_text}', tail: '{verification_tail}'"
            )


# --- Reduce Excessive Emphasis ---
def reduce_emphasis(xml_text: str, min_distance: int) -> str:
    """
    Remove <em> tags that are too close together (in words).
    Uses a more robust approach to handle XML structure.

    Args:
        xml_text: XML text to process
        min_distance: Minimum word distance required between <em> tags

    Returns:
        Processed XML text with reduced emphasis
    """
    # Safety check for empty input
    if not xml_text:
        logger.warning("Empty XML text provided to reduce_emphasis")
        return xml_text

    # Find all <em>...</em> spans and their positions with a more robust pattern
    # This pattern is designed to better handle nested tags and complex content
    em_pattern = r"<em(?:\s[^>]*)?>(.*?)</em>"
    em_spans = list(re.finditer(em_pattern, xml_text, re.DOTALL))
    if len(em_spans) < 2:
        return xml_text

    logger.info(f"Found {len(em_spans)} emphasized spans in the text")

    # Collect (start, end, text) for each <em>
    em_locs = [(m.start(), m.end(), m.group(1)) for m in em_spans]

    # Compute word distances and mark for removal
    to_unwrap = set()
    for i in range(1, len(em_locs)):
        prev_end = em_locs[i - 1][1]
        curr_start = em_locs[i][0]

        # Ensure indexes are valid
        if (
            prev_end >= len(xml_text)
            or curr_start >= len(xml_text)
            or prev_end > curr_start
        ):
            logger.warning(
                f"Invalid span indexes: prev_end={prev_end}, curr_start={curr_start}"
            )
            continue

        between = xml_text[prev_end:curr_start]
        # More robust word count that handles various unicode characters
        word_count = len(re.findall(r"\w+", between, re.UNICODE))
        if word_count < min_distance:
            to_unwrap.add(i)
            logger.debug(
                f"Marking emphasis span {i} for removal (distance: {word_count} words)"
            )

    # Unwrap by removing <em>...</em> tags for marked indices in reverse order
    # to avoid index shifting problems
    new_text = xml_text
    for idx in sorted(to_unwrap, reverse=True):
        if idx >= len(em_spans):
            logger.warning(
                f"Invalid emphasis span index: {idx}, max: {len(em_spans) - 1}"
            )
            continue

        m = em_spans[idx]
        start, end = m.start(), m.end()
        content = m.group(1)

        # Safety check for valid indices
        if start < 0 or end > len(new_text) or start > end:
            logger.warning(
                f"Invalid text positions: start={start}, end={end}, len={len(new_text)}"
            )
            continue

        # Replace <em>...</em> with just the content
        new_text = new_text[:start] + content + new_text[end:]

    logger.info(f"Removed {len(to_unwrap)} emphasis spans that were too close")
    return new_text


# --- Main Processing Logic ---
def process_document(
    input_file: str,
    output_file: str,
    em: int | None = None,
    model: str = DEFAULT_MODEL,
    temperature: float = DEFAULT_TEMPERATURE,
    verbose: bool = False,
) -> None:
    """
    Main processing function for malmo_tonedown.

    Args:
        input_file: Path to the input XML file
        output_file: Path to the output XML file
        em: Minimum word distance between emphasis tags (optional)
        model: LLM model to use
        temperature: Temperature setting for the LLM
        verbose: Whether to enable verbose logging
    """
    # Configure logging
    log_level = "DEBUG" if verbose else "INFO"
    logger.remove()
    logger.add(lambda msg: console.print(msg, highlight=False), level=log_level)

    # Check if input file exists
    if not os.path.exists(input_file):
        logger.error(f"Input file not found: {input_file}")
        msg = f"Input file not found: {input_file}"
        raise FileNotFoundError(msg)

    logger.info(f"Processing XML document: {input_file}")
    logger.info(f"Output will be saved to: {output_file}")
    logger.info(f"Using LLM model: {model}")

    # Resolve model name if it's a key in MODEL_DICT
    if model in MODEL_DICT:
        model = MODEL_DICT[model]
        logger.info(f"Resolved model name to: {model}")

    # Read XML content - save original for comparison
    with open(input_file, encoding="utf-8") as f:
        original_xml_text = f.read()

    # Parse XML
    root = parse_xml(input_file)

    # Step 1: Build NEIs dict and update <nei> tags
    neis = process_nei_tags(root)
    if neis:
        logger.info(f"Found {len(neis)} named entities of interest")

        # IMPORTANT: Print original NEIs dictionary BEFORE sending to LLM
        if verbose:
            console.print("\n[bold green]Original Named Entities:[/bold green]")
            console.print(
                json.dumps(neis, ensure_ascii=False, indent=2), highlight=False
            )
            logger.debug(f"Initial NEIs dict contains {len(neis)} entries")
    else:
        logger.info("No named entities found in the document")

    # Step 2: Language detection
    fragment = extract_middle_fragment(original_xml_text)
    language = detect_language_llm(fragment, model, temperature=1.0)
    logger.info(f"Detected document language: {language}")

    # Step 3: Pronunciation cue review (only if we found NEIs)
    if neis:
        # Print a clear message indicating the start of LLM processing
        logger.info(f"Starting LLM review of {len(neis)} named entities...")
        revised_neis = revise_neis_llm(neis, language, model, temperature)
        logger.info("LLM review completed")

        # Always log the count of revisions
        unchanged_count = sum(1 for k, v in revised_neis.items() if neis.get(k) == v)
        changed_count = len(revised_neis) - unchanged_count
        logger.info(
            f"Named entities revised: {changed_count} changed, {unchanged_count} unchanged"
        )

        if verbose:
            console.print("\n[bold blue]Revised Named Entities:[/bold blue]")
            console.print(
                json.dumps(revised_neis, ensure_ascii=False, indent=2), highlight=False
            )

            # Log changes for clarity
            if changed_count > 0:
                console.print("\n[bold yellow]Changes Made:[/bold yellow]")
                for key, new_value in revised_neis.items():
                    old_value = neis.get(key)
                    if old_value != new_value:
                        console.print(f"[green]{key}[/green]:")
                        console.print(f"  [red]- {old_value}[/red]")
                        console.print(f"  [green]+ {new_value}[/green]")

        # Step 4: Replace cues in XML
        update_nei_cues(root, revised_neis)
        logger.info("Updated pronunciation cues in the document")

    # Step 5: Get XML with proper encoding, trying to preserve formatting
    try:
        # Use method='xml' to ensure proper XML serialization
        xml_out = etree.tostring(
            root,
            encoding="utf-8",
            xml_declaration=True,
            pretty_print=False,  # Don't use pretty_print to avoid text node changes
            method="xml",
            with_tail=True,  # Ensure tail text is included
        ).decode("utf-8")

        # Fix any self-closing tags that should be properly closed
        # (especially for <nei> tags which must not be self-closing)
        xml_out = re.sub(r"<nei([^>]*)/>(?!</nei>)", r"<nei\1></nei>", xml_out)

        # Validate that all nei tags are properly closed and not self-closing
        nei_count = xml_out.count("<nei")
        nei_close_count = xml_out.count("</nei>")
        if nei_count != nei_close_count:
            logger.warning(
                f"Potential issue with nei tags: {nei_count} opening tags but {nei_close_count} closing tags"
            )

    except Exception as e:
        logger.error(f"Error serializing XML: {e}")
        # Fallback to string representation if tostring fails
        try:
            # Convert to string with safe fallback
            xml_bytes = etree.tostring(
                root,
                pretty_print=False,
                encoding="utf-8",
                with_tail=True,  # Ensure tail text is included
            )
            xml_out = xml_bytes.decode("utf-8") if xml_bytes is not None else ""
            logger.warning("Using fallback XML serialization method")
        except Exception:
            logger.error(
                "Both XML serialization methods failed, using basic string conversion"
            )
            # Last resort fallback - this should always work unless root is None
            xml_out = (
                str(etree.tostring(root, encoding="utf-8"))
                .replace("b'", "")
                .replace("'", "")
            )

    # Ensure xml_out is not None before continuing
    if xml_out is None:
        logger.error("XML serialization returned None, cannot continue")
        msg = "Failed to serialize XML"
        raise ValueError(msg)

    # Step 6: Reduce excessive emphasis if --em is set
    if em is not None:
        logger.info(f"Reducing excessive emphasis with minimum distance of {em} words")
        xml_out = reduce_emphasis(xml_out, em)

    # Step 7: Validate the resulting XML to detect corruption
    try:
        # Try to parse the resulting XML as a validation step
        validation_parser = etree.XMLParser(recover=True)
        validation_root = etree.fromstring(xml_out.encode("utf-8"), validation_parser)

        # Count items in original and processed XML
        original_item_count = len(
            etree.fromstring(
                original_xml_text.encode("utf-8"), etree.XMLParser(recover=True)
            ).xpath("//item")
        )
        processed_item_count = len(validation_root.xpath("//item"))

        if original_item_count != processed_item_count:
            logger.warning(
                f"XML structure changed: original had {original_item_count} items, "
                f"processed has {processed_item_count} items"
            )

        # Check if any <nei> tags are self-closing or improperly formatted
        for nei in validation_root.xpath("//nei"):
            if nei.tag is None or not nei.text:
                # Convert bytes to string before logging
                nei_bytes = etree.tostring(nei, encoding="utf-8")
                nei_str = (
                    nei_bytes.decode("utf-8")
                    if nei_bytes is not None
                    else "Unknown tag"
                )
                logger.warning(f"Potentially malformed <nei> tag detected: {nei_str}")

        logger.info("XML validation passed")

    except Exception as e:
        logger.error(f"XML validation failed: {e}")
        logger.warning("Output XML may be corrupted, attempting to repair")

        # Provide a clean empty dict if we don't have revised_neis
        # Try to repair by reprocessing from scratch with more conservative options
        try:
            logger.info("Attempting XML repair...")

            # Use a different serialization approach that prioritizes content preservation
            if os.path.exists(input_file):
                # Just re-parse the file and serialize it again without modifying nei tags
                logger.info("Repairing by re-parsing original file")
                repaired_xml = repair_xml_by_reparsing(input_file)
            else:
                # Use a different serialization approach with the current root
                logger.info("Repairing by re-serializing current tree")
                repaired_xml = etree.tostring(
                    root,
                    encoding="utf-8",
                    xml_declaration=True,
                    pretty_print=False,  # Avoid pretty_print to prevent text node changes
                    method="xml",
                    with_tail=True,
                ).decode("utf-8")

            # Keep the repaired version
            xml_out = repaired_xml
            logger.info("XML repair successful")
        except Exception as repair_err:
            logger.error(f"XML repair failed: {repair_err}")
            # At this point, we'll use what we have and hope for the best

    # Write output
    with open(output_file, "w", encoding="utf-8") as f:
        f.write(xml_out)
    logger.info(f"Successfully wrote output to {output_file}")


# --- XML Repair Function ---
def repair_xml_by_reparsing(file_path: str) -> str:
    """
    Repair XML by re-parsing the original file.

    Args:
        file_path: Path to the original XML file

    Returns:
        Repaired XML as string
    """
    # Re-parse with even more conservative settings
    repair_parser = etree.XMLParser(
        remove_blank_text=False,
        recover=True,
        resolve_entities=False,
        huge_tree=True,
        remove_comments=False,
        remove_pis=False,
    )

    with open(file_path, encoding="utf-8") as f:
        original_content = f.read()

    repair_root = etree.fromstring(original_content.encode("utf-8"), repair_parser)

    # Just serialize without modifying
    return etree.tostring(
        repair_root,
        encoding="utf-8",
        xml_declaration=True,
        pretty_print=False,
        method="xml",
        with_tail=True,
    ).decode("utf-8")


# --- CLI Entrypoint ---
def main(
    input_file: str,
    output_file: str,
    em: int | None = None,
    model: str = DEFAULT_MODEL,
    temperature: float = DEFAULT_TEMPERATURE,
    verbose: bool = True,
) -> None:
    """
    CLI entrypoint for malmo_tonedown.

    Args:
        input_file: Path to the input XML file
        output_file: Path to the output XML file
        em: Minimum word distance between emphasis tags (optional)
        model: LLM model identifier or key from MODEL_DICT
        temperature: LLM temperature setting (0.0-1.0)
        verbose: Whether to enable verbose logging
    """
    if verbose:
        logger.level("DEBUG")
        _turn_on_debug()

    try:
        process_document(
            input_file=input_file,
            output_file=output_file,
            em=em,
            model=model,
            temperature=temperature,
            verbose=verbose,
        )
    except Exception as e:
        logger.error(f"Error in main function: {e}")
        if verbose:
            import traceback

            logger.error(traceback.format_exc())
        raise


if __name__ == "__main__":
    fire.Fire(main)
</file>

<file path="sapko.sh">
#!/usr/bin/env bash
# this_file: work2/sapko.sh
#
# Script to process Sapkowski's Witcher text through the Malmo pipeline
# This script runs the full processing chain from chunking to 11labs format

set -euo pipefail

python ../malmo_orator.py "Andrzej Sapkowski - WiedzÃÅmin Geralt z Rivii 0.1 - RozdrozÃáe krukoÃÅw_step2_entited.xml" "Andrzej Sapkowski - WiedzÃÅmin Geralt z Rivii 0.1 - RozdrozÃáe krukoÃÅw_step3_orated.xml" --all_steps --verbose --backup

python ../malmo_tonedown.py "Andrzej Sapkowski - WiedzÃÅmin Geralt z Rivii 0.1 - RozdrozÃáe krukoÃÅw_step3_orated.xml" "Andrzej Sapkowski - WiedzÃÅmin Geralt z Rivii 0.1 - RozdrozÃáe krukoÃÅw_step4_toneddown.xml" --verbose

python ../malmo_11labs.py "Andrzej Sapkowski - WiedzÃÅmin Geralt z Rivii 0.1 - RozdrozÃáe krukoÃÅw_step4_toneddown.xml" "Andrzej Sapkowski - WiedzÃÅmin Geralt z Rivii 0.1 - RozdrozÃáe krukoÃÅw_step5_11.txt" --verbose
</file>

<file path="say_11labs.py">
#!/usr/bin/env -S uv run -s
# /// script
# dependencies = ["elevenlabs", "fire", "python-dotenv", "loguru", "tenacity", "rich"]
# ///
# this_file: to11/say_11labs.py

import os
from pathlib import Path
from collections.abc import Iterator

import fire
from dotenv import load_dotenv
from loguru import logger
from tenacity import (
    retry,
    stop_after_attempt,
    wait_exponential,
    retry_if_exception_type,
)
from rich.progress import (
    Progress,
    TextColumn,
    BarColumn,
    TaskProgressColumn,
    TimeRemainingColumn,
)

from elevenlabs.client import ElevenLabs
from elevenlabs import save, Voice
from elevenlabs.core import ApiError

# Load environment variables from .env file
load_dotenv()


class VoiceSynthesizer:
    """
    A CLI tool to synthesize text using all personal ElevenLabs voices
    and save the output as MP3 files.
    """

    def __init__(self, api_key: str | None = None, verbose: bool = False):
        """
        Initializes the ElevenLabs client.

        Args:
            api_key: ElevenLabs API key. If None, it will try to load from the
                ELEVENLABS_API_KEY environment variable.
            verbose: Enable verbose logging
        """
        # Configure loguru
        if verbose:
            logger.add("say_11labs.log", rotation="10 MB", level="DEBUG")
        else:
            logger.add("say_11labs.log", rotation="10 MB", level="INFO")

        self.api_key = api_key or os.getenv("ELEVENLABS_API_KEY")
        if not self.api_key:
            logger.error(
                "ELEVENLABS_API_KEY not found. Please set it in your .env file "
                "or pass it as an argument."
            )
            msg = "API key is required."
            raise ValueError(msg)

        self.client = ElevenLabs(api_key=self.api_key)
        logger.info("ElevenLabs client initialized.")

    @retry(
        retry=retry_if_exception_type(ApiError),
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=2, max=10),
        reraise=True,
    )
    def _get_my_voices(self) -> list[Voice]:
        """
        Retrieves all voices belonging to the user (cloned, generated, professional).
        Excludes premade and other categories that aren't typically "my" voices.

        Returns:
            List of user's voices
        """
        logger.debug("Fetching user's voices")
        all_voices_response = self.client.voices.get_all()
        my_voices = [
            voice
            for voice in all_voices_response.voices
            if voice.category
            in ["cloned", "generated", "professional"]  # Adjust as needed
        ]
        if not my_voices:
            logger.warning(
                "No personal/cloned/professional voices found for this API key."
            )
        logger.debug(f"Found {len(my_voices)} voices belonging to the user")
        return my_voices

    def _sanitize_filename(self, name: str) -> str:
        """
        Sanitizes a string to be used as part of a filename.
        Replaces spaces with underscores and removes problematic characters.

        Args:
            name: The string to sanitize

        Returns:
            Sanitized string suitable for filenames
        """
        name = name.replace(" ", "_")
        # Remove or replace characters not allowed in filenames on most OS
        name = "".join(c for c in name if c.isalnum() or c in ("_", "-"))
        return name if name else "unnamed_voice"

    @retry(
        retry=retry_if_exception_type(ApiError),
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=2, max=10),
        reraise=True,
    )
    def _synthesize_text(
        self, voice_id: str, text: str, model_id: str, output_format: str
    ) -> Iterator[bytes]:
        """
        Synthesizes text to speech with the given voice and parameters.
        Includes retry logic for API errors.

        Args:
            voice_id: The voice ID to use
            text: The text to synthesize
            model_id: The model ID to use
            output_format: The output format for the audio

        Returns:
            Iterator of audio data bytes
        """
        logger.debug(f"Synthesizing text with voice ID {voice_id}")
        return self.client.text_to_speech.convert(
            voice_id=voice_id,
            text=text,
            model_id=model_id,
            output_format=output_format,  # type: ignore
        )

    def synthesize_all(
        self,
        text: str,
        output_folder: str = "output_audio",
        model_id: str = "eleven_multilingual_v2",
        output_format: str = "mp3_44100_128",
    ):
        """
        Synthesizes the given text using all personal voices and saves them to MP3s.

        Args:
            text: The text to synthesize.
            output_folder: The folder where MP3 files will be saved.
            model_id: The model ID to use for synthesis.
                Common options: "eleven_multilingual_v2", "eleven_turbo_v2_5".
            output_format: The output format for the audio.
        """
        if not text:
            logger.error("Text to synthesize cannot be empty.")
            return

        my_voices = self._get_my_voices()
        if not my_voices:
            logger.info("No voices to process.")
            return

        output_path = Path(output_folder)
        output_path.mkdir(parents=True, exist_ok=True)
        logger.info(f"Output will be saved to: {output_path.resolve()}")

        successful_syntheses = 0
        failed_syntheses = 0

        # Create a progress bar for synthesis
        with Progress(
            TextColumn("[bold blue]{task.description}"),
            BarColumn(),
            TaskProgressColumn(),
            TimeRemainingColumn(),
        ) as progress:
            task = progress.add_task("Synthesizing voices", total=len(my_voices))

            for voice in my_voices:
                if not voice.voice_id or not voice.name:
                    logger.warning(f"Skipping voice with missing ID or name: {voice}")
                    failed_syntheses += 1
                    progress.update(task, advance=1)
                    continue

                sanitized_voice_name = self._sanitize_filename(voice.name)
                filename = f"{voice.voice_id}--{sanitized_voice_name}.mp3"
                filepath = output_path / filename

                progress.update(task, description=f"Synthesizing with {voice.name}")

                try:
                    audio_data = self._synthesize_text(
                        voice_id=voice.voice_id,
                        text=text,
                        model_id=model_id,
                        output_format=output_format,
                    )

                    # audio_data will be an iterator of bytes
                    full_audio_bytes = b"".join(audio_data)

                    save(full_audio_bytes, str(filepath))
                    logger.info(f"Successfully saved: {filepath}")
                    successful_syntheses += 1
                except ApiError as e:
                    logger.error(
                        f"API Error with voice {voice.name} (ID: {voice.voice_id}): "
                        f"{e.body or e}"
                    )
                    failed_syntheses += 1
                except Exception as e:
                    logger.error(
                        f"Error with voice {voice.name} (ID: {voice.voice_id}): {e}"
                    )
                    failed_syntheses += 1
                finally:
                    progress.update(task, advance=1)

        # Print summary
        logger.info("----- Synthesis Summary -----")
        logger.info(f"Total voices processed: {len(my_voices)}")
        logger.info(f"Successful syntheses: {successful_syntheses}")
        logger.info(f"Failed syntheses: {failed_syntheses}")
        logger.info(f"Output saved to: {output_path.resolve()}")


def say_all(text, output_folder="output_audio"):
    synthesizer = VoiceSynthesizer()
    synthesizer.synthesize_all(text, output_folder)


if __name__ == "__main__":
    fire.Fire(say_all)
</file>

</files>
