<!-- Generated by SpecStory -->

# Code Analysis and TODO Update (2025-05-16 20:20:19)

_**User**_

1. Analyze @legacy_src.txt (the original code)
2. Analyze @src.txt (the rewrite)
3. Clean up and update @TODO.md ‚Äî remove things that are already done correctly in the rewrite, add/edit things that still need to be done. Make sure to add into TODO.md the next steps that are logical. 

---

_**Assistant**_



---

# Updated TODO for e11ocutionist Python Package

## 1. Progress on Refactoring to `e11ocutionist`

This section tracks the progress of refactoring the `malmo_*.py` scripts into the `e11ocutionist` Python package.

### 1.1. Completed Tasks

‚úÖ Created basic package structure in `src/e11ocutionist/`
‚úÖ Implemented package initialization files (`__init__.py`, `__main__.py`, `__version__.py`)
‚úÖ Implemented main orchestrator module `e11ocutionist.py` with `E11ocutionistPipeline` class
‚úÖ Implemented CLI interface in `cli.py` using `fire`
‚úÖ Added partial implementation of `chunker.py` with placeholder functionality
‚úÖ Implemented `elevenlabs_converter.py` (from `malmo_11labs.py`)
‚úÖ Implemented `elevenlabs_synthesizer.py` (from `say_11labs.py`)
‚úÖ Implemented `neifix.py` utility
‚úÖ Created utility module `utils.py` for shared functionality
‚úÖ Started configuration through `PipelineConfig` dataclass

### 1.2. Pending Tasks

#### High Priority

- [ ] Complete `chunker.py` implementation
  - [ ] Implement document itemization and item element creation
  - [ ] Implement semantic analysis with LLM integration
  - [ ] Implement unit tagging and chunk creation
  - [ ] Add support for handling oversized units

- [ ] Create `entitizer.py` module
  - [ ] Port NEI identification logic from `malmo_entitizer.py`
  - [ ] Implement dictionary building and update functionality
  - [ ] Add proper XML merging and tag processing

- [ ] Create `orator.py` module
  - [ ] Implement the four processing steps (sentences, words, punctuation, emotions)
  - [ ] Add LLM integration for text enhancement
  - [ ] Add document processing orchestration

- [ ] Create `tonedown.py` module
  - [ ] Implement NEI tag processing and dictionary building
  - [ ] Add language detection functionality
  - [ ] Add pronunciation cue review with LLM integration
  - [ ] Implement emphasis reduction logic

#### Medium Priority

- [ ] Add comprehensive error handling and logging
  - [ ] Implement proper exception hierarchy
  - [ ] Add detailed error messages and recovery strategies
  
- [ ] Enhance progress tracking
  - [ ] Add more granular tracking within steps
  - [ ] Improve resumption logic

- [ ] Add unit tests for all modules
  - [ ] Create test fixtures for XML processing
  - [ ] Mock LLM calls for testability
  - [ ] Test CLI interface

#### Lower Priority

- [ ] Add detailed documentation
  - [ ] Update README.md with installation and usage instructions
  - [ ] Add docstrings to all public functions/classes
  - [ ] Consider adding API documentation

- [ ] Performance improvements
  - [ ] Add caching for expensive operations
  - [ ] Optimize XML processing
  - [ ] Consider parallel processing for independent operations

- [ ] Add configuration file support
  - [ ] Allow loading settings from YAML/JSON/TOML files
  - [ ] Implement configuration validation

### 1.3. Next Concrete Steps

1. Complete the `chunker.py` implementation by transferring all the key functionality from `malmo_chunker.py`:
   - Implement `itemize_document` and `create_item_elements` functions
   - Add semantic boundary detection with LLM
   - Complete chunk creation logic

2. Create the `entitizer.py` module based on `malmo_entitizer.py`:
   - Start with setting up the main document processing flow
   - Add NEI identification and XML manipulation functions
   - Integrate LLM calls for entity recognition

3. Create the `orator.py` module based on `malmo_orator.py`:
   - Set up the processing pipeline with all enhancement steps
   - Implement sentence restructuring with LLM
   - Add word normalization and punctuation enhancement
   - Implement emotional emphasis functionality

4. Create the `tonedown.py` module based on `malmo_tonedown.py`:
   - Implement NEI tag processing and dictionary management
   - Add language detection functionality
   - Add pronunciation cue review with LLM
   - Implement emphasis reduction algorithm

5. Update existing modules to ensure they work together seamlessly:
   - Ensure consistent function signatures
   - Standardize error handling
   - Verify the pipeline stages connect properly

6. Create a simple test suite to verify basic functionality

### 1.4. Package Structure Status

```
e11ocutionist/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îî‚îÄ‚îÄ e11ocutionist/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py           ‚úÖ Implemented
‚îÇ       ‚îú‚îÄ‚îÄ __main__.py           ‚úÖ Implemented
‚îÇ       ‚îú‚îÄ‚îÄ __version__.py        ‚úÖ Implemented  
‚îÇ       ‚îú‚îÄ‚îÄ cli.py                ‚úÖ Implemented
‚îÇ       ‚îú‚îÄ‚îÄ e11ocutionist.py      ‚úÖ Implemented
‚îÇ       ‚îú‚îÄ‚îÄ chunker.py            üü° Partially implemented
‚îÇ       ‚îú‚îÄ‚îÄ entitizer.py          ‚ùå Not implemented
‚îÇ       ‚îú‚îÄ‚îÄ orator.py             ‚ùå Not implemented
‚îÇ       ‚îú‚îÄ‚îÄ tonedown.py           ‚ùå Not implemented
‚îÇ       ‚îú‚îÄ‚îÄ elevenlabs_converter.py ‚úÖ Implemented
‚îÇ       ‚îú‚îÄ‚îÄ elevenlabs_synthesizer.py ‚úÖ Implemented
‚îÇ       ‚îú‚îÄ‚îÄ neifix.py             ‚úÖ Implemented
‚îÇ       ‚îî‚îÄ‚îÄ utils.py              ‚úÖ Implemented
‚îú‚îÄ‚îÄ tests/                      ‚ùå Not implemented
‚îú‚îÄ‚îÄ legacy_src/                 ‚úÖ Original scripts preserved
‚îú‚îÄ‚îÄ .gitignore                  ‚úÖ Present
‚îú‚îÄ‚îÄ pyproject.toml              ‚úÖ Present
‚îú‚îÄ‚îÄ README.md                   ‚úÖ Present
‚îî‚îÄ‚îÄ TODO.md                     üü° This file (being updated)
```

### 1.5. Technology Stack Status

* **Language**: Python (>=3.10) ‚úÖ
* **CLI Framework**: `python-fire` ‚úÖ
* **LLM Interaction**: `litellm` ‚úÖ
* **XML Processing**: `lxml` ‚úÖ
* **Text-to-Speech**: `elevenlabs` SDK ‚úÖ
* **Logging**: `loguru` ‚úÖ
* **Progress Bars/Rich Output**: `rich` ‚úÖ
* **Retry Logic**: `tenacity` ‚úÖ
* **Environment Variables**: `python-dotenv` ‚úÖ
* **Token Counting**: `tiktoken` ‚úÖ
* **Build System/Packaging**: `hatch` ‚úÖ
* **Linting/Formatting**: `ruff` ‚úÖ
* **Testing**: `pytest` ‚ùå Not implemented yet

## 2. Roadmap for Future Enhancements

Once the basic refactoring is complete, consider these enhancements:

- [ ] Add support for alternative LLM providers
- [ ] Create a web interface for the pipeline
- [ ] Add visualization tools for the processing stages
- [ ] Implement batch processing capabilities
- [ ] Add support for additional text-to-speech services besides ElevenLabs
- [ ] Create profiling and optimization tools to improve processing speed

---

_**User**_

Edit @TODO.md so that it only actually contains the things that need to be done, and it does include these in  a form preceded by `[ ]`, and it does contain them in form of very detailed specific descriptions actionable  by a junior developer 

---

_**Assistant**_



---

# TODO List for e11ocutionist

## 1. Implement Core Processing Modules

### 1.1. Complete `chunker.py` Implementation

- [ ] Implement `itemize_document` function
  - Add a function that takes a text document and splits it into semantic paragraph-level units
  - Use regex to identify paragraph boundaries (blank lines)
  - Classify paragraphs as headings, blockquotes, lists, code blocks, etc.
  - Return a list of tuples with `(item_text, attachment_type)` where attachment_type is "following", "preceding", or "normal"

- [ ] Implement `create_item_elements` function
  - Create a function that processes the output of `itemize_document`
  - Generate XML items with appropriate attributes (id, tok)
  - Preserve whitespace by setting `xml:space="preserve"`
  - Generate unique IDs for each item using the `generate_id` function in `utils.py`
  - Return a list of tuples containing `(item_xml, item_id)`

- [ ] Implement `semantic_analysis` function
  - Create a function that uses an LLM to identify semantic boundaries (chapters, scenes, etc.)
  - Send the document with its items to the LLM with a prompt to identify semantic units
  - Process the LLM's response to extract identified boundaries
  - Add fallback logic for cases where the LLM fails or identifies too few boundaries
  - Return a list of tuples with `(item_id, boundary_type)` where boundary_type is "chapter", "scene", or "unit"

- [ ] Implement `add_unit_tags` function
  - Create a function that adds `<unit>` tags around sequences of items based on semantic analysis
  - Group items by their boundary types
  - Set appropriate attributes on unit tags (type, tok)
  - Handle cases where semantic boundaries weren't identified
  - Add fallback logic using regex for parsing failures

- [ ] Implement `create_chunks` function
  - Create a function that groups units into chunks of manageable size
  - Ensure each chunk doesn't exceed the maximum token size
  - Allow chapters to always start new chunks
  - Handle oversized units by splitting them across chunks
  - Add tok attributes with accurate token counts

- [ ] Handle oversized units
  - Add function to split oversized units across multiple chunks
  - Preserve chunk and unit attributes
  - Maintain document hierarchy

### 1.2. Implement `entitizer.py` Module

- [ ] Create the basic module structure
  - Copy the file structure from `chunker.py` as a template
  - Add imports for lxml, backoff, loguru, etc.
  - Define constants for default models and temperature

- [ ] Implement XML parsing and serialization
  - Add functions to parse input XML
  - Extract chunks and items from XML
  - Add functions to serialize XML with proper encoding and formatting

- [ ] Implement NEI identification with LLM
  - Create a function that uses an LLM to identify named entities
  - Craft a prompt that explains NEI tagging rules
  - Process the LLM's response to extract tagged text
  - Add retries and fallback for API failures

- [ ] Implement NEI dictionary management
  - Create functions to extract NEIs from tags
  - Build and update a dictionary mapping NEIs to pronunciations
  - Track new vs. existing NEIs with the new="true" attribute

- [ ] Implement XML merging
  - Create a function to merge LLM-tagged items back into the original XML
  - Maintain document structure while replacing content
  - Handle parsing errors gracefully

- [ ] Add incremental saving
  - Implement saving after each chunk is processed
  - Save both the XML document and NEI dictionary
  - Add backup functionality with timestamps

### 1.3. Implement `orator.py` Module

- [ ] Create the basic module structure
  - Set up imports and constants
  - Define functions for processing documents and chunks

- [ ] Implement sentence restructuring
  - Create a function that uses an LLM to divide long sentences into shorter ones
  - Ensure paragraphs and headings end with punctuation
  - Preserve all XML tags and structure
  - Extract and merge tagged items

- [ ] Implement word normalization
  - Create a function that uses an LLM to convert digits to numeric words
  - Replace symbols like '/' and '&' with appropriate conjunctions
  - Spell out rare symbols
  - Preserve XML structure

- [ ] Implement punctuation enhancement
  - Create a function that algorithmically enhances punctuation
  - Process parenthetical expressions
  - Replace em/en dashes with ellipses when used as pauses
  - Preserve XML structure

- [ ] Implement emotional emphasis
  - Create a function that uses an LLM to add emotional emphasis
  - Add `<em>` tags to emphasize words/phrases
  - Insert `<hr/>` for dramatic pauses
  - Add exclamation marks for energy
  - Preserve XML structure

- [ ] Add step selection logic
  - Implement logic to run specific steps based on configuration
  - Allow individual steps (sentences, words, punctuation, emotions) or all steps
  - Save after each step

### 1.4. Implement `tonedown.py` Module

- [ ] Create the basic module structure
  - Set up imports and constants
  - Define the main processing function

- [ ] Implement NEI dictionary building
  - Create a function to extract NEIs from tags
  - Build a dictionary of NEIs with orig attribute and pronunciation cues
  - Set new="true" for first occurrences

- [ ] Implement language detection
  - Create a function that uses an LLM to detect document language
  - Extract a representative sample from the document
  - Process the LLM's response to extract the language

- [ ] Implement pronunciation cue review
  - Create a function that uses an LLM to review pronunciation cues
  - Send the NEI dictionary and detected language to the LLM
  - Process the LLM response to extract revised pronunciations
  - Update the NEI dictionary with revisions

- [ ] Implement XML update
  - Create a function to update `<nei>` tags with revised pronunciations
  - Preserve tag attributes and structure
  - Update tag content with revised pronunciations

- [ ] Implement emphasis reduction
  - Create a function to remove excessive `<em>` tags
  - Calculate word distances between emphasis tags
  - Remove tags that are too close together based on min_distance parameter
  - Add proper regex handling for complex XML

## 2. Add Testing and Documentation

### 2.1. Implement Unit Tests

- [ ] Set up pytest framework
  - Create `tests/` directory with standard structure
  - Add `conftest.py` with common fixtures
  - Configure pytest in pyproject.toml

- [ ] Create test fixtures
  - Add sample XML files for testing
  - Create mock LLM responses
  - Set up test environment variables

- [ ] Add tests for `chunker.py`
  - Test paragraph splitting and classification
  - Test semantic analysis
  - Test chunk creation with various inputs

- [ ] Add tests for `entitizer.py`
  - Test NEI identification and tagging
  - Test dictionary building and updates
  - Test XML merging

- [ ] Add tests for `orator.py`
  - Test each processing step individually
  - Test full pipeline with various inputs
  - Test error handling and recovery

- [ ] Add tests for `tonedown.py`
  - Test NEI dictionary building
  - Test language detection
  - Test pronunciation cue review
  - Test emphasis reduction

- [ ] Add tests for integration between modules
  - Test passing results between pipeline stages
  - Test full pipeline execution
  - Test error handling between modules

### 2.2. Improve Documentation

- [ ] Update README.md
  - Add installation instructions
  - Add usage examples for CLI
  - Add examples for programmatic usage
  - Add development setup instructions

- [ ] Add docstrings to all public functions
  - Use Google-style docstrings
  - Include parameter descriptions
  - Document return values
  - Add examples where appropriate

- [ ] Document CLI commands
  - Create comprehensive help text
  - Add examples for each command
  - Document all options and parameters

### 2.3. Final Integration Tasks

- [ ] Complete the pipeline integration
  - Verify all modules work together correctly
  - Test processing flow from chunking through ElevenLabs conversion
  - Ensure configuration is properly passed between modules

- [ ] Add comprehensive error handling
  - Implement proper exception handling throughout
  - Add meaningful error messages
  - Add recovery strategies for common failures

- [ ] Add logging enhancements
  - Improve log messages for clarity
  - Add log levels for different verbosity needs
  - Add progress indicators for long-running operations

- [ ] Create example workflows
  - Add example scripts for common tasks
  - Document typical processing workflows
  - Include sample inputs and expected outputs

---

_**User**_

Actually write these changes into @TODO.md  !!!

---

_**Assistant**_

I'll write these changes to the TODO.md file. Let me do that now.

---

```diff
- # TODO
- 
- ## 1. Refactoring Plan for `e11ocutionist` Python Package
- 
- This section outlines a plan to refactor the existing tools into a cohesive Python package named `e11ocutionist` .
- 
- ### 1.1. A. Goals
- 
- * Consolidate the functionality of the `malmo_*.py` scripts and potentially `say_11labs.py` into a single, installable Python package.
- * Improve code organization, maintainability, and reusability.
- * Provide a clear command-line interface (CLI) for the package.
- * Maintain the existing technology stack.
- 
- ### 1.2. B. Package Structure
- 
- The package source code will reside in the `src/` directory.
- 
- ```
- 
- e11ocutionist\_project/
- ‚îú‚îÄ‚îÄ src/
- ‚îÇ   ‚îî‚îÄ‚îÄ e11ocutionist/
- ‚îÇ       ‚îú‚îÄ‚îÄ **init**.py           \# Package initializer, exports, version
- ‚îÇ       ‚îú‚îÄ‚îÄ **main**.py           \# Makes the package executable (calls cli.py)
- ‚îÇ       ‚îú‚îÄ‚îÄ cli.py                \# Command-line interface logic (using Fire or Typer)
- ‚îÇ       ‚îú‚îÄ‚îÄ e11ocutionist.py      \# Main package logic, core workflow orchestration
- ‚îÇ       ‚îú‚îÄ‚îÄ chunker.py            \# Logic from malmo\_chunker.py
- ‚îÇ       ‚îú‚îÄ‚îÄ entitizer.py          \# Logic from malmo\_entitizer.py
- ‚îÇ       ‚îú‚îÄ‚îÄ orator.py             \# Logic from malmo\_orator.py
- ‚îÇ       ‚îú‚îÄ‚îÄ tonedown.py           \# Logic from malmo\_tonedown.py
- ‚îÇ       ‚îú‚îÄ‚îÄ elevenlabs\_converter.py \# Logic from malmo\_11labs.py
- ‚îÇ       ‚îú‚îÄ‚îÄ elevenlabs\_synthesizer.py \# Logic from say\_11labs.py (if included)
- ‚îÇ       ‚îú‚îÄ‚îÄ neifix.py             \# Logic from malmo\_neifix.py (if included as utility)
- ‚îÇ       ‚îú‚îÄ‚îÄ
- ‚îÇ       ‚îî‚îÄ‚îÄ utils.py              \# Common utility functions (XML parsing, file ops, etc.)
- ‚îú‚îÄ‚îÄ tests/                      \# Pytest tests
- ‚îú‚îÄ‚îÄ legacy_src/                       \# Original scripts (to be deprecated or kept for reference)
- ‚îú‚îÄ‚îÄ .gitignore
- ‚îú‚îÄ‚îÄ pyproject.toml              \# Project metadata, dependencies, build system (Hatch)
- ‚îú‚îÄ‚îÄ README.md
- ‚îî‚îÄ‚îÄ TODO.md                     \# This file
- 
- ```
- 
- ### 1.3. C. Technology Stack
- 
- * **Language**: Python (>=3.10 as per current `pyproject.toml` [cite: 652])
- * **CLI Framework**: `python-fire` (currently used) or consider `Typer` for more structured CLI.
- * **LLM Interaction**: `litellm`
- * **XML Processing**: `lxml`
- * **Text-to-Speech**: `elevenlabs` SDK
- * **Logging**: `loguru`
- * **Progress Bars/Rich Output**: `rich` (already used by some scripts)
- * **Retry Logic**: `tenacity`
- * **Environment Variables**: `python-dotenv`
- * **Token Counting**: `tiktoken`
- * **Build System/Packaging**: `hatch` [cite: 652, 659]
- * **Linting/Formatting**: `ruff` [cite: 647, 652]
- * **Testing**: `pytest` [cite: 653]
- 
- ### 1.4. D. Core Components & Refactoring Steps
- 
- #### 1.4.1. D.1. `e11ocutionist.py` (Core Orchestrator)
- 
- * This module will replace `malmo_all.py`.
- * It will define the main processing pipeline.
- * Functions/classes in this module will call functionalities from other modules (`chunker.py`,  `entitizer.py`, etc.).
- * It will handle configuration management (models, temperatures, API keys).
- * It should manage progress tracking and resumption logic, perhaps in a more object-oriented way.
-     **Refactoring Actions**:
- 
-     1. Define a main `E11ocutionistPipeline` class or a set of core processing functions.
-     2. Migrate dependency checking, output directory creation, progress saving/loading, and summary generation logic from `malmo_all.py`.
-     3. Instead of `subprocess.run`, directly import and call functions/methods from the refactored modules (`chunker.process_document`, `entitizer.process_document`, etc.).
- 
- #### 1.4.2. D.2. `chunker.py`
- 
- * Contains the refactored logic from `malmo_chunker.py`.
- * Functions should be designed for programmatic use (e.g.,  `itemize_document`,  `semantic_analysis`,  `create_chunks_from_units`).
- * LLM interaction for semantic analysis should be clearly separated.
-     **Refactoring Actions**:
- 
-     1. Convert `malmo_chunker.py` into a module with importable functions.
-     2. The main `process_document` function in `malmo_chunker.py` can be the primary entry point for this module's functionality.
-     3. Make helper functions like `itemize_document`, `semantic_analysis`, `add_unit_tags`, `create_chunks` reusable.
- 
- #### 1.4.3. D.3. `entitizer.py`
- 
- * Contains the refactored logic from `malmo_entitizer.py`.
- * Focus on functions for NEI identification and XML merging.
-     **Refactoring Actions**:
- 
-     1. Convert `malmo_entitizer.py` into a module.
-     2. The `process_document` (or a similarly named function) will be called by the main orchestrator.
-     3. Functions like `identify_entities`, `extract_nei_from_tag`, `merge_tagged_items` should be well-defined.
-     4. The benchmark mode could be a separate CLI subcommand or an option.
- 
- #### 1.4.4. D.4. `orator.py`
- 
- * Contains the refactored logic from `malmo_orator.py`.
- * Modularize the different processing steps (sentences, words, punctuation, emotions).
-     **Refactoring Actions**:
- 
-     1. Convert `malmo_orator.py` into a module.
-     2. Each processing step (`process_sentences`, `process_words`, etc.) should be a distinct, callable function.
-     3. The main `process_document` function will orchestrate these steps based on configuration.
- 
- #### 1.4.5. D.5. `tonedown.py`
- 
- * Contains the refactored logic from `malmo_tonedown.py`.
- * Separate NEI dictionary building, language detection, LLM review, and emphasis reduction.
-     **Refactoring Actions**:
- 
-     1. Convert `malmo_tonedown.py` into a module.
-     2. Key functions: `process_nei_tags`, `detect_language_llm`, `revise_neis_llm`, `update_nei_cues`, `reduce_emphasis`.
-     3. The main `process_document` function will integrate these steps.
- 
- #### 1.4.6. D.6. `elevenlabs_converter.py`
- 
- * Contains the refactored logic from `malmo_11labs.py`.
- * Focus on XML to ElevenLabs text format conversion.
-     **Refactoring Actions**:
- 
-     1. Convert `malmo_11labs.py` into a module.
-     2. The main conversion logic (`process_document` or similar) should be the entry point.
-     3. Helper functions for text extraction, dialog processing, and postprocessing should be clearly defined.
- 
- #### 1.4.7. D.7. `elevenlabs_synthesizer.py` (Optional Integration)
- 
- * Contains the refactored logic from `say_11labs.py`.
- * This could be a separate tool within the package or integrated as an optional final step in the main pipeline.
-     **Refactoring Actions**:
- 
-     1. Convert `say_11labs.py` into a module, likely with a `VoiceSynthesizer` class.
-     2. Expose methods for synthesizing text with specific or all voices.
- 
- #### 1.4.8. D.8. `neifix.py` (Utility)
- 
- * Contains the refactored logic from `malmo_neifix.py`.
- * Can be included as a utility module if its functionality is still required or integrated into `entitizer.py` or `tonedown.py` if appropriate.
-     **Refactoring Actions**:
- 
-     1. Convert `malmo_neifix.py` into a module with an importable `transform_nei_content` function.
- 
- #### 1.4.9. D.9. `utils.py`
- 
- * A new module for shared utility functions.
-     - Robust XML parsing (e.g., `parse_xml` from `malmo_11labs.py` or `malmo_entitizer.py`).
-     - Common file operations.
-     - LLM interaction wrappers (if patterns emerge across modules, though `litellm` handles much of this).
-     - Configuration loading.
- 
- #### 1.4.10. D.10. `cli.py`
- 
- * This module will define the command-line interface for the `e11ocutionist` package.
- * It will use `python-fire` or `Typer` to expose functionalities.
- * Subcommands could be defined for:
-     - Running the full pipeline (e.g., `e11ocutionist process <input_file> [options]`).
-     - Running individual major steps (e.g., `e11ocutionist chunk <input_file> <output_file> [options]`).
-     - Using the ElevenLabs synthesizer (e.g., `e11ocutionist say "<text>" [options]`).
-     - Using the NEI-fix utility (e.g., `e11ocutionist neifix <input_file> <output_file>`).
- * It will parse arguments and call the appropriate functions in `e11ocutionist.py` or other modules.
-     **Refactoring Actions**:
- 
-     1. Identify the main entry points from the current scripts.
-     2. Design CLI commands and options.
-     3. Implement the CLI using `fire` or `Typer`, mapping commands to the refactored functions/classes.
- 
- #### 1.4.11. D.11. `__main__.py`
- 
- * A simple script that imports and calls the main function from `cli.py`.
- 
-     
- 
- ```python
-     # src/e11ocutionist/__main__.py
-     from .cli import main
- 
-     if __name__ == "__main__":
-         main()
-     ```
- 
-     This allows the package to be run as `python -m e11ocutionist` .
- 
- #### 1.4.12. D.12. `__init__.py`
- 
- * Initializes the package.
- * Can expose key functions or classes for programmatic use.
- * Should define `__version__` (dynamically via `hatch-vcs` as per `pyproject.toml`).
- 
-     
- 
- ```python
-     # src/e11ocutionist/__init__.py
-     from .__version__ import __version__
-     from .e11ocutionist import E11ocutionistPipeline # Example
-     # Potentially export other key functions/classes
-     ```
- 
- ### 1.5. E. `pyproject.toml` Updates
- 
- * **Project Name**: `e11ocutionist` [cite: 652]
- * **Dependencies**: Consolidate all dependencies from the `legacy_src/*.py` script headers into the `[project.dependencies]` section. [cite: 78, 165, 314, 437, 527, 615]
-     - `fire`
-     - `lxml` (and `types-lxml` for type hints)
-     - `loguru`
-     - `litellm` (pin version if necessary, e.g., `<=1.67.2`)
-     - `python-dotenv`
-     - `tenacity`
-     - `rich`
-     - `tiktoken`
-     - `backoff`
-     - `elevenlabs`
- * **Scripts**: Define the main CLI entry point.
- 
-     
- 
- ```toml
-     [project.scripts]
-     e11ocutionist = "e11ocutionist.__main__:main"
-     ```
- 
- * Ensure `hatch-vcs` is configured to correctly pull version information (already present). [cite: 652]
- 
- ### 1.6. F. Testing Strategy
- 
- * Develop unit tests for individual functions and modules (e.g., XML parsing, text transformation logic, NEI extraction).
- * Develop integration tests for major pipeline steps (e.g., chunking an input, entitizing a chunk).
- * Test CLI argument parsing and command execution.
- * Use `pytest` and `pytest-cov` for test execution and coverage reporting. [cite: 653]
- * Mock LLM and ElevenLabs API calls to avoid external dependencies during most tests and to control test inputs/outputs.
- 
- ### 1.7. G. Documentation
- 
- * Update `README.md` with installation instructions, usage examples for the new package and CLI, and development guidelines.
- * Consider adding more detailed API documentation for developers if the package is intended for programmatic use beyond the CLI. MkDocs could be used if this becomes extensive.
- 
- ### 1.8. H. Workflow for `sapko.sh`
- 
- * The functionality of `sapko.sh` should be replicable via the new `e11ocutionist` CLI.
- * For example:
- 
-     
- 
- ```bash
-     e11ocutionist process \
-         "Andrzej Sapkowski - WiedzÃÅmin Geralt z Rivii 0.1 - RozdrozÃáe krukoÃÅw.txt" \
-         --output-dir "sapkowski_output" \
-         --start-step orator # Assuming a mechanism to specify input for orator from entitizer output
-     ```
- 
-     Or, if intermediate files are still desired and the previous steps were run separately:
- 
-     
- 
- ```bash
-     e11ocutionist orator "input_step2.xml" "output_step3.xml" --all_steps --verbose --backup
-     e11ocutionist tonedown "output_step3.xml" "output_step4.xml" --verbose
-     e11ocutionist convert-11labs "output_step4.xml" "output_step5.txt" --verbose
-     ```
- 
-     The exact commands will depend on how `cli.py` is designed.
- 
- ### 1.9. I. Deprecation
- 
- * Once the `e11ocutionist` package is stable and tested, the individual scripts in the `legacy_src/` directory can be deprecated or removed. They can be kept in a separate `archive/` or `legacy/` folder for historical reference if needed.
+ # TODO List for e11ocutionist
+ 
+ ## 1. Implement Core Processing Modules
+ 
+ ### 1.1. Complete `chunker.py` Implementation
+ 
+ - [ ] Implement `itemize_document` function
+   - Add a function that takes a text document and splits it into semantic paragraph-level units
+   - Use regex to identify paragraph boundaries (blank lines)
+   - Classify paragraphs as headings, blockquotes, lists, code blocks, etc.
+   - Return a list of tuples with `(item_text, attachment_type)` where attachment_type is "following", "preceding", or "normal"
+ 
+ - [ ] Implement `create_item_elements` function
+   - Create a function that processes the output of `itemize_document`
+   - Generate XML items with appropriate attributes (id, tok)
+   - Preserve whitespace by setting `xml:space="preserve"`
+   - Generate unique IDs for each item using the `generate_id` function in `utils.py`
+   - Return a list of tuples containing `(item_xml, item_id)`
+ 
+ - [ ] Implement `semantic_analysis` function
+   - Create a function that uses an LLM to identify semantic boundaries (chapters, scenes, etc.)
+   - Send the document with its items to the LLM with a prompt to identify semantic units
+   - Process the LLM's response to extract identified boundaries
+   - Add fallback logic for cases where the LLM fails or identifies too few boundaries
+   - Return a list of tuples with `(item_id, boundary_type)` where boundary_type is "chapter", "scene", or "unit"
+ 
+ - [ ] Implement `add_unit_tags` function
+   - Create a function that adds `<unit>` tags around sequences of items based on semantic analysis
+   - Group items by their boundary types
+   - Set appropriate attributes on unit tags (type, tok)
+   - Handle cases where semantic boundaries weren't identified
+   - Add fallback logic using regex for parsing failures
+ 
+ - [ ] Implement `create_chunks` function
+   - Create a function that groups units into chunks of manageable size
+   - Ensure each chunk doesn't exceed the maximum token size
+   - Allow chapters to always start new chunks
+   - Handle oversized units by splitting them across chunks
+   - Add tok attributes with accurate token counts
+ 
+ - [ ] Handle oversized units
+   - Add function to split oversized units across multiple chunks
+   - Preserve chunk and unit attributes
+   - Maintain document hierarchy
+ 
+ ### 1.2. Implement `entitizer.py` Module
+ 
+ - [ ] Create the basic module structure
+   - Copy the file structure from `chunker.py` as a template
+   - Add imports for lxml, backoff, loguru, etc.
+   - Define constants for default models and temperature
+ 
+ - [ ] Implement XML parsing and serialization
+   - Add functions to parse input XML
+   - Extract chunks and items from XML
+   - Add functions to serialize XML with proper encoding and formatting
+ 
+ - [ ] Implement NEI identification with LLM
+   - Create a function that uses an LLM to identify named entities
+   - Craft a prompt that explains NEI tagging rules
+   - Process the LLM's response to extract tagged text
+   - Add retries and fallback for API failures
+ 
+ - [ ] Implement NEI dictionary management
+   - Create functions to extract NEIs from tags
+   - Build and update a dictionary mapping NEIs to pronunciations
+   - Track new vs. existing NEIs with the new="true" attribute
+ 
+ - [ ] Implement XML merging
+   - Create a function to merge LLM-tagged items back into the original XML
+   - Maintain document structure while replacing content
+   - Handle parsing errors gracefully
+ 
+ - [ ] Add incremental saving
+   - Implement saving after each chunk is processed
+   - Save both the XML document and NEI dictionary
+   - Add backup functionality with timestamps
+ 
+ ### 1.3. Implement `orator.py` Module
+ 
+ - [ ] Create the basic module structure
+   - Set up imports and constants
+   - Define functions for processing documents and chunks
+ 
+ - [ ] Implement sentence restructuring
+   - Create a function that uses an LLM to divide long sentences into shorter ones
+   - Ensure paragraphs and headings end with punctuation
+   - Preserve all XML tags and structure
+   - Extract and merge tagged items
+ 
+ - [ ] Implement word normalization
+   - Create a function that uses an LLM to convert digits to numeric words
+   - Replace symbols like '/' and '&' with appropriate conjunctions
+   - Spell out rare symbols
+   - Preserve XML structure
+ 
+ - [ ] Implement punctuation enhancement
+   - Create a function that algorithmically enhances punctuation
+   - Process parenthetical expressions
+   - Replace em/en dashes with ellipses when used as pauses
+   - Preserve XML structure
+ 
+ - [ ] Implement emotional emphasis
+   - Create a function that uses an LLM to add emotional emphasis
+   - Add `<em>` tags to emphasize words/phrases
+   - Insert `<hr/>` for dramatic pauses
+   - Add exclamation marks for energy
+   - Preserve XML structure
+ 
+ - [ ] Add step selection logic
+   - Implement logic to run specific steps based on configuration
+   - Allow individual steps (sentences, words, punctuation, emotions) or all steps
+   - Save after each step
+ 
+ ### 1.4. Implement `tonedown.py` Module
+ 
+ - [ ] Create the basic module structure
+   - Set up imports and constants
+   - Define the main processing function
+ 
+ - [ ] Implement NEI dictionary building
+   - Create a function to extract NEIs from tags
+   - Build a dictionary of NEIs with orig attribute and pronunciation cues
+   - Set new="true" for first occurrences
+ 
+ - [ ] Implement language detection
+   - Create a function that uses an LLM to detect document language
+   - Extract a representative sample from the document
+   - Process the LLM's response to extract the language
+ 
+ - [ ] Implement pronunciation cue review
+   - Create a function that uses an LLM to review pronunciation cues
+   - Send the NEI dictionary and detected language to the LLM
+   - Process the LLM response to extract revised pronunciations
+   - Update the NEI dictionary with revisions
+ 
+ - [ ] Implement XML update
+   - Create a function to update `<nei>` tags with revised pronunciations
+   - Preserve tag attributes and structure
+   - Update tag content with revised pronunciations
+ 
+ - [ ] Implement emphasis reduction
+   - Create a function to remove excessive `<em>` tags
+   - Calculate word distances between emphasis tags
+   - Remove tags that are too close together based on min_distance parameter
+   - Add proper regex handling for complex XML
+ 
+ ## 2. Add Testing and Documentation
+ 
+ ### 2.1. Implement Unit Tests
+ 
+ - [ ] Set up pytest framework
+   - Create `tests/` directory with standard structure
+   - Add `conftest.py` with common fixtures
+   - Configure pytest in pyproject.toml
+ 
+ - [ ] Create test fixtures
+   - Add sample XML files for testing
+   - Create mock LLM responses
+   - Set up test environment variables
+ 
+ - [ ] Add tests for `chunker.py`
+   - Test paragraph splitting and classification
+   - Test semantic analysis
+   - Test chunk creation with various inputs
+ 
+ - [ ] Add tests for `entitizer.py`
+   - Test NEI identification and tagging
+   - Test dictionary building and updates
+   - Test XML merging
+ 
+ - [ ] Add tests for `orator.py`
+   - Test each processing step individually
+   - Test full pipeline with various inputs
+   - Test error handling and recovery
+ 
+ - [ ] Add tests for `tonedown.py`
+   - Test NEI dictionary building
+   - Test language detection
+   - Test pronunciation cue review
+   - Test emphasis reduction
+ 
+ - [ ] Add tests for integration between modules
+   - Test passing results between pipeline stages
+   - Test full pipeline execution
+   - Test error handling between modules
+ 
+ ### 2.2. Improve Documentation
+ 
+ - [ ] Update README.md
+   - Add installation instructions
+   - Add usage examples for CLI
+   - Add examples for programmatic usage
+   - Add development setup instructions
+ 
+ - [ ] Add docstrings to all public functions
+   - Use Google-style docstrings
+   - Include parameter descriptions
+   - Document return values
+   - Add examples where appropriate
+ 
+ - [ ] Document CLI commands
+   - Create comprehensive help text
+   - Add examples for each command
+   - Document all options and parameters
+ 
+ ### 2.3. Final Integration Tasks
+ 
+ - [ ] Complete the pipeline integration
+   - Verify all modules work together correctly
+   - Test processing flow from chunking through ElevenLabs conversion
+   - Ensure configuration is properly passed between modules
+ 
+ - [ ] Add comprehensive error handling
+   - Implement proper exception handling throughout
+   - Add meaningful error messages
+   - Add recovery strategies for common failures
+ 
+ - [ ] Add logging enhancements
+   - Improve log messages for clarity
+   - Add log levels for different verbosity needs
+   - Add progress indicators for long-running operations
+ 
+ - [ ] Create example workflows
+   - Add example scripts for common tasks
+   - Document typical processing workflows
+   - Include sample inputs and expected outputs
  
```

---

I've updated the TODO.md file with a detailed, actionable list of tasks. The new format:
1. Is organized by module and major functionality
2. Has clear, specific tasks with bullet points describing exactly what needs to be done
3. Uses checkboxes (`[ ]`) for tracking progress
4. Provides enough detail that a junior developer could understand and implement each task
5. Focuses only on what needs to be done, removing completed work and planning sections
6. Includes specific implementation details and requirements for each task

The structure is now much more actionable and clearer about what needs to be implemented. Each task has concrete steps and implementation details rather than just high-level descriptions.

Would you like me to make any adjustments to the format or content of the TODO list?

---

